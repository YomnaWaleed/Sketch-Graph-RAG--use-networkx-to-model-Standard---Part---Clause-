{
  "0": {
    "id": "6f61503a-d3a1-49ca-a9c0-092821f3c7fd",
    "title": "Supplier Monitoring",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "1": {
    "id": "841b8a3d-b19e-437a-bf75-4054cc96eb53",
    "title": "Technical Requirements",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "2": {
    "id": "64d401d5-1d19-4e6f-92e7-b7fbfb632783",
    "title": "Legal and Administrative",
    "level": 2,
    "parent_id": null,
    "content": "Requirements"
  },
  "3": {
    "id": "09dc2fed-4e82-476e-a073-4ae6a5480797",
    "title": "Project Requirements",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "4": {
    "id": "1ebd9ae8-df82-4150-9645-b33829177bd3",
    "title": "Request for Proposals",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "5": {
    "id": "01d955f4-2845-439e-b8f7-8d8215d5c7b6",
    "title": "Supplier Qualification",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "6": {
    "id": "940d7a06-8942-45f4-b437-c6c7939b236c",
    "title": "Supplier Tendering",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "7": {
    "id": "45a6e692-939d-4cbc-b013-9b620fc05ebd",
    "title": "Product Release",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "8": {
    "id": "3b150aa6-d371-450e-ae5d-65cbdf5e8d84",
    "title": "Quality Assurance",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "9": {
    "id": "f88f2933-481f-4c43-afd8-8c0eeb85bcc7",
    "title": "Verification",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "10": {
    "id": "4f8c6824-66f5-45bf-960b-2dfcffe23c12",
    "title": "Joint Review",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "11": {
    "id": "274b2e87-9e7c-4f38-bc8a-87dea5400eff",
    "title": "Documentation",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "12": {
    "id": "d6dde172-a1d8-49c8-9d3f-b2349e513023",
    "title": "Configuration",
    "level": 2,
    "parent_id": null,
    "content": "Management"
  },
  "13": {
    "id": "6127166c-8c2f-4e15-bcd9-d3a30c44d433",
    "title": "Problem Resolution",
    "level": 2,
    "parent_id": null,
    "content": "Management"
  },
  "14": {
    "id": "517b36f5-0601-4b34-aa57-465f2731b977",
    "title": "Change Request",
    "level": 2,
    "parent_id": null,
    "content": "Management"
  },
  "15": {
    "id": "64e11af7-3032-44a5-81a9-bea79e01a680",
    "title": "Project Management",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "16": {
    "id": "7a4a9876-be98-4066-bfd6-ba2759dccc2b",
    "title": "Risk Management",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "17": {
    "id": "362c33c3-722e-4237-b00e-8eba83dde871",
    "title": "Measurement",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "18": {
    "id": "6322265a-ad19-4619-a209-8c462bf97d70",
    "title": "Contract Agreement",
    "level": 2,
    "parent_id": null,
    "content": "Process Improvement \nProcess Group (PIM)"
  },
  "19": {
    "id": "e9be388c-6076-4de2-a8d0-8b7d045f2fbd",
    "title": "Process Improvement",
    "level": 2,
    "parent_id": null,
    "content": "Reuse Process Group \n(REU)"
  },
  "20": {
    "id": "3db28753-10f5-484a-92c0-6af5ad566e00",
    "title": "Reuse Program",
    "level": 2,
    "parent_id": null,
    "content": "Management\nSystem Engineering Process Group (SYS)"
  },
  "21": {
    "id": "8505509a-b5fb-4f36-9b4e-009c0a34eb6a",
    "title": "Requirements Elicitation",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "22": {
    "id": "904f9e46-0c5d-4d67-80c7-b7e4f8f2c1cc",
    "title": "System Requirements",
    "level": 2,
    "parent_id": null,
    "content": "Analysis"
  },
  "23": {
    "id": "f1c5fb27-e1fc-4704-b462-55da1e88b68b",
    "title": "System Architectural",
    "level": 2,
    "parent_id": null,
    "content": "Design"
  },
  "24": {
    "id": "265bb610-4621-404a-baae-a0ea70d323d8",
    "title": "System Integration and",
    "level": 2,
    "parent_id": null,
    "content": "Integration Test"
  },
  "25": {
    "id": "372884e8-20ae-4235-a140-ac661d888494",
    "title": "System Qualification Test",
    "level": 2,
    "parent_id": null,
    "content": "Software Engineering Process Group (SWE)"
  },
  "26": {
    "id": "aa12509d-bf76-4821-ad23-ae5a619dc60a",
    "title": "Software Requirements",
    "level": 2,
    "parent_id": null,
    "content": "Analysis"
  },
  "27": {
    "id": "35ba1de5-7e50-4c9a-a589-91ca28b27394",
    "title": "Software Architectural",
    "level": 2,
    "parent_id": null,
    "content": "Design"
  },
  "28": {
    "id": "af5f1893-01a1-4690-942a-89c4bd91848b",
    "title": "Software Detailed Design",
    "level": 2,
    "parent_id": null,
    "content": "and Unit Construction"
  },
  "29": {
    "id": "c387b1f2-72d4-4aa9-befe-2849755a1041",
    "title": "Software Unit Verification",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "30": {
    "id": "b66bbcab-53cf-4855-9205-5fc041c253d3",
    "title": "Software Integration and",
    "level": 2,
    "parent_id": null,
    "content": "Integration Test"
  },
  "31": {
    "id": "90a460a2-21e4-43ad-99da-d8ccda9cc7d1",
    "title": "Software Qualification Test",
    "level": 2,
    "parent_id": null,
    "content": "Primary Life Cycle Processes Supporting Life Cycle ProcessesOrganizational Life Cycle Processes \n  \nFigure 2 — Automotive SPICE process reference model - Overview \n  \n\n  \n \n \n \n© VDA Quality Management Center 13\n\n### Primary life cycle processes category\n\nThe primary life cycle processes category consists of processes that may be used by the customer \nwhen acquiring products from a supplier, and by the supplier when responding and delivering \nproducts to the customer including the engineering processes needed for specification, design, \ndevelopment, integration and testing. \nThe primary life cycle processes category consists of the following groups: \n• the Acquisition process group;  \n• the Supply process group;  \n• the System engineering process group; \n• the Software engineering process group. \nThe Acquisition process group (ACQ) consists of processes that are performed by the customer, or \nby the supplier when acting as a customer for its own suppliers, in order to acquire a product and/or \nservice."
  },
  "32": {
    "id": "923418a0-a97a-41d3-a99c-a9704774d749",
    "title": "Contract Agreement",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "33": {
    "id": "cc2b38f8-3857-4933-a954-6151d9012bbc",
    "title": "Supplier Monitoring",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "34": {
    "id": "1fb3eca1-5eeb-4fad-b573-c241b60fad81",
    "title": "Technical Requirements",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "35": {
    "id": "20982853-2850-4a06-9fe7-ce71aeef5d66",
    "title": "Legal and Administrative Requirements",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "36": {
    "id": "cc4b5799-35e2-4769-bbef-3c342e665b0a",
    "title": "Project Requirements",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "37": {
    "id": "c810a0aa-ae9c-4991-a961-bede3bcee0e4",
    "title": "Request for Proposals",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "38": {
    "id": "389a75ed-f071-4656-a4e6-e684623f6734",
    "title": "Supplier Qualification",
    "level": 2,
    "parent_id": null,
    "content": "Table 2 — Primary life cycle processes – ACQ process group \nThe Supply process group (SPL) consists of processes performed by the supplier in order to supply \na product and/or a service."
  },
  "39": {
    "id": "fd77c144-32de-4e1d-8993-dc8eaadae545",
    "title": "Supplier Tendering",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "40": {
    "id": "53102462-a8a0-40ae-9c11-4509b39ed5e9",
    "title": "Product Release",
    "level": 2,
    "parent_id": null,
    "content": "Table 3 — Primary life cycle processes – SPL process group \nThe System Engineering process group (SYS) consists of processes addressing the elicitation and \nmanagement of customer and internal requirements, the definition of the system architecture and \nthe integration and testing on the system level."
  },
  "41": {
    "id": "4583bd54-e77b-40b6-b7ce-635d20a70c96",
    "title": "Requirements Elicitation",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "42": {
    "id": "746c5e9f-4428-42c4-b4f9-84bbe48f0f1b",
    "title": "System Requirements Analysis",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "43": {
    "id": "55a38e58-7dbd-49e5-bbd6-081f3e33f75f",
    "title": "System Architectural Design",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "44": {
    "id": "2973c75f-e2d6-4cce-95a5-7c674c3f5e0d",
    "title": "System Integration and Integration Test",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "45": {
    "id": "0f180b5a-81ae-4a21-9966-aa189149651f",
    "title": "System Qualification Test",
    "level": 2,
    "parent_id": null,
    "content": "Table 4 — Primary life cycle processes – SYS process group \nThe Software Engineering process group (SWE) consists of processes addressing the management \nof software requirements derived from the system requirements, the development of the \n\n  \n \n \n \n© VDA Quality Management Center 14 \n \n \ncorresponding software architecture and design as well as the implementation,  integration and \ntesting of the software."
  },
  "46": {
    "id": "d3c5f4fd-56d6-4dba-9a58-2d28a379c2d2",
    "title": "Software Requirements Analysis",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "47": {
    "id": "8abe70ae-9bdc-4eb3-924f-d3fc674d1ee1",
    "title": "Software Architectural Design",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "48": {
    "id": "dbc75ee9-52ca-487d-9fa6-f42f06c03a06",
    "title": "Software Detailed Design and Unit Construction",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "49": {
    "id": "d73a7df1-c8f3-44f4-aea1-03f109e983c8",
    "title": "Software Unit Verification",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "50": {
    "id": "1e1f6e3a-a1f6-4d6a-b24d-61e0057da856",
    "title": "Software Integration and Integration Test",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "51": {
    "id": "da92fca4-9a28-445a-a1aa-2bd54720e733",
    "title": "Software Qualification Test",
    "level": 2,
    "parent_id": null,
    "content": "Table 5 — Primary life cycle processes – SWE process group\n\n### Supporting life cycle processes category\n\nThe supporting life cycle processes category consists of processes that may be employed by any of \nthe other processes at various points in the life cycle."
  },
  "52": {
    "id": "35cf1013-d281-4b48-b852-9ee58611ebee",
    "title": "Quality Assurance",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "53": {
    "id": "e6f20e69-163f-4fa9-b820-5b46b967de02",
    "title": "Verification",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "54": {
    "id": "423da31d-2115-4692-a4bb-784065132657",
    "title": "Joint Review",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "55": {
    "id": "4c2cb5dc-c36d-4785-b05b-1b2dcaf89d86",
    "title": "Documentation",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "56": {
    "id": "67fffccb-4d4c-4483-986b-a090a862a094",
    "title": "Configuration Management",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "57": {
    "id": "2d0c7fe2-f975-4348-8c1d-f9655d54aa09",
    "title": "Problem Resolution Management",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "58": {
    "id": "2fa41924-a28f-4fe5-a100-db8bc475dbd5",
    "title": "Change Request Management",
    "level": 2,
    "parent_id": null,
    "content": "Table 6 — Supporting life cycle processes - SUP process group\n\n### Organizational life cycle processes category\n\nThe o rganizational life cycle processes  category consists of processes that develop process, \nproduct, and resource assets which, when used by projects in the organization, will help the \norganization achieve its business goals. \nThe organizational life cycle processes category consists of the following groups: \n• the Management process group;  \n• the Process Improvement process group;  \n• the Reuse process group. \nThe Management process group (MAN) consists of processes that may be used by anyone who \nmanages any type of project or process within the life cycle."
  },
  "59": {
    "id": "a800ee43-d9f7-4462-96b5-b071f1157fb8",
    "title": "Project Management",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "60": {
    "id": "941bc755-e1a2-48ac-8b86-ed70205e7f33",
    "title": "Risk Management",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "61": {
    "id": "91fe45b1-ae36-4de9-8744-4a7f6924a884",
    "title": "Measurement",
    "level": 2,
    "parent_id": null,
    "content": "Table 7 — Organizational life cycle processes - MAN process group \n\n  \n \n \n \n© VDA Quality Management Center 15 \n \n \nThe Process Improvement process group (PIM) covers one process that contains practices to \nimprove the processes performed in the organizational unit."
  },
  "62": {
    "id": "83102064-1a28-47e1-80fb-b9019e5adb6f",
    "title": "Process Improvement",
    "level": 2,
    "parent_id": null,
    "content": "Table 8 — Organizational life cycle processes - PIM process group \nThe Reuse process group (REU) covers one process to systematically exploit reuse opportunities in \norganization’s reuse programs."
  },
  "63": {
    "id": "1a40ea0b-f75f-430a-877d-235ca6675360",
    "title": "Reuse Program Management",
    "level": 2,
    "parent_id": null,
    "content": "Table 9 — Organizational life cycle processes - REU process group\n\n## Measurement framework\n\nThe m easurement f ramework provides the necessary requirements and rules for the capability \ndimension. It defines a schema which enables an assessor to determine the capability level of a \ngiven process. These capability levels are defined as part of the measurement framework. \nTo enable the rating, the measurement framework provides process attributes defining a measurable \nproperty of process capability. Each process attribute is assigned to a specific capability level. The \nextent of achievement of a certain process attribute is represented by means of a rating based on a \ndefined rating scale. The rules from which an assessor can derive a final capability level for a given \nprocess are represented by a process capability level model. \nAutomotive SPICE 3.1 uses the measurement framework defined in ISO/IEC 33020:2015. \nNOTE: Text incorporated from ISO/IEC 33020 within this chapter is written in italic font and marked with a \nleft side bar.\n\n### Process capability levels and process attributes\n\nThe process capability levels and process attributes are identical to those defined in ISO/IEC 33020 \nclause 5.2. The detailed descriptions of the capability levels and the corresponding process attributes \ncan be found in chapter 5. \nProcess attributes are  features of a process that can be evaluated on a scale of achievement, \nproviding a measure of the capability of the process. They are applicable to all processes. \nA capability level is a set of process attribute(s) that work together to provide a major enhancement \nin the capability to perform a process. Each attribute addresses a specific aspect of the capability \nlevel. The levels constitute a rational way of progressing through improvement of the capability of \nany process. \nAccording to ISO/IEC 33020 there are six capability levels, incorporating nine process attributes: \nLevel 0: \nIncomplete process \nThe process is not implemented, or fails to achieve its process \npurpose. \nLevel 1: \nPerformed process \nThe implemented process achieves its process purpose \nLevel 2: \nManaged process \nThe previously described performed process is now implemented in a \nmanaged fashion (planned, monitored and adjusted) and its work \nproducts are appropriately established, controlled and maintained. \n\n  \n \n \n \n© VDA Quality Management Center 16 \n \n \nLevel 3: \nEstablished process \nThe previously described managed process is now implemented using \na defined process that is capable of achieving its process outcomes. \nLevel 4: \nPredictable process \nThe previously described established process now operates \npredictively within defined limits to achieve its process outcomes. \nQuantitative management needs are identified, measurement data are \ncollected and analyzed to identify assignable causes of variation. \nCorrective action is taken to address assignable causes of variation. \nLevel 5: \nInnovating process \nThe previously described predictable process is now continually \nimproved to respond to organizational change. \nTable 10 — Process capability levels according to ISO/IEC 33020 \nWithin this process assessment model, the determination of capability is based upon the nine \nprocess attributes (PA) defined in ISO/IEC 33020 and listed in Table 11. \nAttribute ID Process Attributes \nLevel 0: Incomplete process \nLevel 1: Performed process \nPA 1.1 Process performance process attribute \nLevel 2: Managed process \nPA 2.1 Performance management process attribute \nPA 2.2 Work product management process attribute \nLevel 3: Established process \nPA 3.1 Process definition process attribute \nPA 3.2 Process deployment process attribute \nLevel 4: Predictable process \nPA 4.1 Quantitative analysis process attribute \nPA 4.2 Quantitative control process attribute \nLevel 5: Innovating process \nPA 5.1 Process innovation process attribute \nPA 5.2 Process innovation implementation process attribute \nTable 11 — Process attributes according to ISO/IEC 33020\n\n### Process attribute rating\n\nTo support the rating of process attributes, the ISO/IEC 33020 measurement framework provides a \ndefined rating scale with an option for refinement, different rating methods and different aggregation \nmethods depending on the class of the assessment (e.g. required for organizational maturity \nassessments). \nRating scale \nWithin this process measurement framework, a process attribute is a m easureable property of \nprocess capability. A process attribute rating is a judgement of the degree of achievement of the \nprocess attribute for the assessed process. \n \nThe rating scale is defined by ISO/IEC 33020 as shown in table 12. \n\n  \n \n \n \n© VDA Quality Management Center 17 \n \n \nN Not achieved There is little or no evidence of achievement of the defined process \nattribute in the assessed process. \nP Partially achieved \nThere is some evidence of an approach to, and some achievement of, \nthe defined process attribute in the assessed process. Some aspects \nof achievement of the process attribute may be unpredictable. \nL Largely achieved \nThere is evidence of a systematic approach to, and significant \nachievement of, the defined process attribute in the assessed process. \nSome weaknesses related to this process att ribute may exist in the \nassessed process. \nF Fully achieved \nThere is evidence of a complete and systematic approach to, and full \nachievement of, the defined process attribute in the assessed process. \nNo significant weaknesses related to this process attribute exist in the \nassessed process. \nTable 12 — Rating scale according to ISO/IEC 33020 \nThe ordinal scale defined above shall be understood in terms of percentage achievement of a \nprocess attribute. \nThe corresponding percentages shall be: \n \nN Not achieved 0 to ≤ 15% achievement \nP Partially achieved > 15% to ≤ 50% achievement \nL Largely achieved > 50% to ≤ 85% achievement \nF Fully achieved > 85% to ≤ 100% achievement \nTable 13 — Rating scale percentage values according to ISO/IEC 33020 \nThe ordinal scale may be further refined for the measures P and L as defined below. \n \nP- Partially achieved: There is some evidence of an approach to, and some achievement of, \nthe defined process attribute in the assessed process. Many aspects of \nachievement of the process attribute may be unpredictable. \nP+ Partially achieved: There is some evidence of an approach to, and some achievement of, \nthe defined process attribute in the assessed process. Some aspects of \nachievement of the process attribute may be unpredictable. \nL- Largely achieved: There is evidence of a systematic approach to, and significant \nachievement of, the defined process attribute in the assessed process. \nMany weaknesses related t o this process attribute may exist in the \nassessed process. \nL+ Largely achieved: There is evidence of a systematic approach to, and significant \nachievement of, the defined process attribute in the assessed process. \nSome weaknesses related to this process attribute may exist in the \nassessed process. \nTable 14 — Refinement of rating scale according to ISO/IEC 33020 \n  \n\n  \n \n \n \n© VDA Quality Management Center 18 \n \n \nThe corresponding percentages shall be: \n \nP- Partially achieved - > 15% to ≤ 32.5% achievement \nP+ Partially achieved + > 32.5 to ≤ 50% achievement \nL- Largely achieved - > 50% to ≤ 67.5% achievement \nL+ Largely achieved + > 67.5% to ≤ 85% achievement \nTable 15 — Refined rating scale percentage values according to ISO/IEC 33020 \nRating and aggregation method \nISO/IEC 33020 provides the following definitions: \nA process outcome is the observable result of successful achievement of the process purpose. \nA process attribute outcome is the observable result of achievement of a specified process \nattribute. \nProcess outcomes and process attribute outcomes may be characterised as an intermediate step \nto providing a process attribute rating. \nWhen performing rating, the rating method employed shall be specified relevant to the class of \nassessment. The following rating methods are defined. \nThe use of rating method may vary according to the class, scope and context of an assessment. \nThe lead assessor shall decide which (if any) rating method to use. The selected rating method(s) \nshall be specified in the assessment input and referenced in the assessment report. \n \nISO/IEC 33020 provides the following 3 rating methods: \nRating method R1 \nThe approach to process attribute rating shall satisfy the following conditions: \na) Each process outcome of each process within the scope of the assessment shall be \ncharacterized for each process instance, based on validated data; \nb) Each process attribute outcome of each process attribute for each process within the scope of \nthe assessment shall be characterised for each process instance, based on validated data; \nc) Process outcome characterisations for all assessed process instances shall be aggregated to \nprovide a process performance attribute achievement rating; \nd) Process attribute outcome characterisations for all assessed process insta nces shall be \naggregated to provide a process attribute achievement rating. \nRating method R2 \nThe approach to process attribute rating shall satisfy the following conditions: \na) Each process attribute for each process within the scope of the assessment shal l be \ncharacterized for each process instance, based on validated data; \nb) Process attribute characterisations for all assessed process instances shall be aggregated to \nprovide a process attribute achievement rating. \nRating method R3 \nProcess attribute rating across assessed process instances shall be made without aggregation. \n \n  \n\n  \n \n \n \n© VDA Quality Management Center 19 \n \n \nIn principle the three rating methods defined in ISO/IEC 33020 depend on  \na) whether the rating is made only on process attribute level (Rating method 3 and 2) or – with \nmore level of detail – both on process attribute and process attribute outcome level (Rating \nmethod 1); and \nb) the type of aggregation ratings across the assessed process instances for each process \nIf a rating is performed for both process attributes and process attribute o utcomes (Rating method \n1), the result will be a process performance attribute outcome rating on level 1 and a process attribute \nachievement rating on higher levels. \nDepending on the class, scope and context of the assessment an aggregation within one proce ss \n(one-dimensional, vertical aggregation), across multiple process instances (one -dimensional, \nhorizontal aggregation) or both (two-dimensional, matrix aggregation) is performed. \nISO/IEC 33020 provides the following examples: \nWhen performing an assessment, ratings may be summarised across one or two dimensions. \nFor example, when rating a \n• process attribute for a given process, one may aggregate ratings of the associated process \n(attribute) outcomes – such an aggregation will be performed as a vertical aggre gation (one \ndimension). \n• process (attribute) outcome for a given process attribute across multiple process instances, \none may aggregate the ratings of the associated process instances for the given process \n(attribute) outcome such an aggregation will be per formed as a horizontal aggregation (one \ndimension) \n• process attribute for a given process, one may aggregate the ratings of all the process \n(attribute) outcomes for all the processes instances – such an aggregation will be performed \nas a matrix aggregation across the full scope of ratings (two dimensions) \n \nThe standard defines different methods for aggregation. Further information can be taken from \nISO/IEC 33020. \n  \n\n  \n \n \n \n© VDA Quality Management Center 20\n\n### Process capability level model\n\nThe process capability level achieved by a process shall be derived from the process attribute ratings \nfor that process according to the process capability level model defined in Table 16. \nThe process capability level model defines the rules how the achievement of each level depends on \nthe rating of the process attributes for the assessed and all lower levels. \nAs a general rule the achievement of a given level requires a largely achievement of the \ncorresponding process attributes and a full achievement of any lower lying process attribute. \n \nScale Process attribute Rating \nLevel 1 PA 1.1: Process Performance Largely \nLevel 2 \nPA 1.1: Process Performance \nPA 2.1: Performance Management \nPA 2.2: Work Product Management \nFully \nLargely \nLargely \nLevel 3 \nPA 1.1: Process Performance \nPA 2.1: Performance Management \nPA 2.2: Work Product Management \nPA 3.1: Process Definition \nPA 3.2: Process Deployment \nFully \nFully \nFully \nLargely \nLargely \nLevel 4 \nPA 1.1: Process Performance \nPA 2.1: Performance Management \nPA 2.2: Work Product Management \nPA 3.1: Process Definition \nPA 3.2: Process Deployment \nPA 4.1: Quantitative Analysis \nPA 4.2: Quantitative Control \nFully \nFully \nFully \nFully \nFully \nLargely \nLargely \nLevel 5 \nPA 1.1: Process Performance \nPA 2.1: Performance Management \nPA 2.2: Work Product Management \nPA 3.1: Process Definition \nPA 3.2: Process Deployment \nPA 4.1: Quantitative Analysis \nPA 4.2: Quantitative Control \nPA 5.1: Process Innovation \nPA 5.2: Process Innovation Implementation \nFully \nFully \nFully \nFully \nFully \nFully \nFully \nLargely \nLargely \nTable 16 — Process capability level model according to ISO/IEC 33020 \n  \n\n  \n \n \n \n© VDA Quality Management Center 21\n\n## Process assessment model\n\nThe process assessment model offers indicators in order to identify whether the process outcomes \nand the p rocess attribute outcomes (achievements) are present or absent in the instantiated \nprocesses of projects and organizational units. These indicators provide guidance for assessors in \naccumulating the necessary objective evidence to support judgments of capability. They are not \nintended to be regarded as a mandatory set of checklists to be followed. \nIn order to judge the presence or absence of process outcomes and process achievements an \nassessment obtains objective evidence. All such evidence comes from the examination of work \nproducts and reposit ory content of the assessed processes, and from testimony provided by the \nperformers and managers of the assessed processes. This evidence is mapped to the PAM \nindicators to allow establishing the correspondence to the relevant process outcomes and process  \nattribute achievements. \nThere are two types of indicators: \n• Process performance i ndicators, which apply exclusively to capability Level 1. They \nprovide an indication of the extent of fulfillment of the process outcomes \n \n• Process capability indicators, which apply to Capability Levels 2 to 5. They provide an \nindication of the extent of fulfillment of the process attribute achievements. \nAssessment indicators are used to confirm that certain practices were performed, as shown by \nevidence collected during an assessment. All such evidence comes either from the examination of \nwork products of the processes assessed, or from statements made by the performers and \nmanagers of the processes. The existence of base practices and work products provide evidence of \nthe performance of the processes associated with them. Similarly, the existence of process capability \nindicators provides evidence of process capability. \nThe evidence obtained should be recorded in a form that clearly relates to an associated indicator, \nin order that support for the assessor’s judgment can be confirmed or verified as required by ISO/IEC \n33002.\n\n### Process performance indicators\n\nTypes of process performance indicators are \n• Base practices (BP) \n• Work products (WP). \nBoth BPs and WPs relate to one or more process outcomes. Consequently, BPs and WPs are always \nprocess-specific and not generic. BPs represent activity-oriented indicators. WPs represent result -\noriented indicators. Both BP and WP are used for judging objective evidence that an assessor is to \ncollect, and accumulate, in the performance of an assessment . In that respect BPs and WPs are \nalternative indicator sets the assessor can use. \nThe PAM offers a set of work product characteristics (WPC, see Annex B) for each WP. These are \nmeant to offer a good practice and state-of-the-art knowledge guide for the assessor. Therefore, WP \nand WPC are supposed to be a quickly accessible information source during an assessment. In that \nrespect WPs and WPC s represent an example structure only. They are neither a \"strict must\" nor \nare they normative for organizations. Instead, the actual structure, form and content of concrete work \nproducts and documents for the implemented processes must be defined by the project and \norganization, respectively. The project and/or o rganization ensures that the work products are \nappropriate for the intended purpose and needs, and in relation to the development goals. \n\n  \n \n \n \n© VDA Quality Management Center 22\n\n### Process capability indicators\n\nTypes of process capability indicators are: \n• Generic Practice (GP) \n• Generic Resource (GR) \nBoth GPs and GRs relate to one or more PA Achievements. In contras t to process p erformance \nindicators, however, they are of generic type, i.e. they apply to any process. \nThe difference between GP and GR is that the former represent activity-oriented indicators while the \nlatter represent infrastructure- oriented indicators for judging objective evidence. An assessor has to \ncollect and accumulate evidence supporting process capability indicators during an assessment. In \nthat respect GPs and GRs are alternative indicators sets the assessor can use. \nIn spite of the fact that level 1 capability of a process is only characterized by the meas ure of the \nextent to which the process outcomes are achieved the measurement framework (see chapter 3.2) \nrequires each level to reveal a process a ttribute, and, thus, requires the  PAM to introduce at least \none process capability i ndicator. Therefore, the only process performance attribute for c apability \nLevel 1 (PA.1.1) has a single generic p ractice (GP 1.1.1) pointing as an edi torial reference to the \nrespective process performance indicators (see Figure 3). \nMeasurement framework\n(ISO/IEC 33020)\n• Capability levels\n• Process attributes\n• Rating\n• Scale\n• Rating method\n• Aggregation method\n• Process capability level model\nOutcomes of \nprocess 3\nProcess assessment model\n(Automotive SPICE)\n• Process capability indicators\n• Process performance indicators\nProcess reference model\n(Automotive SPICE)\n• Domain and scopes\n• Process purposes\n• Process outcomes\nCL 1\nCL 2\nCL 3\nCL 4\nCL 5 PA 5.2\nPA 5.1\nPA 4.2\nPA 4.1\nPA 3.2\nPA 3.1\nPA 2.2\nPA 2.1\nPA 1.1\nGPs, GRs\nGPs, GRs\nOutcomes of \nprocess 1\nOutcomes of \nprocess 2\nGP BPs, WPs and WPCs\n \nFigure 3 — Relationship between assessment indicators and process capability\n\n### Understanding the level of abstraction of a PAM\n\nThe term \"process\" can be understood at three levels of abstraction. Note that these levels of \nabstraction are not meant to define a strict black-or-white split, nor is it the aim to provide a scientific \nclassification schema – the message here is to understand that, in practice, when it comes to the \nterm \"process\" there are different abstraction levels, and that a PAM resides at the highest. \n \n\n  \n \n \n \n© VDA Quality Management Center 23 \n \n \nProcess Assessment Model(s)\nMethods\nExecution\nThe \"What\"\n(Goals of the process)\n(How to achieve the goals)\nThe \"How\"\n(Performing the tasks to achieve \nthe goals by using the methods)\nThe \"Doing\"\n• What is to be done\n• Why it has to be done\n• What are the technical dependencies\n• Methods, tools, templates, metrics\n• Definitions of logical order, concrete \nworkflows\n• Authority and competence definitions\n• Tailoring\n• Setup\n• Performance according to the tailored \nmethod\n \nFigure 4 — Possible levels of abstraction for the term \"process\" \nCapturing experience acquired during product development  (i.e. at the DO ING level) in order to \nshare this experience with others means creating a HOW level. However, a HOW is always specific \nto a particular context such as a company, an organizational unit, or a product line. For example, the \nHOW of a project, organizational unit, or company A is potentially not applicable as is to a project, \norganizational unit, or company B. However, both might be expected to adhere the principles \nrepresented by PAM indicators for process outcomes and process attribute achievements. These \nindicators are at the WHAT level while deciding on solutions for concrete templates, proceedings, \nand tooling etc. is left to the HOW level. \nProcess Assessment Model(s)MethodsExecution\nPerforming interviews on the actual \"Doing\", \nInvestigating work products and tool \nrepositories, …\nReading through the defined \"How\"\n1\n2\n3\n… mapping the information to the indicators ...\n… and determine the capability profile.\n \nFigure 5 — Performing a process assessment for determining process capability \n\n  \n \n \n \n© VDA Quality Management Center 24\n\n# Process reference model and performance indicators (Level 1)\n\nThe processes in the process dimension can be drawn from the Automotive SPICE process \nreference model, which is incorporated in the tables below indicated by a red bar at the left side. \nEach table related to one process in the process dimension contains the process reference model \n(indicated by a red bar) and the process performance i ndicators necessary to define the process \nassessment model. The process performance indicators consist of base practices (indicated by a \ngreen bar) and output work products (indicated by a blue bar). \n \nProcess \nreference \nmodel Process ID The individual proce sses are describ ed in terms of \nprocess name, p rocess purpose, and p rocess \noutcomes to define the Automotive SPICE process \nreference model. Additional ly a process i dentifier is \nprovided. \nProcess name \nProcess purpose \nProcess outcomes \nProcess \nperformance \nindicators \nBase practices A set of base practices for the process providing a \ndefinition of the tasks and activities needed to \naccomplish the process purpose and fulfill the process \noutcomes \nOutput work \nproducts \nA number of output work products associated with \neach process \nNOTE: Refer to Annex B for the characteristics \nassociated with each work product. \nTable 17 — Template for the process description \n  \n\n  \n \n \n \n© VDA Quality Management Center 25\n\n## Acquisition process group (ACQ)\n\n\n\n### ACQ.3 Contract Agreement\n\nProcess ID ACQ.3 \nProcess name Contract Agreement \nProcess purpose The purpose of Contract Agreement Process is to negotiate and approve \na contract/agreement with the supplier. \nProcess \noutcomes \nAs a result of successful implementation of this process:  \n1) a contract/agreement is negotiated, reviewed, approved and awarded \nto the supplier(s);  \n2) the contract/agreement clearly and unambiguously specifies the \nexpectations, responsibilities, work products/deliverables and liabilities \nof both the supplier(s) and the acquirer;  \n3) mechanisms for monitoring the capability and performance of the \nsupplier(s) and for mitigation of identified risks are reviewed and \nconsidered for inclusion in the contract conditions; and  \n4) proposers/tenderers are notified of the result of proposal/tender \nselection. \nBase practices ACQ.3.BP1: Negotiate the contract/agreement . Negotiate all relevant \naspects of the contract/agreement with the supplier. [OUTCOME 1] \nNOTE 1: Relevant aspects of the procurement may include \n• system requirements  \n• acceptance criteria and evaluation criteria \n• linkage between payment and successful completion of acceptance testing  \n• process requirements, process interfaces and joint processes. \nACQ.3.BP2: Specify rights and duties.  Unambiguously specify the \nexpectations, responsibilities, work products/deliverables and liabilities of \nthe parties in the contract/agreement. [OUTCOME 2]  \nACQ.3.BP3: Review contract/agreement for supplier capability \nmonitoring. Review and consider a mechanism for monitoring the \ncapability and performance  of the supplier for inclusion in the \ncontract/agreement conditions. [OUTCOME 3]  \nACQ.3.BP4: Review contract/agreement for risk mitigation actions.  \nReview and consider a mechanism for the mitigation of identified risk for \ninclusion in the contract/agreement conditions. [OUTCOME 3]  \nACQ.3.BP5: Approve contract/agreement. The contract/agreement is \napproved by relevant stakeholders. [OUTCOME 1] \nACQ.3.BP6: Award contract/agreement.  The contract/agreement is \nawarded to the successful proposer/tenderer. [OUTCOME 1]  \nACQ.3.BP7: Communicate result to tenderers. Notify the result of the \nproposal/tender selection to proposers/tenders. After contract award inform \nall tenderers of the decision. [OUTCOME 4] \nOutput work \nproducts \n02-00 Contract   → [OUTCOME 1, 2, 3] \n02-01 Commitment/agreement → [OUTCOME 1] \n\n  \n \n \n \n© VDA Quality Management Center 26 \n \n \n13-04 Communication record → [OUTCOME 4] \n13-05 Contract review record → [OUTCOME 1] \n13-09 Meeting support record → [OUTCOME 1]\n\n### ACQ.4 Supplier Monitoring\n\nProcess ID ACQ.4 \nProcess name Supplier Monitoring \nProcess purpose The purpose of the Supplier Monitoring Process is to track and assess the \nperformance of the supplier against agreed requirements. \nProcess \noutcomes \nAs a result of successful implementation of this process:  \n1) joint activities, as agreed between the customer and the supplier, are \nperformed as needed; \n2) all information, agreed upon for exchange, is communicated regularly \nbetween the supplier and customer; \n3) performance of the supplier is monitored against the agreements; and \n4) changes to the agreement, if needed, are negotiated between the \ncustomer and the supplier and documented in the agreement. \nBase practices ACQ.4.BP1: Agree on and maintain joint processes, joint interfaces, \nand information to be exchanged. Establish and maintain an agreement on \ninformation to be exchanged and on joint processes and joint interfaces, \nresponsibilities, type and frequency of joint activities, communications, \nmeetings, status reports and reviews. [OUTCOME 1, 2, 4] \nNOTE1: Joint processes and interfaces usually include project management, \nrequirements management, change management, configuration management, \nproblem resolution, quality assurance and customer acceptance. \nNOTE 2: Joint activities to be performed should be mutually agreed between \nthe customer and the supplier. \nNOTE 3: The term customer in this process refers to the assessed party. The \nterm supplier refers to the supplier of the assessed party. \nACQ.4.BP2: Exchange all agreed information.  Use the defined joint \ninterfaces between customer and supplier for the exchange of all agreed \ninformation. [OUTCOME 1, 2, 3] \nNOTE 4: Agreed information should include all relevant work products. \nACQ.4.BP3: Review technical development with the supplier.  Review \ndevelopment with the supp lier on the agreed regular basis, covering \ntechnical aspects, problems and risks and also track open items to closure. \n[OUTCOME 1, 3, 4]  \nACQ.4.BP4: Review progress of the supplier . Review progress of the \nsupplier regarding schedule, quality, and cost on the agreed regular basis. \nTrack open items to closure and perform risk mitigation activities. [OUTCOME \n1, 3, 4] \nACQ.4.BP5: Act to correct deviations.  Take action when agreed \nobjectives are not achieved to correct deviations from the agreed project \nplans and  to prevent reoccurrence of problems identified. Negotiate \nchanges to objectives and document them in the agreements. [OUTCOME 4] \n\n  \n \n \n \n© VDA Quality Management Center 27 \n \n \nOutput work \nproducts \n02-01 Commitment/agreement → [OUTCOME 4] \n13-01 Acceptance record  → [OUTCOME 3] \n13-04 Communication record → [OUTCOME 1, 2] \n13-09 Meeting support record → [OUTCOME 1] \n13-14 Progress status record  → [OUTCOME 2] \n13-16 Change request  → [OUTCOME 4] \n13-19 Review record   → [OUTCOME 2] \n14-02 Corrective action register → [OUTCOME 4] \n15-01 Analysis report   → [OUTCOME 3]\n\n### ACQ.11 Technical Requirements\n\nProcess ID ACQ.11 \nProcess name Technical Requirements \nProcess purpose The purpose of the Technical Requirements Process is to establish the \ntechnical requirements of the acquisition. This involves the elicitation of \nfunctional and non-functional requirements that consider the deployment \nlife cycle of the products so as to establish a technical requirement \nbaseline. \nProcess \noutcomes \nAs a result of successful implementation of this process:  \n1) the technical requirements, including environment effect evaluation, \nsafety and security requirements where appropriate, are defined and \ndeveloped to match needs and expectations; \n2) the current and evolving acquisition needs are gathered and defined; \n3) the requirements and potential solutions are communicated to all \naffected groups; \n4) a mechanism is established to incorporate changed or new \nrequirements into the established baseline; \n5) a mechanism for identifying and managing the impact of changing \ntechnology to the technical requirements is defined; and \n6) the requirements include compliance with relevant standards, \nincluding environment effect evaluation, safety and security standards \nwhere appropriate. \nBase practices ACQ.11.BP1: Elicit needs. Elicit the needs of all relevant user groups.  \n[OUTCOME 1] \nACQ.11.BP2: Define technical requirements.  Define and develop the \ntechnical requirements and potential solutions (where relevant), including \nenvironment effect evaluation, safety and security, performance, \nsupportability requirements to match the needs and expectations of the \nrelevant user groups. [OUTCOME 1] \nNOTE 1: This may include  \n• the categorization, prioritization and indication of requirements \n• the indication of mandatory requirements \n• classification of requirements into functional areas \n• using defined end user types to describe the functional requirements \nwithin an organization \n\n  \n \n \n \n© VDA Quality Management Center 28 \n \n \nACQ.11.BP3: Identify acquisition needs. Gather and define the current \nand evolving acquisition needs. [OUTCOME 2] \nACQ.11.BP4: Ensure consistency. Ensure consistency of the technical \nrequirements with the defined acquisition needs. [OUTCOME 2] \nACQ.11.BP5: Identify affected groups.  Identify all groups to which the \ntechnical requirements and potential solutions should be communicated.  \n[OUTCOME 3] \nACQ.11.BP6: Communicate to affected groups.  Communicate the \ntechnical requirements and potential solutions to all affected groups.  \n[OUTCOME 3] \nNOTE 2: To ensure a better understanding: \n• the requirements might be specified in business terms \n• simulation and exploratory prototyping techniques might be used \nACQ.11.BP7: Establish a change mechanism . Establish a mechanism \nto incorporate changed or new technical requirements into the established \nbaseline. [OUTCOME 4] \nNOTE 3: This may include analyzing, structuring and prioritizing technical \nrequirements according to their importance to the business. \nACQ.11.BP8: Track impact of changing technology.  Define a \nmechanism for identifying and managing the impact of changing \ntechnology to the technical requi rements and integrate the resulting \nconsequences into the technical requirements. [OUTCOME 5] \nACQ.11.BP9: Identify constraints and standards. Identify constraints \nand standards applicable to the technical requirements (e.g. open systems \nstandards). [OUTCOME 6] \nACQ.11.BP10: Ensure compliance of stated requirements. Ensure that \nthe technical requirements include compliance with identified relevant \nstandards, including environment effect evaluation, safety and security \nstandards where appropriate. [OUTCOME 6] \nOutput work \nproducts \n08-28 Change management plan → [OUTCOME 4] \n08-51 Technology monitoring plan → [OUTCOME 5] \n13-04 Communication record → [OUTCOME 3] \n13-17 Customer request  → [OUTCOME 1] \n13-21 Change control record  → [OUTCOME 2] \n13-24 Validation results  → [OUTCOME 6] \n14-01 Change history   → [OUTCOME 2] \n14-02 Corrective action register → [OUTCOME 2] \n14-50 Stakeholder groups list → [OUTCOME 1] \n17-00 Requirement specification → [OUTCOME 6] \n17-03 Customer requirements → [OUTCOME 6]\n\n### ACQ.12 Legal and Administrative Requirements\n\nProcess ID ACQ.12 \n\n  \n \n \n \n© VDA Quality Management Center 29 \n \n \nProcess name Legal and Administrative Requirements \nProcess purpose The purpose of the Legal and Administrative Requirements Process is to \ndefine the awarding aspects – expectations, liabilities, legal and other \nissues and which comply with national and international laws of contract. \nProcess \noutcomes \nAs a result of successful implementation of this process:  \n1) a contractual approach is defined which is compliant with relevant \nnational, international and regulatory laws, guidance and policies; \n2) an agreement, (contractual) terms and conditions are defined to \ndescribe how the supplier will meet the needs and expectations; \n3) acceptance criteria and mechanisms for handling of breaches to the \nfulfillment of contract are established; \n4) the rights of the acquirer to assume, modify or evaluate, directly or \nindirectly Intellectual Property Rights are established; \n5) warranties and service level agreements are provided for where \napplicable; \n6) provision for the suppliers to deliver other requirements (e.g. quality \nplan, escrow arrangements etc.) is defined; and \n7) recognized criteria for proprietary, regulatory and other product \nliabilities issues are established. \nBase practices ACQ.12.BP1: Identify relevant regulations. Identify relevant national, \ninternational and regulatory laws, guidance and policies. [OUTCOME 1] \nACQ.12.BP2: Consider relevant regulations . Consider identified \nrelevant laws, guidance and policy when defining a contractual approach.  \n[OUTCOME 2] \nACQ.12.BP3: Agree on (contractual) terms and conditions. \n[OUTCOME 2] \nNOTE 1: This may include \n• responsibilities of the purchaser and supplier; and the basis for payments \n• responsibility for maintenance and upgrades \n• a separate maintenance or support agreement \n• kind of payment \nACQ.12.BP4: Ensure usage of agreed terms and conditions . Ensure \nthe usage of agreed terms and conditions when describing how the supplier \nwill meet the needs and expectations. [OUTCOME 2] \nACQ.12.BP5: Establish acceptance criteria. [OUTCOME 3] \nACQ.12.BP6: Establish escalation mechanisms. Establish mechanisms \nfor handling of breaches to the fulfillment of contract. [OUTCOME 3] \nNOTE 2: This may include planning of the control of contract changes.  \nACQ.12.BP7: Establish management of intellectual property rights . \nEstablish the rights of the acquirer to assume, modify or evaluate, directly \nor indirectly, Intellectual Property Rights. [OUTCOME 4] \nACQ.12.BP8: Provide for warranties and service level agreements . \nProvide for warranties and service level agreements where applicable.  \n[OUTCOME 5] \n\n  \n \n \n \n© VDA Quality Management Center 30 \n \n \nACQ.12.BP9: Define provision for the suppliers. Define provision for the \nsuppliers to deliver other requirements such as  quality plan or escrow \narrangements. [OUTCOME 6] \nACQ.12.BP10: Establish criteria  for liability issues . Establish \nrecognized criteria for proprietary, regulatory and other product liabilit y \nissues. [OUTCOME 7] \nOutput work \nproducts \n02-00 Contract   → [OUTCOME 1-7] \n02-01 Commitment/agreement → [OUTCOME 2, 4, 5, 6, 7] \n10-00 Process description  → [OUTCOME 1, 3] \n14-02 Corrective action register → [OUTCOME 3] \n17-00 Requirement specification → [OUTCOME 1-7] \n18-01 Acceptance criteria  → [OUTCOME 3]\n\n### ACQ.13 Project Requirements\n\nProcess ID ACQ.13 \nProcess name Project Requirements \nProcess purpose The purpose of the Project Requirements Process is to specify the \nrequirements to ensure the acquisition project is performed with adequate \nplanning, staffing, directing, organizing and control over project tasks and \nactivities. \nProcess \noutcomes \nAs a result of successful implementation of this process:  \n1) consistency between financial, technical, contractual and project \nrequirements is established; \n2) requirements for the organizational, management, controlling, and \nreporting aspects of a project are defined; \n3) requirements for adequate staffing of projects by a competent team \n(e.g. resources with requisite legal, contractual, technical and project \ncompetence) with clear responsibilities and goals are defined; \n4) the needs for exchanging information between all affected parties are \nestablished; \n5) requirements for the completion and acceptance of interim work \nproducts and release of payments are established; \n6) potential risks are identified; \n7) requirements for ownership of interactions and relationships with \nsuppliers are defined; \n8) rights for use and distribution of the product by the customer and \nsupplier are defined; and \n9) support and maintenance requirements are established. \nBase practices ACQ.13.BP1: Identify relevant groups . Identify relevant \nparties/stakeholders and experts for financial, technical, contract and \nproject issues. [OUTCOME 1] \nACQ.13.BP2: Communicate with relevant groups . Communicate with \nthe relevant parties regarding the specification of financial, technical, \ncontract and project requirements. [OUTCOME 1] \n\n  \n \n \n \n© VDA Quality Management Center 31 \n \n \nACQ.13.BP3: Define organizational requirements. Define requirements \nfor the organizational aspect of the project. [OUTCOME 2] \nNOTE 1: Requirements for the organizational aspects refer to the organization \nof the people on the project e.g. who is responsible etc. at different levels. \nACQ.13.BP4: Define management requirements . Define requirements \nfor the management, controlling and reporting aspect s of the project.  \n[OUTCOME 2] \nNOTE 2: Requirements for the management, controlling and reporting aspects \nof the project may be \n• the necessity to structure the acquisition process in logical phases \n• the use of experience and skills of third parties \n• the sketch of a work breakdown structure \n• that all documentation conforms to appropriate standards, and should be \ncontractually agreed with the suppliers \n• requirements to supplier’s processes, process interfaces and joint \nprocesses \nACQ.13.BP5: Identify required competenc y. Identify required \ncompetency (e.g. legal, contractual, technical and pro ject competencies) \nfor key resources. [OUTCOME 3] \nACQ.13.BP6: Define responsibilities and goals . Define responsibilities \nand goals of the team members. [OUTCOME 3] \nACQ.13.BP7: Identify information needs . Identify information needs of \nthe relevant parties. [OUTCOME 4] \nACQ.13.BP8: Define exchange of information. Consider how exchange \nof information may be affected. [OUTCOME 4] \nNOTE 3: Techniques for supporting the exchange of information may include \nelectronic solutions, face-to-face interactions and decisions about the \nfrequency. \nACQ.13.BP9: Establish criteria for interim work products.  Establish \nrequirements for the completion and acceptance of interim work products. \n[OUTCOME 5] \nACQ.13.BP10: Establish payment requirements. Establish \nrequirements for the release of payments. [OUTCOME 5] \nNOTE 4: This may include for example the decision to link the major \nproportion of the supplier’s payment to successful completion of the \nacceptance test, the definition of supplier performance criteria and ways to \nmeasure, test and link them to the payment schedule or the decision that \npayments be made on agreed results. \nACQ.13.BP11: Identify risks . Identify risks associated with project life \ncycle and with suppliers. [OUTCOME 6] \nNOTE 5: Potential risk areas are for example stakeholder (customer, user, and \nsponsor), product (uncertainty, complexity), processes (acquisition, \nmanagement, support, and organization), resources (human, financial, time, \ninfrastructure), context (corporate context, project context, regulatory context, \nlocation) or supplier (process maturity, resources, experience). \nACQ.13.BP12: Communicate risks.  Assure that all identified risks are \ncommunicated to the relevant parties. [OUTCOME 6] \n\n  \n \n \n \n© VDA Quality Management Center 32 \n \n \nACQ.13.BP13: Define ownership of relationships. Define requirements \nfor ownership of interactions and relationships with suppliers. [OUTCOME 7] \nNOTE 6: This may include for example who has the lead on which type of \ninteraction, who maintains an open-issue-list, who are the contact persons for \nmanagement, technical and contractual issues, the frequency and type of \ninteraction, to whom the relevant information is distributed. \nACQ.13.BP14: Define rights for use and distribution.  Define rights for \nuse and distribution of the product by the customer and supplier. [OUTCOME \n8] \nNOTE 7: This may include unrestricted right of product use or delivery of \nsource code trial installation for \"sale or return\". \nACQ.13.BP15: Establish support and maintenance requirements. \n[OUTCOME 9] \nNOTE 8: This may include for example training requirements, the decision if \nsupport and maintenance should be conducted in-house or by a third party or \nthe establishment of service level agreements. \nOutput work \nproducts \n02-00 Contract   → [OUTCOME 1-9] \n13-19 Review record   → [OUTCOME 1] \n13-20 Risk action request  → [OUTCOME 6] \n17-00 Requirement specification → [OUTCOME 1-9]\n\n### ACQ.14 Request for Proposals\n\nProcess ID ACQ.14 \nProcess name Request for Proposals \nProcess purpose The purpose of the Request for Proposals Process is to prepare and \nissue the necessary acquisition requirements. The documentation will \ninclude, but not be limited to, the contract, project, finance and technical \nrequirements to be provided for use in the Call For Proposals (CFP) / \nInvitation To Tender (ITT). \nProcess \noutcomes \nAs a result of successful implementation of this process:  \n1) rules are defined for proposal/tender invitation and evaluation which \ncomply with the acquisition policy and strategy; \n2) the baseline technical and non-technical requirements are assembled \nto accompany the CFP / ITT; \n3) the agreement (contractual) terms of reference and conditions for \nCFP / ITT are established; \n4) the financial terms of reference for costs and payments for CFP / ITT \nare defined; \n5) the project terms of reference for CFP / ITT are defined; \n6) the technical terms of reference for CFP / ITT are defined; and \n7) a CFP / ITT is prepared and issued in accordance with acquisition \npolicies and which complies with relevant national, international and \nregulatory laws, requirements, and policies. \n\n  \n \n \n \n© VDA Quality Management Center 33 \n \n \nBase practices ACQ.14.BP1: Define rules for CFP / ITT. Define rules for proposal/tender \ninvitation and evaluation which comply with the acquisition policy and \nstrategy. [OUTCOME 1] \nNOTE 1: Examples are: \n• a rule that a multiphase tendering process should be used (reasonable \nwhen uncertainty is high) \n• pre-planned interactions with suppliers \n• a rule that the supplier will be informed about the evaluation criteria \n• a rule that a timetable should be stipulated to allow suppliers specified \ntimes to respond to the call for tender \n• a rule prescribing to use a two stage evaluation process (reduce a long list \nof suppliers to a short list of suppliers who are invited to tender) \nACQ.14.BP2: Assemble requirements. Assemble the baseline technical \nand non-technical requirements to accompany the CFP / ITT. [OUTCOME 2] \nNOTE 2: The goal is to provide the supplier with an in-depth understanding of \nyour business to enable him to offer the specified solution. \nACQ.14.BP3: Establish terms and conditions for CFP  / ITT. Establish \nthe agreement (contractual) terms of reference and conditions for CFP  / \nITT. [OUTCOME 3] \nACQ.14.BP4: Define financial terms.  Define the financial terms of \nreference for costs and payments for CFP / ITT. [OUTCOME 4] \nACQ.14.BP5: Define project terms. Define the project terms of reference \nfor CFP / ITT. [OUTCOME 5] \nNOTE 3: The overall purpose of this is to communicate the documented \nbusiness requirements of the acquisition to the suppliers. \nACQ.14.BP6: Define technical terms.  Define the technical terms of \nreference for CFP / ITT. [OUTCOME 6] \nACQ.14.BP7: Identify relevant regulations. Identify international and \nregulatory laws, requirements and policies which are relevant for CFP \npreparation. [OUTCOME 7] \nACQ.14.BP8: Prepare and issue a CFP / ITT. Prepare and issue a CFP \n/ ITT in accordance with acquisition policies, which complies with relevant \nnational, international and regulatory laws, requirements and policies.  \n[OUTCOME 7] \nOutput work \nproducts \n02-01 Commitment/agreement → [OUTCOME 3] \n12-01 Request for proposal  → [OUTCOME 7] \n17-00 Requirement specification → [OUTCOME 2, 4, 5, 6] \n19-11 Validation strategy  → [OUTCOME 1] \n \n  \n\n  \n \n \n \n© VDA Quality Management Center 34\n\n### ACQ.15 Supplier Qualification\n\nProcess ID ACQ.15 \nProcess name Supplier Qualification \nProcess purpose The purpose of the Supplier Qualification Process is to evaluate and \ndetermine if the potential supplier(s) have the required qualification for \nentering the proposal/tender evaluation process. In this process, the \ntechnical background, quality system, servicing, user support capabilities \netc. will be evaluated. \nProcess \noutcomes \nAs a result of successful implementation of this process:  \n1) criteria are established for qualifying suppliers; \n2) supplier capability determination is performed as necessary; \n3) the suppliers which possess required qualification are short-listed for \ntender solution(s) evaluation; \n4) any shortfalls in capability are identified and evaluated; and \n5) any corrective action required by the acquirer is evaluated and \nperformed. \nBase practices ACQ.15.BP1: Establish qualification criteria.  Establish criteria for \nqualifying suppliers. [OUTCOME 1] \nNOTE 1: This could include \n• technical background of the supplier \n• quality system on the supplier side \n• servicing \n• user support capabilities \nACQ15.BP2: Evaluate supplier. Perform supplier capability determination \nas necessary. [OUTCOME 2] \nNOTE 2: It is often required that the supplier should have an ISO 9001 and/or \nan ISO 16949 certificate. \nNOTE 3: Establish the specific target levels against which the supplier’s \ncapability will be measured. \nACQ.15.BP3: Short-list suppliers with required qualification. Short-list \nthe suppliers for tender solution(s) evaluation which possess required \nqualification. [OUTCOME 3] \nACQ.15.BP4: Evaluate an y shortfalls.  Identify and evaluate any \nshortfalls. [OUTCOME 4] \nNOTE 4: This may include developing a method for evaluating risk related to \nthe supplier or the proposed solution. \nACQ.15.BP5: Perform corrective actions.  Evaluate and perform \ncorrective action required by the acquirer. [OUTCOME 5] \nOutput work \nproducts \n14-02 Corrective action register  → [OUTCOME 5] \n14-05 Preferred suppliers register  → [OUTCOME 3] \n15-16 Improvement opportunity  → [OUTCOME 4] \n15-21 Supplier evaluation report  → [OUTCOME 2] \n18-50 Supplier qualification criteria  → [OUTCOME 1] \n\n  \n \n \n \n© VDA Quality Management Center 35\n\n## Supply process group (SPL)\n\n\n\n### SPL.1 Supplier Tendering\n\nProcess ID SPL.1 \nProcess name Supplier Tendering \nProcess purpose The purpose of the Supplier Tendering Process is to establish an \ninterface to respond to customer inquiries and requests for proposal, \nprepare and submit proposals, and confirm assignments through the \nestablishment of a relevant agreement/contract. \nProcess \noutcomes \nAs a result of successful implementation of this process:  \n1) a communication interface is established and maintained in order to \nrespond to customer inquiries and requests for proposal; \n2) requests for proposal are evaluated according to defined criteria to \ndetermine whether or not to submit a proposal; \n3) the need to undertake preliminary surveys or feasibility studies is \ndetermined; \n4) suitable staff are identified to perform the proposed work; \n5) a supplier proposal is prepared in response to the customer request; \nand \n6) formal confirmation of agreement is obtained. \nBase practices SPL.1.BP1: Establish communication interface.  A communication \ninterface is established and maintained in order to respond to customer \ninquiries or requests for proposal. [OUTCOME 1] \nSPL.1.BP2: Perform customer i nquiry screening.  Perform customer \nenquiry screening to ensure validity of contract, ensuring the right person \nis quickly identified to process the lead. [OUTCOME 1] \nSPL.1.BP3: Establish customer proposal evaluation criteria. Establish \nevaluation criteria to determine whether or not to submit a proposal based \non appropriate criteria. [OUTCOME 2] \nSPL.1.BP4: Evaluate customer request for proposal.  Requests for \nproposal are evaluated according to appropriate criteria. [OUTCOME 2] \nSPL.1.BP5: Determine need for preliminary pre -studies. Determine \nneed for p reliminary pre -studies to ensure that a firm quotation can be \nmade based on available requirements. [OUTCOME 3] \nSPL.1.BP6: Identify and nominate staff. Identify and nominate staff with \nappropriate competence for the assignment. [OUTCOME 4] \nSPL.1.BP7: Prepare supplier proposal response.  A supplier proposal \nresponse is prepared in response to the customer request. [OUTCOME 5] \nSPL.1.BP8: Establish confirmation of agreement. Formally confirm the \nagreement to protect the interests of customer and supplier. [OUTCOME 6] \nNOTE.1: The nature of the commitment should be agreed and evidenced in \nwriting. Only authorized signatories should be able to commit to a contract.  \nOutput work \nproducts \n02-01 Commitment/agreement → [OUTCOME 6] \n\n  \n \n \n \n© VDA Quality Management Center 36 \n \n \n08-12 Project plan    → [OUTCOME 4] \n12-04 Supplier proposal response → [OUTCOME 5] \n13-04 Communication record → [OUTCOME 1, 6] \n13-15 Proposal review record → [OUTCOME 3, 4] \n13-19 Review record   → [OUTCOME 2]\n\n### SPL.2 Product Release\n\nProcess ID SPL.2 \nProcess name Product Release \nProcess purpose The purpose of the Product Release Process is to control the release of a \nproduct to the intended customer. \nProcess \noutcomes \nAs a result of successful implementation of this process:  \n1) the contents of the product release are determined; \n2) the release is assembled from configured items; \n3) the release documentation is defined and produced; \n4) the release delivery mechanism and media are determined; \n5) release approval is effected against defined criteria; \n6) the product release is made available to the intended customer; and \n7) confirmation of release is obtained. \nBase practices SPL.2.BP1: Define the functional content of releases. Establish a plan \nfor releases that identifies the functionality to be included in each release. \n[OUTCOME 1, 3] \nNOTE 1: The plan should point out which application parameters influencing \nthe identified functionality are effective for which release. \nSPL.2.BP2: Define release products.  The products associated with the \nrelease are defined. [OUTCOME 1] \nNOTE 2: The release products may include programming tools where these \nare stated. In automotive terms a release may be associated with a sample \ne.g. A, B, C. \nSPL.2.BP3: Establish a product release classification and numbering \nscheme. A product release classification and numbering sch eme are \nestablished based upon the intended purpose and expectations of the \nrelease(s). [OUTCOME 2] \nNOTE 3: A release numbering implementation may include \n• the major release number \n• the feature release number \n• the defect repair number \n• the alpha or beta release \n• the iteration within the alpha or beta release \nSPL.2.BP4: Define the build activities and build environment.  A \nconsistent build process is established and maintained. [OUTCOME 2] \nNOTE 4: A specified and consistent build environment should be used by all \nparties.  \n\n  \n \n \n \n© VDA Quality Management Center 37 \n \n \nSPL.2.BP5: Build the release from configured items. The release is built \nfrom configured items to ensure integrity. [OUTCOME 2] \nNOTE 5: Where relevant the software release should be programmed onto the \ncorrect hardware revision before release. \nSPL2.BP6: Communicate the type, service level and duration of \nsupport for a release. The type, service level and duration of support for \na release are identified and communicated. [OUTCOME 3] \nSPL.2.BP7: Determine the delivery media type for the releas e. The \nmedia type for product delivery is determined in accordance with the needs \nof the customer. [OUTCOME 4] \nNOTE 6: The media type for delivery may be intermediate (placed on an \nadequate media and delivered to customer), or direct (such as delivered in \nfirmware as part of the package) or a mix of both. The release may be \ndelivered electronically by placement on a server. The release may also need \nto be duplicated before delivery. \nSPL.2.BP8: Identify the packaging for the release media.  The \npackaging for different types of media is identified. [OUTCOME 4] \nNOTE 7: The packaging for certain types of media may need physical or \nelectronic protection for instance specific encryption techniques. \nSPL.2.BP9: Define and produce the product release \ndocumentation/release notes. Ensure that all documentation to support \nthe release is produced, reviewed, approved and available. [OUTCOME 3] \nSPL.2.BP10: Ensure product release approval before delivery. Criteria \nfor the product release are satisfied before release takes pla ce. [OUTCOME \n5] \nSPL.2.BP11: Ensure consistency. Ensure consistency between software \nrelease number, paper label and EPROM-Label (if relevant). [OUTCOME 5] \nSPL.2.BP12: Provide a release note.  A release is supported by \ninformation detailing key characteristics of the release. [OUTCOME 6] \nNOTE 8: The release note may include an introduction, the environmental \nrequirements, installation procedures, product invocation, new feature \nidentification and a list of defect resolutions, known defects and workarounds.  \nSPL.2.BP13: Deliver the release to the intended customer. The product \nis delivered to the intended customer with positive confirmation of receipt.  \n[OUTCOME 6, 7] \nNOTE 9: Confirmation of receipt may be achieved by hand, electronically, by \npost, by telephone or through a distribution service provider. \nNOTE 10: These practices are typically supported by the SUP.8 Configuration \nManagement Process. \nOutput work \nproducts \n08-16 Release plan    → [OUTCOME 1, 3] \n11-03 Product release information  → [OUTCOME 1, 3, 4, 6] \n11-04 Product release package  → [OUTCOME 2, 3, 6] \n11-07 Temporary solution   → [OUTCOME 6] \n13-06 Delivery record    → [OUTCOME 6,7] \n13-13 Product release approval record → [OUTCOME 5] \n\n  \n \n \n \n© VDA Quality Management Center 38 \n \n \n15-03 Configuration status report  → [OUTCOME 2] \n18-06 Product release criteria  → [OUTCOME 5, 7]\n\n## System engineering process group (SYS)\n\n\n\n### SYS.1 Requirements Elicitation\n\nProcess ID SYS.1 \nProcess name Requirements Elicitation \nProcess purpose The purpose of the Requirements Elicitation Process is to gather, \nprocess, and track evolving stakeholder needs and requirements \nthroughout the lifecycle of the product and/or service so as to establish a \nrequirements baseline that serves as the basis for defining the needed \nwork products. \nProcess \noutcomes \nAs a result of successful implementation of this process: \n1) continuing communication with the stakeholder is established; \n2) agreed stakeholder requirements are defined and baselined; \n3) a change mechanism is established to evaluate and incorporate \nchanges to stakeholder requirements into the baselined requirements \nbased on changing stakeholder needs; \n4) a mechanism is established for continuous monitoring of stakeholder \nneeds; \n5) a mechanism is established for ensuring that customers can easily \ndetermine the status and disposition of their requests; and  \n6) changes arising from changing technology and stakeholder needs are \nidentified, the associated risks assessed and their impact managed. \nBase practices SYS.1.BP1: Obtain stakeholder requirements and requests.  Obtain \nand define stakeholder requirements and requests through direct \nsolicitation of customer input and through review of customer business \nproposals (where relevant), target operating and hardware environment, \nand other documents bearing on customer requirements. [OUTCOME 1, 4] \nNOTE 1: Requirements elicitation may involve the customer and the supplier.  \nNOTE 2: The agreed stakeholder requirements and evaluation of any change \nmay be based on feasibility studies and/or cost and time analyzes.  \nNOTE 3: The information needed to keep traceability for each customer \nrequirement has to be gathered and documented.  \nSYS.1.BP2: Understand stakeholder expectations.  Ensure that both \nsupplier and customer understand each requirement in the same way. \n[OUTCOME 2]  \nNOTE 4: Reviewing the requirements and requests with the customer supports \na better understanding of customer needs and expectations. Refer to the \nprocess SUP.4 Joint Review.  \nSYS.1.BP3: Agree on requirements.  Obtain an explicit agreement from \nall relevant parties to work on these requirements. [OUTCOME 2] \n\n  \n \n \n \n© VDA Quality Management Center 39 \n \n \nSYS.1.BP4: Establish stakeholder requirements baseline.  Formalize \nthe stakeholder's requirements and establish them as a baseline for project \nuse and monitoring against stakeholder needs. The supplier should \ndetermine the requirements not s tated by the stakeholder but necessary \nfor specified and intended use and include them in the baseline. [OUTCOME \n2, 3] \nSYS.1.BP5: Manage stakeholder requirements changes.  Manage all \nchanges made to the stakeholder requirements against the stakeholder \nrequirements baseline to ensure enhancements resulting from changing \ntechnology and stakeholder needs are identified and that those who are \naffected by the changes are able to assess the impact and risks and initiate \nappropriate change control and mitigation actions. [OUTCOME 3, 6] \nNOTE 5: Requirements change may arise from different sources as for \ninstance changing technology and stakeholder needs, legal constraints.  \nNOTE 6: An information management system may be needed to manage, \nstore and reference any information gained and needed in defining agreed \nstakeholder requirements.  \nSYS.1.BP6: Establish customer -supplier query communication \nmechanism. Provide means by which the customer can be aware of the \nstatus and disposition of their requirements changes and th e supplier can \nhave the ability to communicate necessary information, including data, in a \ncustomer-specified language and format. [OUTCOME 5] \nNOTE 7: Any changes should be communicated to the customer before \nimplementation in order that the impact, in terms of time, cost and functionality \ncan be evaluated. \nNOTE 8: This may include joint meetings with the customer or formal \ncommunication to review the status for their requirements and requests; Refer \nto the process SUP.4 Joint Review. \nNOTE 9: The formats of the information communicated by the supplier may \ninclude computer-aided design data and electronic data exchange.  \nOutput work \nproducts \n08-19 Risk management plan → [OUTCOME 6] \n08-20 Risk mitigation plan  → [OUTCOME 6] \n13-04 Communication record → [OUTCOME 1, 4] \n13-19 Review record   → [OUTCOME 4, 5] \n13-21 Change control record  → [OUTCOME 3, 4] \n15-01 Analysis report   → [OUTCOME 2, 3, 6] \n17-03 Stakeholder Requirements → [OUTCOME 1, 2]\n\n### SYS.2 System Requirements Analysis\n\nProcess ID SYS.2 \nProcess name System Requirements Analysis \nProcess purpose The purpose of the System Requirements Analysis Process is to \ntransform the defined stakeholder requirements into a set of system \nrequirements that will guide the design of the system. \n\n  \n \n \n \n© VDA Quality Management Center 40 \n \n \nProcess \noutcomes \nAs a result of successful implementation of this process: \n1) a defined set of system requirements is established; \n2) system requirements are categorized and analyzed for correctness \nand verifiability; \n3) the impact of system requirements on the operating environment is \nanalyzed; \n4) prioritization for implementing the system requirements is defined; \n5) the system requirements are updated as needed; \n6) consistency and bidirectional traceability are established between \nstakeholder requirements and system requirements; \n7) the system requirements are evaluated for cost, schedule and \ntechnical impact; and \n8) the system requirements are agreed and communicated to all affected \nparties. \nBase practices SYS.2.BP1: Specify system requirements.  Use the stakeholder \nrequirements and changes to the stakeholder requirements to identify the \nrequired functions and capabilities of the system. Specify functional and \nnon-functional system requirements in a system requirements \nspecification. [OUTCOME 1, 5, 7] \nNOTE 1: Application parameter influencing functions and capabilities are part \nof the system requirements.  \nNOTE 2: For changes to the stakeholder's requirements SUP.10 applies . \nSYS.2.BP2: Structure system requirements.  Structure the system \nrequirements in the system requirements specification by e.g. \n• grouping to project relevant clusters, \n• sorting in a logical order for the project, \n• categorizing based on relevant criteria for the project, \n• prioritizing according to stakeholder needs. \n[OUTCOME 2, 4] \nNOTE 3: Prioritizing typically includes the assignment of functional content to \nplanned releases. Refer to SPL.2.BP1. \nSYS.2.BP3: Analyze system requirements. Analyze the specified system \nrequirements including their interdependencies to ensure correctness, \ntechnical feasibility  and verifiability, and to support risk identification. \nAnalyze the impact on cost, schedule and the technical impact. [OUTCOME \n1, 2, 7] \nNOTE 4: The analysis of impact on cost and schedule supports the adjustment \nof project estimates. Refer to MAN.3.BP5. \nSYS.2.BP4: Analyze the impact on the operating environment. Identify \nthe interfaces between the specified system and other elements of the \noperating environment. Analyze the impact that the system requirements \nwill have on these interfaces and the operating environment. [OUTCOME 3, 7] \nSYS.2.BP5: Develop verification criteria. Develop the verification criteria \nfor each system requirement that define the qualitative and quantitative \nmeasures for the verification of a requirement. [OUTCOME 2, 7] \nNOTE 5: Verification criteria demonstrate that a requirement can be verified \nwithin agreed constraints and is typically used as the input for the development \n\n  \n \n \n \n© VDA Quality Management Center 41 \n \n \nof the system test cases or other verification measures that ensures \ncompliance with the system requirements. \nNOTE 6: Verification which cannot be covered by testing is covered by SUP.2.  \nSYS.2.BP6: Establish bidirectional traceability.  Establish bidirectional \ntraceability between stakeholder requirements and system requirements . \n[OUTCOME 6] \nNOTE 7: Bidirectional traceability supports coverage, consistency and impact \nanalysis. \nSYS.2.BP7: Ensure consistency . Ensure consistency between \nstakeholder requirements and system requirements. [OUTCOME 6] \nNOTE 8: Consistency is supported by bidirectional traceability and can be \ndemonstrated by review records.  \nSYS.2.BP8: Communicate agreed system requirements. Communicate \nthe agreed system requirements and updates to system requirements to all \nrelevant parties. [OUTCOME 8] \nOutput work \nproducts \n13-04 Communication record  → [OUTCOME 8] \n13-19 Review record    → [OUTCOME 6] \n13-21 Change control record   → [OUTCOME 1] \n13-22 Traceability record   → [OUTCOME 6] \n15-01 Analysis report    → [OUTCOME 2, 3, 4, 7] \n17-08 Interface requirements  specification → [OUTCOME 1, 3] \n17-12 System requirements specification → [OUTCOME 1, 5] \n17-50 Verification criteria   → [OUTCOME 2]\n\n### SYS.3 System Architectural Design\n\nProcess ID SYS.3 \nProcess name System Architectural Design \nProcess purpose The purpose of the System Architectural Design Process is to establish a \nsystem architectural design and identify which system requirements are to \nbe allocated to which elements of the system, and to evaluate the system \narchitectural design against defined criteria. \nProcess \noutcomes \nAs a result of successful implementation of this process: \n1) a system architectural design is defined that identifies the elements of \nthe system; \n2) the system requirements are allocated to the elements of the system; \n3) the interfaces of each system element are defined; \n4) the dynamic behavior of the system elements is defined; \n5) consistency and bidirectional traceability are established between \nsystem requirements and system architectural design; and \n6) the system architectural design is agreed and communicated to all \naffected parties. \nBase practices SYS.3.BP1: Develop system architectural design.  Develop and \ndocument the system architectural design that specifies the elements of the \n\n  \n \n \n \n© VDA Quality Management Center 42 \n \n \nsystem with respect to functional and non-functional system requirements. \n[OUTCOME 1] \nNOTE 1: The development of system architectural design typically includes the \ndecomposition into elements across appropriate hierarchical levels. \nSYS.3.BP2: Allocate system r equirements. Allocate the system \nrequirements to the elements of the system architectural design. [OUTCOME \n2] \nSYS.3.BP3: Define interfaces of system elements. Identify, develop and \ndocument the interfaces of each system element. [OUTCOME 3] \nSYS.3.BP4: Describe dynamic behavior.  Evaluate and document the \ndynamic behavior of the interaction between system elements. [OUTCOME 4] \nNOTE 2: Dynamic behavior is determined by operating modes (e.g. start-up, \nshutdown, normal mode, calibration, diagnosis, etc.). \nSYS.3.BP5: Evaluate alternative system architectures.  Define \nevaluation criteria for the architecture. Evaluate alternative system \narchitectures according to the defined criteria. Record the rationale for the \nchosen system architecture. [OUTCOME 1] \nNOTE 3: Evaluation criteria may include quality characteristics (modularity, \nmaintainability, expandability, scalability, reliability, security realization and \nusability) and results of make-buy-reuse analysis. \nSYS.3.BP6: Establish bidirectional traceability.  Establish bidirectional \ntraceability between system requirements and elem ents of the system \narchitectural design. [OUTCOME 5] \nNOTE 4: Bidirectional traceability covers allocation of system requirements to \nthe elements of the system architectural design. \nNOTE 5: Bidirectional traceability supports coverage, consistency and impact \nanalysis. \nSYS.3.BP7: Ensure consistency.  Ensure consistency between system \nrequirements and the system architectural design. [OUTCOME 1, 2, 5, 6] \nNOTE 6: Consistency is supported by bidirectional traceability and can be \ndemonstrated by review records. \nNOTE 7: System requirements typically include system architectural \nrequirements. Refer to BP5. \nSYS.3.BP8: Communicate agreed system architectural design.  \nCommunicate the agreed system architectural design and updates to \nsystem architectural design to all relevant parties. [OUTCOME 6] \nOutput work \nproducts \n04-06 System architectural design  → [OUTCOME 1, 2, 3, 4, 5] \n13-04 Communication record  → [OUTCOME 6] \n13-19 Review record    → [OUTCOME 5] \n13-22 Traceability record   → [OUTCOME 5] \n17-08 Interface requirements specification → [OUTCOME 3] \n \n  \n\n  \n \n \n \n© VDA Quality Management Center 43\n\n### SYS.4 System Integration and Integration Test\n\nProcess ID SYS.4 \nProcess name System Integration and Integration Test \nProcess purpose The purpose of the System Integration and Integration Test Process is to \nintegrate the system items to produce an integrated system consistent \nwith the system architectural design and to ensure that the system items \nare tested to provide evidence for compliance of the integrated system \nitems with the system architectural design, including the interfaces \nbetween system items. \nProcess \noutcomes \nAs a result of successful implementation of this process: \n1) a system integration strategy consistent with the project plan, the \nrelease plan and the system architectural design is developed to \nintegrate the system items; \n2) a system integration test strategy including the regression test \nstrategy is developed to test the system item interactions; \n3) a specification for system integration test according to the system \nintegration test strategy is developed that is suitable to provide \nevidence for compliance of the integrated system items with the \nsystem architectural design, including the interfaces between system \nitems; \n4) system items are integrated up to a complete integrated system \naccording to the integration strategy; \n5) test cases included in the system integration test specification are \nselected according to the system integration test strategy and the \nrelease plan; \n6) system item interactions are tested using the selected test cases and \nthe results of system integration testing are recorded; \n7) consistency and bidirectional traceability between the elements of the \nsystem architectural design and test cases included in the system \nintegration test specification and bidirectional traceability between test \ncases and test results is established; and \n8) results of the system integration test are summarized and \ncommunicated to all affected parties. \nBase practices SYS.4.BP1: Develop system integration strategy. Develop a strategy for \nintegrating the system items consistent with the project plan and the \nrelease plan. Identify system items based on the system architectural \ndesign and define a sequence for integrating them. [OUTCOME 1] \nSYS.4.BP2: Develop system inte gration test strategy including \nregression test strategy.  Develop a strategy for testing the integrated \nsystem items following the integration strategy. This includes a regression \ntest strategy for re -testing integrated system items if a system item is \nchanged. [OUTCOME 2]  \nSYS.4.BP3: Develop specification for system integration test. Develop \nthe test specification for system integration test including the test cases for \neach integration step of a system item according to the system integration \ntest strategy. The test specification shall be suitable to provide evidence for \ncompliance of the integrated system items with the system architectural \ndesign. [OUTCOME 3] \n\n  \n \n \n \n© VDA Quality Management Center 44 \n \n \nNOTE 1: The interface descriptions between system elements are an input for \nthe system integration test cases. \nNOTE 2: Compliance to the architectural design means that the specified \nintegration tests are suitable to prove that the interfaces between the system \nitems fulfill the specification given by the system architectural design. \nNOTE 3: The system integration test cases may focus on \n• the correct signal flow between system items \n• the timeliness and timing dependencies of signal flow between system \nitems  \n• the correct interpretation of signals by all system items using an interface  \n• the dynamic interaction between system items \nNOTE 4: The system integration test may be supported using simulation of the \nenvironment (e.g. Hardware-in-the-Loop simulation, vehicle network \nsimulations, digital mock-up). \nSYS.4.BP4: Integrate system items.  Integrate the sy stem items to an \nintegrated system according to the system integration strategy. [OUTCOME 4] \nNOTE 5: The system integration can be performed step wise integrating \nsystem items (e.g. the hardware elements as prototype hardware, peripherals \n(sensors and actuators), the mechanics and integrated software) to produce a \nsystem consistent with the system architectural design. \nSYS.4.BP5: Select test cases.  Select test cases from the system \nintegration test specification. The selection of test cases shall have \nsufficient coverage according to the system integration test strategy and \nthe release plan. [OUTCOME 5] \nSYS.4.BP6: Perform system integration test . Perform the system \nintegration test using the selected test cases. Record the integration test \nresults and logs. [OUTCOME 6] \nNOTE 6: See SUP.9 for handling of non-conformances.  \nSYS.4.BP7: Establish bidirectional traceability.  Establish bidirectional \ntraceability between elements of the system architectural design and test \ncases included in the system integration test specification.  \nEstablish bidirectional traceability between test cases included in the \nsystem integration test  specification and system integration test results. \n[OUTCOME 7] \nNOTE 7: Bidirectional traceability supports coverage, consistency and impact \nanalysis.  \nSYS.4.BP8: Ensure consistency. Ensure consistency between elements \nof the system architectural design an d test cases included in the system \nintegration test specification. [OUTCOME 7] \nNOTE 8: Consistency is supported by bidirectional traceability and can be \ndemonstrated by review records. \nSYS.4.BP9: Summarize and communicate results.  Summarize the \nsystem int egration test results and communicate them to all affected \nparties. [OUTCOME 8] \nNOTE 9: Providing all necessary information from the test case execution in a \nsummary enables other parties to judge the consequences.  \n\n  \n \n \n \n© VDA Quality Management Center 45 \n \n \nOutput work \nproducts \n08-50 Test specification  → [OUTCOME 3, 5] \n08-52 Test plan   → [OUTCOME 1, 2] \n11-06 System    → [OUTCOME 4] \n13-04 Communication record → [OUTCOME 8] \n13-19 Review record   → [OUTCOME 7] \n13-22 Traceability record  → [OUTCOME 7] \n13-50 Test result   → [OUTCOME 6, 8]\n\n### SYS.5 System Qualification Test\n\nProcess ID SYS.5 \nProcess name System Qualification Test \nProcess purpose The purpose of the System Qualification Test Process is to ensure that the \nintegrated system is tested to provide evidence for compliance with the  \nsystem requirements and that the system is ready for delivery. \nProcess \noutcomes \nAs a result of successful implementation of this process: \n1) a system qualification test strategy including regression test strategy \nconsistent with the project plan and release plan is developed to test \nthe integrated system; \n2) a specification for system qualification test of the integrated system \naccording to the system qualification test strategy is developed that is \nsuitable to provide evidence for compliance with the system \nrequirements; \n3) test cases included in the system qualification test specification are \nselected according to the system qualification test strategy and the \nrelease plan; \n4) the integrated system is tested using the selected test cases and the \nresults of system qualification test are recorded; \n5) consistency and bidirectional traceability are established between \nsystem requirements and test cases included in the system \nqualification test specification and between test cases and test results; \nand \n6) results of the system qualification test are summarized and \ncommunicated to all affected parties. \nBase practices SYS.5.BP1: Develop system qualification test strategy including \nregression test strategy. Develop a strategy for system qualification test \nconsistent with the project plan and the release plan. This includes a \nregression test strategy for re-testing the integrated system if a system item \nis changed. [OUTCOME 1] \nSYS.5.BP2: Develop specification for system qualification test.  \nDevelop the specification for system qualification test including test cases \nbased on the verification criteria according to the system qualification test \nstrategy. The test specification shall be suitable to provide evidence for \ncompliance of the integrated system with the system requirements . \n[OUTCOME 2]  \n\n  \n \n \n \n© VDA Quality Management Center 46 \n \n \nSYS.5.BP3: Select test cases.  Select test cases from the system \nqualification test specification. The selection of test cases shall have \nsufficient coverage according to the system qualification test strategy and \nthe release plan. [OUTCOME 3] \nSYS.5.BP4: Test integrated system. Test the integrated system using the \nselected test cases. Record the system qualification test results and logs. \n[OUTCOME 4] \nNOTE 1: See SUP.9 for handling of non-conformances. \nSYS.5.BP5: Establish bidirectional tracea bility. Establish bidirectional \ntraceability between system requirements and test cases included in the \nsystem qualification test specification. Establish bidirectional traceability \nbetween test cases included in the system qualification test specification  \nand system qualification test results. [OUTCOME 5]  \nNOTE 2: Bidirectional traceability supports coverage, consistency and impact \nanalysis. \nSYS.5.BP6: Ensure consistency.  Ensure consistency between system \nrequirements and test cases included in the system qualification test \nspecification. [OUTCOME 5] \nNOTE 3: Consistency is supported by bidirectional traceability and can be \ndemonstrated by review records. \nSYS.5.BP7: Summarize and communicate results.  Summarize the \nsystem qualification test results and commun icate them to all affected \nparties. [OUTCOME 6] \nNOTE 4: Providing all necessary information from the test case execution in a \nsummary enables other parties to judge the consequences. \nOutput work \nproducts \n08-50 Test specification  → [OUTCOME 2, 3] \n08-52 Test plan   → [OUTCOME 1] \n13-04 Communication record → [OUTCOME 6] \n13-19 Review record   → [OUTCOME 5] \n13-22 Traceability record  → [OUTCOME 5] \n13-50 Test result   → [OUTCOME 4, 6] \n \n  \n\n  \n \n \n \n© VDA Quality Management Center 47\n\n## Software engineering process group (SWE)\n\n\n\n### SWE.1 Software Requirements Analysis\n\nProcess ID SWE.1 \nProcess name Software Requirements Analysis \nProcess purpose The purpose of the Software Requirements Analysis Process is to \ntransform the software related parts of the system requirements into a set \nof software requirements. \nProcess \noutcomes \nAs a result of successful implementation of this process: \n1) the software requirements to be allocated to the software elements of \nthe system and their interfaces are defined; \n2) software requirements are categorized and analyzed for correctness \nand verifiability; \n3) the impact of software requirements on the operating environment is \nanalyzed; \n4) prioritization for implementing the software requirements is defined; \n5) the software requirements are updated as needed; \n6) consistency and bidirectional traceability are established between \nsystem requirements and software requirements; and consistency and \nbidirectional traceability are established between system architectural \ndesign and software requirements; \n7) the software requirements are evaluated for cost, schedule and \ntechnical impact; and  \n8) the software requirements are agreed and communicated to all \naffected parties. \nBase practices SWE.1.BP1: Specify software requirements.  Use the system \nrequirements and the system architecture and changes to system \nrequirements and architecture to identify the required functions and \ncapabilities of the software. Specify functional and non-functional software \nrequirements in a software requirements specification. [OUTCOME 1, 5, 7] \nNOTE 1: Application parameter influencing functions and capabilities are part \nof the system requirements. \nNOTE 2: In case of software development only, the system requirements and \nthe system architecture refer to a given operating environment (see also \nnote 5). In that case, stakeholder requirements should be used as the basis for \nidentifying the required functions and capabilities of the software as well as for \nidentifying application parameters influencing software functions and \ncapabilities. \nSWE.1.BP2: Structure software requirements.  Structure the software \nrequirements in the software requirements specification by e.g. \n• grouping to project relevant clusters,  \n• sorting in a logical order for the project,  \n• categorizing based on relevant criteria for the project, \n• prioritizing according to stakeholder needs.  \n[OUTCOME 2, 4] \nNOTE 3: Prioritizing typically includes the assignment of software content to \nplanned releases. Refer to SPL.2.BP1. \n\n  \n \n \n \n© VDA Quality Management Center 48 \n \n \nSWE.1.BP3: Analyze software requirements. Analyze the specified \nsoftware requirements including their interdependencies to ensure \ncorrectness, technical feasibility and verifiability,  and to support risk \nidentification. Analyze the impact on cost, schedule and the technical \nimpact. [OUTCOME 2, 7] \nNOTE 4: The analysis of impact on cost and schedule supports the adjustment \nof project estimates. Refer to MAN.3.BP5. \nSWE.1.BP4: Analyze the impact on the operating environment.  \nAnalyze the impact that the software requirements will have on i nterfaces \nof system elements and the operating environment. [OUTCOME 3, 7] \nNOTE 5: The operating environment is defined as the system in which the \nsoftware executes (e.g. hardware, operating system, etc.). \nSWE.1.BP5: Develop verification criteria.  Develop the verification \ncriteria for each software requirement that define the qualitative and \nquantitative measures for the verification of a requirement. [OUTCOME 2, 7] \nNOTE 6: Verification criteria demonstrate that a requirement can be verified \nwithin agreed constraints and is typically used as the input for the development \nof the software test cases or other verification measures that should \ndemonstrate compliance with the software requirements. \nNOTE 7: Verification which cannot be covered by testing is covered by SUP.2. \nSWE.1.BP6: Establish bidirectional traceability. Establish bidirectional \ntraceability between system requirements and software requirements. \nEstablish bidirectional traceability between the system architecture and \nsoftware requirements. [OUTCOME 6] \nNOTE 8: Redundancy should be avoided by establishing a combination of \nthese approaches that covers the project and the organizational needs. \nNOTE 9: Bidirectional traceability supports coverage, consistency and impact \nanalysis. \nSWE.1.BP7: Ensure consi stency. Ensure consistency between system \nrequirements and software requirements. Ensure consistency between the \nsystem architecture and software requirements. [OUTCOME 6] \nNOTE 10: Consistency is supported by bidirectional traceability and can be \ndemonstrated by review records. \nNOTE 11: In case of software development only, the system requirements and \nsystem architecture refer to a given operating environment (see also note 2). \nIn that case, consistency and bidirectional traceability have to be ensured \nbetween stakeholder requirements and software requirements. \nSWE.1.BP8: Communicate agreed software requirements.  \nCommunicate the agreed software requirements and updates to software \nrequirements to all relevant parties. [OUTCOME 8] \nOutput work \nproducts \n13-04 Communication record  → [OUTCOME 8] \n13-19 Review record    → [OUTCOME 6] \n13-21 Change control record   → [OUTCOME 5, 7] \n13-22 Traceability record   → [OUTCOME 1, 6] \n15-01 Analysis report    → [OUTCOME 2, 3, 4, 7] \n\n  \n \n \n \n© VDA Quality Management Center 49 \n \n \n17-08 Interface requirements specification → [OUTCOME 1, 3] \n17-11 Software requirements specification → [OUTCOME 1] \n17-50 Verification criteria   → [OUTCOME 2]\n\n### SWE.2 Software Architectural Design\n\nProcess ID SWE.2 \nProcess name Software Architectural Design \nProcess purpose The purpose of the Software Architectural Design Process is to establish \nan architectural design and to identify which software requirements are to \nbe allocated to which elements of the software, and to evaluate the \nsoftware architectural design against defined criteria. \nProcess \noutcomes \nAs a result of successful implementation of this process: \n1) a software architectural design is defined that identifies the elements \nof the software; \n2) the software requirements are allocated to the elements of the \nsoftware;  \n3) the interfaces of each software element are defined; \n4) the dynamic behavior and resource consumption objectives of the \nsoftware elements are defined; \n5) consistency and bidirectional traceability are established between \nsoftware requirements and software architectural design; and \n6) the software architectural design is agreed and communicated to all \naffected parties. \nBase practices SWE.2.BP1: Develop software architectural design.  Develop and \ndocument the software architectural design that specifies the elements of \nthe software with respect to functional and non -functional software \nrequirements. [OUTCOME 1] \nNOTE 1: The software is decomposed into elements across appropriate \nhierarchical levels down to the software components (the lowest level \nelements of the software architectural design) that are described in the \ndetailed design. \nSWE.2.BP2: Allocate software requirements.  Allocate the software \nrequirements to the elements of the so ftware architectural design. \n[OUTCOME 2] \nSWE.2.BP3: Define interfaces of software elements.  Identify, develop \nand document the interfaces of each software element. [OUTCOME 3] \nSWE.2.BP4: Describe dynamic behavior.  Evaluate and document the \ntiming and dynamic interaction of software elements to meet the required \ndynamic behavior of the system. [OUTCOME 4] \nNOTE 2: Dynamic behavior is determined by operating modes (e.g. start-up, \nshutdown, normal mode, calibration, diagnosis, etc.), processes and process \nintercommunication, tasks, threads, time slices, interrupts, etc. \nNOTE 3: During evaluation of the dynamic behavior the target platform and \npotential loads on the target should be considered. \n\n  \n \n \n \n© VDA Quality Management Center 50 \n \n \nSWE.2.BP5: Define resource consumption objectives.  Determine and \ndocument the resource consumption objectives for all relevant elements of \nthe software architectural design on the appropriate hierarchical level. \n[OUTCOME 4] \nNOTE 4: Resource consumption is typically determined for resources like \nMemory (ROM, RAM, external / internal EEPROM or Data Flash), CPU load, \netc. \nSWE.2.BP6: Evaluate alternative software architectures.  Define \nevaluation criteria for the architecture. Evaluate alternative software \narchitectures according to the defined criteria. Record the rationale for the \nchosen software architecture. [OUTCOME 1, 2, 3, 4, 5] \nNOTE 5: Evaluation criteria may include quality characteristics (modularity, \nmaintainability, expandability, scalability, reliability, security realization and \nusability) and results of make-buy-reuse analysis. \nSWE.2.BP7: Establish bidirectional traceability. Establish bidirectional \ntraceability between software requirements and elements of the software \narchitectural design. [OUTCOME 5] \nNOTE 6: Bidirectional traceability covers allocation of software requirements to \nthe elements of the software architectural design.  \nNOTE 7: Bidirectional traceability supports coverage, consistency and impact \nanalysis. \nSWE.2.BP8: Ensure consistency. Ensure consistency between software \nrequirements and the software architectural design. [OUTCOME 1, 2, 5, 6] \nNOTE 8: Consistency is supported by bidirectional traceability and can be \ndemonstrated by review records. \nSWE.2.BP9: Communicate agreed software architectural design.  \nCommunicate the agreed software architectural des ign and updates to \nsoftware architectural design to all relevant parties. [OUTCOME 6] \nOutput work \nproducts \n04-04 Software architectural design  → [OUTCOME 1, 2, 3, 4, 5] \n13-04 Communication record  → [OUTCOME 6] \n13-19 Review record    → [OUTCOME 5] \n13-22 Traceability record   → [OUTCOME 5] \n17-08 Interface requirement specification  →  [OUTCOME 3]\n\n### SWE.3 Software Detailed Design and Unit Construction\n\nProcess ID SWE.3 \nProcess name Software Detailed Design and Unit Construction \nProcess purpose The purpose of the Software Detailed Design and Unit Construction \nProcess is to provide an evaluated detailed design for the software \ncomponents and to specify and to produce the software units. \nProcess \noutcomes \nAs a result of successful implementation of this process: \n1) a detailed design is developed that describes software units; \n2) interfaces of each software unit are defined; \n3) the dynamic behavior of the software units is defined; \n\n  \n \n \n \n© VDA Quality Management Center 51 \n \n \n4) consistency and bidirectional traceability are established between \nsoftware requirements and software units; and consistency and \nbidirectional traceability are established between software \narchitectural design and software detailed design; and consistency \nand bidirectional traceability are established between software \ndetailed design and software units;  \n5) the software detailed design and the relationship to the software \narchitectural design is agreed and communicated to all affected \nparties; and \n6) software units defined by the software detailed design are produced. \nBase practices SWE.3.BP1: Develop software detailed design.  Develop a detailed \ndesign for each software component defined in the software architectural \ndesign that specifies all software units with respect to functional and non -\nfunctional software requirements. [OUTCOME 1] \nSWE.3.BP2: Define interfaces of software units. Identify, specify and \ndocument the interfaces of each software unit. [OUTCOME 2] \nSWE.3.BP3: Describe dynamic behavior.  Evaluate and document the \ndynamic behavior of and the interaction between relevant software units. \n[OUTCOME 3]  \nNOTE 1: Not all software units have dynamic behavior to be described. \nSWE.3.BP4: Evaluate software detailed design.  Evaluate the software \ndetailed design in terms of interoperability, interaction, criticality, technical \ncomplexity, risks and testability. [OUTCOME 1,2,3,4] \nNOTE 2: The results of the evaluation can be used as input for software unit \nverification. \nSWE.3.BP5: Establish bidirectional traceability.  Establish bidirectional \ntraceability between software requirements and software units. Establish \nbidirectional traceability between the software architectural design and the \nsoftware detailed design. Establish bidirectional traceability between the \nsoftware detailed design and software units. [OUTCOME 4] \nNOTE 3: Redundancy should be avoided by establishing a combination of \nthese approaches that covers the project and the organizational needs. \nNOTE 4: Bidirectional traceability supports coverage, consistency and impact \nanalysis.  \nSWE.3.BP6: Ensure consistency. Ensure consistency between software \nrequirements and software units. Ensure consistency between the software \narchitectural design, the software detailed design and software units. \n[OUTCOME 4] \nNOTE 5: Consistency is supported by bidirectional traceability and can be \ndemonstrated by review records.  \nSWE.3.BP7: Communicate agreed software detailed design.  \nCommunicate the agreed software detailed design and updates to the \nsoftware detailed design to all relevant parties. [OUTCOME 5] \nSWE.3.BP8: Develop software units.  Develop and document the \nexecutable representations of each software unit according to the software \ndetailed design. [OUTCOME 6] \n\n  \n \n \n \n© VDA Quality Management Center 52 \n \n \nOutput work \nproducts \n04-05 Software detailed design → [OUTCOME 1, 2, 3] \n11-05 Software unit   → [OUTCOME 6] \n13-04 Communication record → [OUTCOME 5] \n13-19 Review record   → [OUTCOME 4] \n13-22 Traceability record  → [OUTCOME 4]\n\n### SWE.4 Software Unit Verification\n\nProcess ID SWE.4 \nProcess name Software Unit Verification \nProcess purpose The purpose of the Software Unit Verification Process is to verify software \nunits to provide evidence for compliance of the software units with the \nsoftware detailed design and with the non-functional software \nrequirements. \nProcess \noutcomes \nAs a result of successful implementation of this process: \n1) a software unit verification strategy including regression strategy is \ndeveloped to verify the software units; \n2) criteria for software unit verification are developed according to the \nsoftware unit verification strategy that are suitable to provide evidence \nfor compliance of the software units with the software detailed design \nand with the non-functional software requirements; \n3) software units are verified according to the software unit verification \nstrategy and the defined criteria for software unit verification and the \nresults are recorded; \n4) consistency and bidirectional traceability are established between \nsoftware units, criteria for verification and verification results; and \n5) results of the unit verification are summarized and communicated to \nall affected parties. \nBase practices SWE.4.BP1: Develop software unit verification strategy including \nregression strategy. Develop a strategy for verification of the software \nunits including regression strategy for re -verification if a software unit is \nchanged. The verification strategy shall define how to provide evidence for \ncompliance of the software units with the software detailed design and with \nthe non-functional requirements. [OUTCOME 1] \nNOTE 1: Possible techniques for unit verification include static/dynamic \nanalysis, code reviews, unit testing etc. \nSWE.4.BP2: Develop criteria for unit verification.  Develop criteria for \nunit verification that are suitable to provide evidence for compliance of the \nsoftware units, and their interactions within the component , with the \nsoftware detailed design and with the n on-functional requirements \naccording to the verification strategy.  For unit testing, criteria shall be \ndefined in a unit test specification. [OUTCOME 2] \nNOTE 2: Possible criteria for unit verification include unit test cases, unit test \ndata, static verification, coverage goals and coding standards such as the \nMISRA rules. \nNOTE 3: The unit test specification may be implemented e.g. as a script in an \nautomated test bench. \n\n  \n \n \n \n© VDA Quality Management Center 53 \n \n \nSWE.4.BP3: Perform static verification of software units.  Verify \nsoftware units for correctness using the defined criteria for verification . \nRecord the results of the static verification. [OUTCOME 3] \nNOTE 4: Static verification may include static analysis, code reviews, checks \nagainst coding standards and guidelines, and other techniques. \nNOTE 5: See SUP.9 for handling of non-conformances. \nSWE.4.BP4: Test software units.  Test software units using the unit test \nspecification according to the software unit verification strategy. Record the \ntest results and logs. [OUTCOME 3] \nNOTE 6: See SUP.9 for handling of non-conformances. \nSWE.4.BP5: Establish bidirectional traceability. Establish bidirectional \ntraceability between software units and static verification results. Establish \nbidirectional traceability between the software detailed design and the unit \ntest specification. Establish bidirectional traceability between the unit test \nspecification and unit test results. [OUTCOME 4] \nNOTE 7: Bidirectional traceability supports coverage, consistency and impact \nanalysis. \nSWE.4.BP6: Ensure consistency.  Ensure consistency between the \nsoftware detailed design and the unit test specification. [OUTCOME 4] \nNOTE 8: Consistency is supported by bidirectional traceability and can be \ndemonstrated by review records. \nSWE.4.BP7: Summarize and communicate results. Summarize the unit \ntest results and static verification results and communicate them to all \naffected parties. [OUTCOME 5] \nNOTE 9: Providing all necessary information from the test case execution in a \nsummary enables other parties to judge the consequences. \nOutput work \nproducts \n08-50 Test specification  → [OUTCOME 2] \n08-52 Test plan   → [OUTCOME 1] \n13-04 Communication record → [OUTCOME 5] \n13-19 Review record   → [OUTCOME 3, 4] \n13-22 Traceability record  → [OUTCOME 4] \n13-25 Verification results  → [OUTCOME 3, 5] \n13-50 Test result   → [OUTCOME 3, 5] \n15-01 Analysis report   → [OUTCOME 3] \n \n  \n\n  \n \n \n \n© VDA Quality Management Center 54\n\n### SWE.5 Software Integration and Integration Test\n\nProcess ID SWE.5 \nProcess name Software Integration and Integration Test \nProcess purpose The purpose of the Software Integration and Integration Test Process is \nto integrate the software units into larger software items up to a complete \nintegrated software consistent with the software architectural design and \nto ensure that the software items are tested to provide evidence for \ncompliance of the integrated software items with the software \narchitectural design, including the interfaces between the software units \nand between the software items. \nProcess \noutcomes \nAs a result of successful implementation of this process: \n1) a software integration strategy consistent with the project plan, \nrelease plan and the software architectural design is developed to \nintegrate the software items; \n2) a software integration test strategy including the regression test \nstrategy is developed to test the software unit and software item \ninteractions; \n3) a specification for software integration test according to the software \nintegration test strategy is developed that is suitable to provide \nevidence for compliance of the integrated software items with the \nsoftware architectural design, including the interfaces between the \nsoftware units and between the software items; \n4) software units and software items are integrated up to a complete \nintegrated software according to the integration strategy; \n5) Test cases included in the software integration test specification are \nselected according to the software integration test strategy, and the \nrelease plan; \n6) integrated software items are tested using the selected test cases and \nthe results of software integration test are recorded; \n7) consistency and bidirectional traceability are established between the \nelements of the software architectural design and the test cases \nincluded in the software integration test specification and between test \ncases and test results; and \n8) results of the software integration test are summarized and \ncommunicated to all affected parties. \nBase practices SWE.5.BP1: Develop software integration strategy. Develop a strategy \nfor integrating software items consistent with the project plan and release \nplan. Identify software items based on the software architectural design and \ndefine a sequence for integrating them. [OUTCOME 1] \nSWE.5.BP2: Develop software int egration test strategy including \nregression test strategy.  Develop a strategy for testing the integrated \nsoftware items following the integration strategy. This includes a regression \ntest strategy for re -testing integrated software items if a software item  is \nchanged. [OUTCOME 2] \nSWE.5.BP3: Develop specification for software integration test.  \nDevelop the test specification for software integration test including the test \ncases according to the software integration test strategy for each integrated \nsoftware item. The test specification shall be suitable to provide evidence \n\n  \n \n \n \n© VDA Quality Management Center 55 \n \n \nfor compliance of the integrated software items with the software \narchitectural design. [OUTCOME 3] \nNOTE 1: Compliance to the architectural design means that the specified \nintegration tests are suitable to prove that the interfaces between the software \nunits and between the software items fulfill the specification given by the \nsoftware architectural design. \nNOTE 2: The software integration test cases may focus on \n• the correct dataflow between software items \n• the timeliness and timing dependencies of dataflow between software \nitems  \n• the correct interpretation of data by all software items using an interface  \n• the dynamic interaction between software items \n• the compliance to resource consumption objectives of interfaces \nSWE.5.BP4: Integrate software units and software items. Integrate the \nsoftware units to software items and software items to integrated software \naccording to the software integration strategy. [OUTCOME 4] \nSWE.5.BP5: Select test cas es. Select test cases from the software \nintegration test specification. The selection of test cases shall have \nsufficient coverage according to the software integration test strategy and \nthe release plan. [OUTCOME 5] \nSWE.5.BP6: Perform software integration  test. Perform the software \nintegration test using the selected test cases. Record the integration test \nresults and logs. [OUTCOME 6] \nNOTE 4: See SUP.9 for handling of non-conformances. \nNOTE 5: The software integration test may be supported by using hardwa re \ndebug interfaces or simulation environments (e.g. Software-in-the-Loop-\nSimulation). \nSWE.5.BP7: Establish bidirectional traceability. Establish bidirectional \ntraceability between elements of the software architectural design and test \ncases included in th e software integration test specification. Establish \nbidirectional traceability between test cases included in the software \nintegration test specification and software integration test results. [OUTCOME \n7] \nNOTE 6: Bidirectional traceability supports coverage, consistency and impact \nanalysis. \nSWE.5.BP8: Ensure consistency. Ensure consistency between elements \nof the software architectural design and test cases included in the software \nintegration test specification. [OUTCOME 7] \nNOTE 7: Consistency is supported by bidirectional traceability and can be \ndemonstrated by review records. \nSWE.5.BP9: Summarize and communicate results.  Summarize the \nsoftware integration test results and communicate them to all affected \nparties. [OUTCOME 8] \nNOTE 8: Providing all necessary information from the test case execution in a \nsummary enables other parties to judge the consequences.  \nOutput work \nproducts \n01-03 Software item   → [OUTCOME 4] \n\n  \n \n \n \n© VDA Quality Management Center 56 \n \n \n01-50 Integrated software  → [OUTCOME 4]  \n08-50 Test specification  → [OUTCOME 3, 5] \n08-52 Test plan   → [OUTCOME 1, 2] \n13-04 Communication record → [OUTCOME 8] \n13-19 Review record   → [OUTCOME 7] \n13-22 Traceability record  → [OUTCOME 7]  \n13-50 Test result   → [OUTCOME 6, 8] \n17-02 Build list   → [OUTCOME 4, 7]\n\n### SWE.6 Software Qualification Test\n\nProcess ID SWE.6 \nProcess name Software Qualification Test \nProcess purpose The purpose of the Software Qualification Test Process is to ensure that \nthe integrated software is tested to provide evidence for compliance with \nthe software requirements. \nProcess \noutcomes \nAs a result of successful implementation of this process: \n1) a software qualification test strategy including regression test strategy \nconsistent with the project plan and release plan is developed to test \nthe integrated software; \n2) a specification for software qualification test of the integrated software \naccording to the software qualification test strategy is developed that \nis suitable to provide evidence for compliance with the software \nrequirements; \n3) test cases included in the software qualification test specification are \nselected according to the software qualification test strategy and the \nrelease plan; \n4) the integrated software is tested using the selected test cases and the \nresults of software qualification test are recorded; \n5) consistency and bidirectional traceability are established between \nsoftware requirements and software qualification test specification \nincluding test cases and between test cases and test results; and \n6) results of the software qualification test are summarized and \ncommunicated to all affected parties. \nBase practices SWE.6.BP1: Develop software qualification test strategy including \nregression test strategy.  Develop a strategy for software qualification \ntesting consistent with the project plan and the release plan. This includes \na regression test strategy for re-testing the integrated software if a software \nitem is changed. [OUTCOME 1] \nSWE.6.BP2: Develop speci fication for software qualification test.  \nDevelop the specification for software qualification test including test cases \nbased on the verification criteria, according to the software test strategy. \nThe test specification shall be suitable to provide evidence for compliance \nof the integrated software with the software requirements. [OUTCOME 2] \n\n  \n \n \n \n© VDA Quality Management Center 57 \n \n \nSWE.6.BP3: Select test cases.  Select test cases from the software test \nspecification. The selection of test cases shall have sufficient coverage \naccording to the software test strategy and the release plan. [OUTCOME 3] \nSWE.6.BP4: Test integrated software. Test the integrated software using \nthe selected test cases. Record the software test results and logs. [OUTCOME \n4] \nNOTE 1: See SUP.9 for handling of non-conformances. \nSWE.6.BP5: Establish bidirectional traceability. Establish bidirectional \ntraceability between software requirements and test cases included in the \nsoftware qualification test specification. Establish bidirectional traceability \nbetween test cases included in the software qualification test specification \nand software qualification test results. [OUTCOME 5] \nNOTE 2: Bidirectional traceability supports coverage, consistency and impact \nanalysis. \nSWE.6.BP6: Ensure consistency. Ensure consistency between software \nrequirements and test cases included in the software qualification test \nspecification. [OUTCOME 5]  \nNOTE 3: Consistency is supported by bidirectional traceability and can be \ndemonstrated by review records. \nSWE.6.BP7: Sum marize and communicate results.  Summarize the \nsoftware qualification test results and communicate them to all affected \nparties. [OUTCOME 6] \nNOTE 4: Providing all necessary information from the test case execution in a \nsummary enables other parties to judge the consequences. \nOutput work \nproducts \n08-50 Test specification  → [OUTCOME 2, 3] \n08-52 Test plan   → [OUTCOME 1] \n13-04 Communication record → [OUTCOME 6] \n13-19 Review record   → [OUTCOME 5] \n13-22 Traceability record  → [OUTCOME 5] \n13-50 Test result   → [OUTCOME 4, 6] \n \n  \n\n  \n \n \n \n© VDA Quality Management Center 58\n\n## Supporting process group (SUP)\n\n\n\n### SUP.1 Quality Assurance\n\nProcess ID SUP.1 \nProcess name Quality Assurance \nProcess purpose The purpose of the Quality Assurance Process is to provide independent \nand objective assurance that work products and processes comply with \npredefined provisions and plans and that non-conformances are resolved \nand further prevented. \nProcess \noutcomes \nAs a result of successful implementation of this process: \n1) a strategy for performing quality assurance is developed, \nimplemented, and maintained; \n2) quality assurance is performed independently and objectively without \nconflicts of interest; \n3) non-conformances of work products, processes, and process activities \nwith relevant requirements are identified, recorded, communicated to \nthe relevant parties, tracked, resolved, and further prevented; \n4) conformance of work products, processes and activities with relevant \nrequirements is verified, documented, and communicated to the \nrelevant parties; \n5) authority to escalate non-conformances to appropriate levels of \nmanagement is established; and \n6) management ensures that escalated non-conformances are resolved. \nBase practices SUP.1.BP1: Develop a project quality assurance strategy.  Develop a \nstrategy in order to ensure that work product and process quality assurance \nis performed at project level independently and objectively without conflicts \nof interest. [OUTCOME 1, 2] \nNOTE 1: Aspects of independence may be financial and/or organizational \nstructure. \nNOTE 2: Quality assurance may be coordinated with, and make use of, the \nresults of other processes such as verification, validation, joint review, audit \nand problem management. \nNOTE 3: Process quality assurance may include process assessments and \naudits, problem analysis, regular check of methods, tools, documents and the \nadherence to defined processes, reports and lessons learned that improve \nprocesses for future projects. \nNOTE 4: Work product quality assurance may include reviews, problem \nanalysis, reports and lessons learned that improve the work products for \nfurther use. \nSUP.1.BP2: Assure quality of work products.  Perform the activities \naccording to the quality assurance strategy and the project schedule to \nensure that the work products meet the defined work product requirements \nand document the results. [OUTCOME 2, 3, 4] \nNOTE 5: Relevant work product requirements may include requirements from \napplicable standards. \n\n  \n \n \n \n© VDA Quality Management Center 59 \n \n \nNOTE 6: Non-conformances detected in work products may be entered into \nthe problem resolution management process (SUP.9) to document, analyze, \nresolve, track to closure and prevent the problems. \nSUP.1.BP3: Assure quality of process activities.  Perform the activities \naccording to the quality assurance strategy and the pr oject schedule to \nensure that the processes meet their defined goals and document the \nresults. [OUTCOME 2, 3, 4] \nNOTE 7: Relevant process goals may include goals from applicable standards.  \nNOTE 8: Problems detected in the process definition or implementation may \nbe entered into a process improvement process (PIM.3) to describe, record, \nanalyze, resolve, track to closure and prevent the problems. \nSUP.1.BP4: Summarize and communicate quality assurance activities \nand results.  Regularly report performance, devi ations, and trends of \nquality assurance activities to relevant parties for information and action \naccording to the quality assurance strategy. [OUTCOME 3, 4] \nSUP.1.BP5: Ensure resolution of non -conformances. Deviations or \nnon-conformance found in process and product quality assurance activities \nshould be analyzed, tracked, corrected, and further prevented . \n[OUTCOME 3,6] \nSUP.1.BP6: Implement an escalation mechanism.  Establish and \nmaintain an escalation mechanism according to the quality assurance \nstrategy th at ensures that quality assurance may escalate problems to \nappropriate levels of management and other relevant stakeholders to \nresolve them. [OUTCOME 5, 6] \nOutput work \nproducts \n08-13 Quality plan   → [OUTCOME 1, 2] \n13-04 Communication record → [OUTCOME 3, 4, 5] \n13-07 Problem record  → [OUTCOME 3, 5] \n13-18 Quality record   → [OUTCOME 2, 3, 4] \n13-19 Review record   → [OUTCOME 2, 3, 4] \n14-02 Corrective action register → [OUTCOME 3, 5, 6] \n18-07 Quality criteria   → [OUTCOME 1]\n\n### SUP.2 Verification\n\nProcess ID SUP.2 \nProcess name Verification \nProcess purpose The purpose of the Verification Process is to confirm that each work \nproduct of a process or project properly reflects the specified \nrequirements. \nProcess \noutcomes \nAs a result of successful implementation of this process:  \n1) a verification strategy is developed, implemented and maintained; \n2) criteria for verification of all required work products are identified; \n3) required verification activities are performed; \n4) defects are identified, recorded and tracked; and \n5) results of the verification activities are made available to the customer \nand other involved parties. \n\n  \n \n \n \n© VDA Quality Management Center 60 \n \n \nBase practices SUP.2.BP1: Develop a verification strategy.  Develop and implement a \nverification strategy, includin g verification activities with associated \nmethods, techniques, and tools ; work product or processes under \nverification; degrees of independence for verification and schedule for \nperforming these activities. [OUTCOME 1] \nNOTE 1: Verification strategy is implemented through a plan.  \nNOTE 2: Software and system verification may provide objective evidence that \nthe outputs of a particular phase of the software development life cycle (e.g. \nrequirements, design, implementation, testing) meet all of the specified \nrequirements for that phase.  \nNOTE 3: Verification methods and techniques may include inspections, peer \nreviews (see also SUP.4), audits, walkthroughs and analysis.  \nSUP.2.BP2: Develop criteria for verification.  Develop the criteria for \nverification of all required technical work products. [OUTCOME 2]  \nSUP.2.BP3: Conduct verification.  Verify identified work products \naccording to the specified strategy and to the developed criteria to confirm \nthat the work products meet their specified requirements. The result s of \nverification activities are recorded. [OUTCOME 3]  \nSUP.2.BP4: Determine and track actions for verification results.  \nProblems identified by the verification should be entered into the problem \nresolution management process (SUP.9) to describe, record, analyze, \nresolve, track to closure and prevent the problems. [OUTCOME 4]  \nSUP.2.BP5: Report verification results.  Verification results should be \nreported to all affected parties. [OUTCOME 5] \nOutput work \nproducts \n13-04 Communication record → [OUTCOME 5] \n13-07 Problem record  → [OUTCOME 3, 4, 5] \n13-25 Verification results  → [OUTCOME 2, 3, 4, 5] \n14-02 Corrective action register → [OUTCOME 4] \n18-07 Quality criteria   → [OUTCOME 2] \n19-10 Verification strategy  → [OUTCOME 1]\n\n### SUP.4 Joint Review\n\nProcess ID SUP.4 \nProcess name Joint Review \nProcess purpose The purpose of the Joint review process is to maintain a common \nunderstanding with the stakeholders of the progress against the \nobjectives of the agreement and what should be done to help ensure \ndevelopment of a product that satisfies the stakeholders. Joint reviews \nare at both project management and technical levels and are held \nthroughout the life of the project. \nProcess \noutcomes \nAs a result of successful implementation of this process:  \n1) management and technical reviews are held based on the needs of \nthe project; \n\n  \n \n \n \n© VDA Quality Management Center 61 \n \n \n2) the status and products of an activity of a process are evaluated \nthrough joint review activities between the stakeholders; \n3) review results are made known to all affected parties;  \n4) action items resulting from reviews are tracked to closure; and \n5) problems are identified and recorded. \nNOTE 1: Joint review should be performed at specific milestones during \nproject/product development. The scope and the goals of joint review may be \ndifferent dependent on project/product development phase (for example, in the \nearly stage of a project joint review may be \"conceptual\" in order to analyze \nthe customer requirements; in later stages joint review may be concerned with \nthe implementation). \nNOTE 2: Joint review should be performed to verify different aspects (for \nexample: hardware resources utilization; the introduction of new requirements \nand new technologies; modification to the working team structure; technology \nchanges). \nBase practices SUP.4.BP1: Define review elements. Based on the needs of the project, \nidentify the schedule, scope and participants of management and technical \nreviews, agree all resources required to conduct the reviews (this includes \npersonnel, location and facilities) and establish review criteria for problem \nidentification, resolution and agreement. [OUTCOME 1] \nSUP.4.BP2: Establish a mechanism to handle review outcomes.  \nEstablish mechanisms to ensure that review results are made available to \nall affected parties that problems detected during the reviews are identified \nand recorded and that action items raised are recorded for action.  \n[OUTCOME 3] \nSUP.4.BP3: Prepare joint review.  Collect, plan, prepare and distribute \nreview material as appropriate in preparation for the review. [OUTCOME 1] \nNOTE 1: The following items may be addressed: Scope and purpose of the \nreview; Products and problems to be reviewed; Entry and exit criteria; Meeting \nagenda; Roles and participants; Distribution list; Responsibilities; Resource \nand facility requirements; Used tools (checklists, scenario for perspective \nbased reviews etc.). \nSUP.4.BP4: Conduct joint reviews.  Conduct joint management and \ntechnical reviews as planned. Record the review results. [OUTCOME 1, 2] \nSUP.4.BP5: Distribute the results.  Document and distribute the revi ew \nresults to all the affected parties. [OUTCOME 3] \nSUP.4.BP6: Determine actions for review results.  Analyze the review \nresults, propose actions for resolution and determine the priority for actions. \n[OUTCOME 4] \nSUP.4.BP7: Track actions for review results. Track actions for resolution \nof identified problems in a review to closure. [OUTCOME 4] \nSUP.4.BP8: Identify and record problems.  Identify and record the \nproblems detected during the reviews according to the established \nmechanism. [OUTCOME 5] \nOutput work \nproducts \n13-04 Communication record →  [OUTCOME 3] \n13-05 Contract review record →  [OUTCOME 1, 2, 3] \n\n  \n \n \n \n© VDA Quality Management Center 62 \n \n \n13-07 Problem record  →  [OUTCOME 3, 5] \n13-09 Meeting support record →  [OUTCOME 1, 2] \n13-19 Review record   →  [OUTCOME ALL] \n14-02 Corrective action register →  [OUTCOME 3, 4, 5] \n14-08 Tracking system  →  [OUTCOME 3, 4, 5] \n15-01 Analysis report   →  [OUTCOME 3, 5] \n15-13 Assessment/audit report →  [OUTCOME 1, 2] \n15-16 Improvement opportunity →  [OUTCOME 3, 4]\n\n### SUP.7 Documentation\n\nProcess ID SUP.7 \nProcess name Documentation \nProcess purpose The purpose of the Documentation Process is to develop and maintain the \nrecorded information produced by a process. \nProcess \noutcomes \nAs a result of successful implementation of this process:  \n1) a strategy identifying the documentation to be produced during the life \ncycle of the product or service is developed; \n2) the standards to be applied for the development of the documentation \nare identified; \n3) documentation to be produced by the process or project is identified; \n4) the content and purpose of all documentation is specified, reviewed \nand approved; \n5) documentation is developed and made available in accordance with \nidentified standards; and \n6) documentation is maintained in accordance with defined criteria. \nBase practices SUP.7.BP1: Develop a documentation management strategy. Develop \na documentation management strategy which addresses where, when and \nwhat should be documented during the life cycle of the product/service. \n[OUTCOME 1]  \nNOTE 1: A documentation management strategy may define the controls \nneeded to approve documentation for adequacy prior to issue; to review and \nupdate as necessary and re-approve documentation; to ensure that changes \nand the current revision status of documentation are identified; to ensure that \nrelevant versions of documentation are available at points of issue; to ensure \nthat documentation remain legible and readily identifiable; to ensure the \ncontrolled distribution of documentation; to prevent unintended use of obsolete \ndocumentation ; and may also specify the levels of confidentiality, copyright or \ndisclaimers of liability for the documentation. \nSUP.7.BP2: Establish standards for documentation.  Establish \nstandards for developing, modifying and maintaining documentation. \n[OUTCOME 2] \nSUP.7.BP3: Specify documentation requirements.  Specify \nrequirements for documentation such as title, date, identifier, version \nhistory, author(s), reviewer, authorizer, outline of contents, purpose, and \ndistribution list. [OUTCOME 2] \n\n  \n \n \n \n© VDA Quality Management Center 63 \n \n \nSUP.7.BP4: Identify the relevant  documentation to be produced . For \nany given development life cycle, identify the documentation to be \nproduced. [OUTCOME 3] \nSUP.7.BP5: Develop documentation.  Develop documentation at \nrequired process points according to established standards and policy, \nensuring the content and purpose is reviewed and approved as \nappropriate. [OUTCOME 4, 5] \nSUP.7.BP6: Check documentation.  Review documentation before \ndistribution, and authorize documentation as appropriate before distribution \nor release. [OUTCOME 5] \nNOTE 2: The documentation intended for use by system and software users \nshould accurately describe the system and software and how it is to be used in \nclear and useful manner for them. \nNOTE 3: Documentation should be checked through verification or validation \nprocess.  \nSUP.7.BP7: Distribute documentation.  Distribute documentation \naccording to determined modes of distribution via appropriate media to all \naffected parties, confirming delivery of documentation, where necessary. \n[OUTCOME 5] \nSUP.7.BP8: Maintain document ation. Maintain documentation in \naccordance with the determined documentation strategy. [OUTCOME 6] \nNOTE 4: If the documentation is part of a product baseline or if its control and \nstability are important, it should be modified and distributed in accordance with \nprocess SUP.8 Configuration management. \nOutput work \nproducts \n08-26 Documentation plan →  [OUTCOME 1, 2] \n13-01 Acceptance record →  [OUTCOME 4, 5] \n13-19 Review record  →  [OUTCOME 4, 5] \n14-01 Change history  →  [OUTCOME 5, 6] \n14-11 Work product list →  [OUTCOME 3]\n\n### SUP.8 Configuration Management\n\nProcess ID SUP.8 \nProcess name Configuration Management \nProcess purpose The purpose of the Configuration Management Process is to establish \nand maintain the integrity of all work products of a process or project and \nmake them available to affected parties. \nProcess \noutcomes \nAs a result of successful implementation of this process: \n1) a configuration management strategy is developed; \n2) all configuration items generated by a process or project are identified, \ndefined and baselined according to the configuration management \nstrategy; \n3) modifications and releases of the configuration items are controlled; \n4) modifications and releases are made available to affected parties; \n\n  \n \n \n \n© VDA Quality Management Center 64 \n \n \n5) the status of the configuration items and modifications is recorded and \nreported; \n6) the completeness and consistency of the baselines is ensured; and \n7) storage of the configuration items is controlled. \nBase practices SUP.8.BP1: Develop a configuration management strategy. Develop a \nconfiguration management strategy, including  \n• responsibilities; \n• tools and repositories; \n• criteria for configuration items; \n• naming conventions; \n• access rights; \n• criteria for baselines; \n• merge and branch strategy; \n• the revision history approach for configuration items \n[OUTCOME 1] \nNOTE 1: The configuration management strategy typically supports the \nhandling of product/software variants which may be caused by different sets of \napplication parameters or by other causes. \nNOTE 2: The branch management strategy specifies in which cases branching \nis permissible, whether authorization is required, how branches are merged, \nand which activities are required to verify that all changes have been \nconsistently integrated without damage to other changes or to the original \nsoftware. \nSUP.8.BP2: Iden tify configuration items.  Identify and document \nconfiguration items according to the configuration management strategy. \n[OUTCOME 2] \nNOTE 3: Configuration control is typically applied for the products that are \ndelivered to the customer, designated internal work products, acquired \nproducts, tools and other configuration items that are used in creating and \ndescribing these work products. \nSUP.8.BP3: Establish a configuration management system.  Establish \na configuration management system according to the configu ration \nmanagement strategy. [OUTCOME 1, 2, 3, 4, 6, 7] \nSUP.8.BP4: Establish branch management.  Establish branch \nmanagement according to the configuration management strategy where \napplicable for parallel developments that use the same base. [OUTCOME 1, \n3, 4, 6, 7] \nSUP.8.BP5: Control modifications and releases. Establish mechanisms \nfor control of the configuration items according to the configuration \nmanagement strategy, and control modifications and releases using these \nmechanisms. [OUTCOME 3, 4, 5] \nSUP.8.BP6: Establish baselines.  Establish baselines for internal \npurposes and for external delivery according to the configuration \nmanagement strategy. [OUTCOME 2] \nNOTE 4: For baseline issues refer also to the product release process SPL.2.  \nSUP.8.BP7: Report conf iguration status. Record and report status of \nconfiguration items to support project management and other relevant \nprocesses. [OUTCOME 5] \n\n  \n \n \n \n© VDA Quality Management Center 65 \n \n \nNOTE 5: Regular reporting of the configuration status (e.g. how many \nconfiguration items are currently under work, checked in, tested, released, \netc.) supports project management activities and dedicated project phases like \nsoftware integration. \nSUP.8.BP8: Verify the information about configured items.  Verify that \nthe information about configured items, and their baselines is complete and \nensure the consistency of baselines. [OUTCOME 6] \nNOTE 6: A typical implementation is performing baseline and configuration \nmanagement audits. \nSUP.8.BP9: Manage the storage of configuration items and baselines. \nEnsure the integrity and availability of configuration items and baselines \nthrough appropriate scheduling and resourcing of storage, archiving (long \nterm storage) and backup of the used CM systems. [OUTCOME 4, 5, 6, 7] \nNOTE 7: Backup, storage and archiving may need to extend beyond the \nguaranteed lifetime of available storage media. Relevant configuration items \naffected may include those referenced in note 2 and note 3. Availability may be \nspecified by contract requirements. \nOutput work \nproducts \n06-02 Handling and storage guide  → [OUTCOME 3, 4, 5, 7] \n08-04 Configuration management plan → [OUTCOME 1, 2, 7] \n08-14 Recovery plan    → [OUTCOME 1, 7] \n13-08 Baseline    → [OUTCOME 2, 3, 4, 5, 6] \n13-10 Configuration management record → [OUTCOME 2, 5, 7] \n14-01 Change history    → [OUTCOME 3] \n16-03 Configuration management system → [OUTCOME 1, 3, 4]\n\n### SUP.9 Problem Resolution Management\n\nProcess ID SUP.9 \nProcess name Problem Resolution Management \nProcess purpose The purpose of the Problem Resolution Management Process is to \nensure that problems are identified, analyzed, managed and controlled to \nresolution. \nProcess \noutcomes \nAs a result of successful implementation of this process: \n1) a problem resolution management strategy is developed; \n2) problems are recorded, uniquely identified and classified; \n3) problems are analyzed and assessed to identify an appropriate \nsolution; \n4) problem resolution is initiated; \n5) problems are tracked to closure; and \n6) the status of problems and their trend are known. \nBase practices SUP.9.BP1: D evelop a problem resolution management strategy.  \nDevelop a problem resolution management strategy, including problem \nresolution activities, a status model for the problems, alert notifications, \nresponsibilities for performing these activities and an urgent  resolution \n\n  \n \n \n \n© VDA Quality Management Center 66 \n \n \nstrategy. Interfaces to affected parties are defined and definitions are \nmaintained. [OUTCOME 1] \nNOTE 1: Problem resolution activities can be different during the product life \ncycle, e.g. during prototype construction and series development.  \nSUP.9.BP2: Identify and record the problem. Each problem is uniquely \nidentified, described and recorded. Supporting information should be \nprovided to reproduce and diagnose the problem. [OUTCOME 2] \nNOTE 2: Supporting information typically includes the origin of the problem, \nhow it can be reproduced, environmental information, by whom it has been \ndetected, etc. \nNOTE 3: Unique identification supports traceability to changes made. \nSUP.9.BP3: Record the status of problems.  A status according to the \nstatus model is assigned to each problem to facilitate tracking. [OUTCOME 6] \nSUP.9.BP4: Diagnose the cause and determine the impact of the \nproblem. Investigate the problem and determine its cause and impact in \norder to categorize the problem and to determine appropria te actions . \n[OUTCOME 2, 3] \nNOTE 4: Problem categorization (e.g. A, B, C, light, medium, severe) may be \nbased on severity, impact, criticality, urgency, relevance for the change \nprocess, etc. \nSUP.9.BP5: Authorize urgent resolution action.  If according to the \nstrategy a problem requires an urgent resolution, authorization shall be \nobtained for immediate action also according to the strategy. [OUTCOME 4] \nSUP.9.BP6: Raise alert notifications.  If according to the strategy the \nproblem has a hig h impact on other systems or other affected parties, an \nalert notification needs to be raised also according to the strategy. \n[OUTCOME 4] \nSUP.9.BP7: Initiate problem resolution.  Initiate appropriate actions \naccording to the strategy to resolve the problem including review of those \nactions, or initiate a change request. [OUTCOME 4] \nNOTE 5: Appropriate actions may include the initiating of a change request. \nSee SUP.10 for managing of change requests. \nNOTE 6: The implementation of process improvements (to prevent problems) \nis done in the process improvement process (PIM.3).The implementation of \ngeneric project management improvements (e.g. lessons learned) are part of \nthe project management process (MAN.3). The implementation of generic \nwork product related improvements are part of the quality assurance process \n(SUP.1). \nSUP.9.BP8: Track problems to closure. Track the status of problems to \nclosure including all related change requests. A formal acceptance has to \nbe authorized before closing the problem. [OUTCOME 5, 6] \nSUP.9.BP9: Analyze problem trends.  Collect and analyze problem \nresolution management data, identify trends, and initiate project related \nactions, according to the strategy. [OUTCOME 6] \n\n  \n \n \n \n© VDA Quality Management Center 67 \n \n \nNOTE 7: Collected data typically contains information about where the \nproblems occurred, how and when they were found, what were their impacts, \netc. \n \nOutput work \nproducts \n08-27 Problem management plan → [OUTCOME 1] \n13-07 Problem record  → [OUTCOME 2, 3, 4, 5] \n15-01 Analysis report   → [OUTCOME 3] \n15-05 Evaluation report  → [OUTCOME 3] \n15-12 Problem status report  → [OUTCOME 6]\n\n### SUP.10 Change Request Management\n\nProcess ID SUP.10 \nProcess name Change Request Management \nProcess purpose The purpose of the Change Request Management Process is to ensure \nthat change requests are managed, tracked and implemented. \nProcess \noutcomes \nAs a result of successful implementation of this process: \n1) a change request management strategy is developed; \n2) requests for changes are recorded and identified; \n3) dependencies and relationships to other change requests are \nidentified; \n4) criteria for confirming implementation of change requests are defined; \n5) requests for change are analyzed, and resource requirements are \nestimated; \n6) changes are approved and prioritized on the basis of analysis results \nand availability of resources; \n7) approved changes are implemented and tracked to closure; \n8) the status of all change requests is known; and \n9) bi-directional traceability is established between change requests and \naffected work products. \nBase practices SUP.10.BP1: Develop a change request management strategy.  \nDevelop a change request management strategy, including change request \nactivities, a status model for the change requests, analysis criteria, and \nresponsibilities for performing these activities. Interfaces to affected parties \nare defined and maintained. [OUTCOME 1] \nNOTE 1: A status model for change requests may contain: open, under \ninvestigation, approved for implementation, allocated, implemented, fixed, \nclosed, etc. \nNOTE 2: Typical analysis criteria are: resource requirements, scheduling \nissues, risks, benefits, etc. \nNOTE 3: Change request activities ensure that change requests are \nsystematically identified, described, recorded, analyzed, implemented, and \nmanaged. \n\n  \n \n \n \n© VDA Quality Management Center 68 \n \n \nNOTE 4: The change request management strategy may cover different \nproceedings across the product life cycle, e.g. during prototype construction \nand series development.  \nSUP.10.BP2: Identify and record the change requests.  Each change \nrequest is uniquely identified, described, and recorded  according to the \nstrategy, including the initiator and reason of the change request. [OUTCOME \n2, 3] \nSUP.10.BP3: Record the status of change requests. A status according \nto the status model is assigned to each change request to facilitate tracking. \n[OUTCOME 8] \nSUP.10.BP4: Analyze and assess change requests.  Change requests \nare analyzed according to the strategy including their dependencies to \naffected work products and other change requests. Assess the impact of \nthe change requests and establish criteria fo r confirming implementation. \n[OUTCOME 3, 4, 5, 9] \nSUP.10.BP5: Approve change requests before implementation.  \nChange requests are prioritized based on analysis results and availability \nof resources before implementation and approved according to the \nstrategy. [OUTCOME 6] \nNOTE 5: A Change Control Board (CCB) is a common mechanism used to \napprove change requests. \nNOTE 6: Prioritization of change requests may be done by allocation to \nreleases. \nSUP.10.BP6: Review the implementation of change requests.   \nThe implementation of change requests is reviewed before closure to \nensure that their criteria for confirming implementation are satisfied, and \nthat all relevant processes have been applied. [OUTCOME 7, 8] \nSUP.10.BP7: Track change requests to closure.  Change requests are \ntracked until closure. Feedback to the initiator is provided. [OUTCOME 7, 8] \nSUP.10.BP8: Establish bidirectional traceability. Establish bidirectional \ntraceability between change requests and work products affected by the \nchange requests. In case that the change request is initiated by a problem, \nestablish bidirectional traceability between change requests and the \ncorresponding problem reports. [OUTCOME 9] \nNOTE 7: Bidirectional traceability supports consistency, completeness and \nimpact analysis. \nOutput work \nproducts \n08-28 Change management plan → [OUTCOME 1] \n13-16 Change request  → [OUTCOME 2, 3, 4, 5, 6, 7] \n13-19 Review record   → [OUTCOME 7] \n13-21 Change control record  → [OUTCOME 8, 9] \n \n  \n\n  \n \n \n \n© VDA Quality Management Center 69\n\n## Management process group (MAN)\n\n\n\n### MAN.3 Project Management\n\nProcess ID MAN.3 \nProcess name Project Management \nProcess purpose The purpose of the Project Management Process is to identify, establish, \nand control the activities and resources necessary for a project to produce \na product, in the context of the project’s requirements and constraints. \nProcess \noutcomes \nAs a result of successful implementation of this process: \n1) the scope of the work for the project is defined; \n2) the feasibility of achieving the goals of the project with available \nresources and constraints is evaluated; \n3) the activities and resources necessary to complete the work are sized \nand estimated; \n4) interfaces within the project, and with other projects and \norganizational units, are identified and monitored; \n5) plans for the execution of the project are developed, implemented and \nmaintained; \n6) progress of the project is monitored and reported; and \n7) corrective action is taken when project goals are not achieved, and \nrecurrence of problems identified in the project is prevented. \nBase practices MAN.3.BP1: Define the scope of work.  Identify the project's goals, \nmotivation and boundaries. [OUTCOME 1] \nMAN.3.BP2: Define project life cycle. Define the life cycle for the project, \nwhich is appropriate to the scope, context, magnitude and complexity of the \nproject. [OUTCOME 2] \nNOTE 1: This typically means that the project life cycle and the customer's \ndevelopment process are consistent with each other. \nMAN.3.BP3: Evaluate feasibility of the project.  Evaluate the feasibility \nof achieving the goals of the p roject in terms of technical feasibility within \nconstraints with respect to time, project estimates, and available resources. \n[OUTCOME 2] \nMAN.3.BP4: Define, monitor and adjust project activities.  Define, \nmonitor and adjust project activities and their depe ndencies according to \ndefined project life cycle and estimations. Adjust activities and their \ndependencies as required. [OUTCOME 3, 5, 7] \nNOTE 2: A structure and a manageable size of the activities and related work \npackages support an adequate progress monitoring.  \nNOTE 3: Project activities typically cover engineering, management and \nsupporting processes. \nMAN.3.BP5: Define, monitor and adjust project estimates and \nresources. Define, monitor and adjust project estimates of effort and \nresources based on project's goals, project risks, motivation and \nboundaries. [OUTCOME 2, 3, 7] \nNOTE 4: Appropriate estimation methods should be used. \n\n  \n \n \n \n© VDA Quality Management Center 70 \n \n \nNOTE 5: Examples of necessary resources are people, infrastructure (such as \ntools, test equipment, communication mechanisms...) and hardware/materials. \nNOTE 6: Project risks (using MAN.5) and quality criteria (using SUP.1) may be \nconsidered. \nNOTE 7: Estimations and resources typically include engineering, \nmanagement and supporting processes. \nMAN.3.BP6: Ensure required skills, knowledge, and experience.  \nIdentify the required skills, knowledge, and experience for the project in line \nwith the estimates and make sure the selected individuals and teams either \nhave or acquire these in time. [OUTCOME 3, 7] \nNOTE 8: In the case of deviations from required skills and knowledge trainings \nare typically provided. \nMAN.3.BP7: Identify, monitor and adjust project interfaces and agreed \ncommitments. Identify and agree interfaces of the project with other (sub-\n) projects, organizational units and other affected stakeholders and monitor \nagreed commitments. [OUTCOME 4, 7] \nNOTE 9: Project interfaces relate to engineering, management and supporting \nprocesses. \nMAN.3.BP8: Define, monitor and a djust project schedule.  Allocate \nresources to activities, and schedule each activity of the whole project. The \nschedule has to be kept continuously updated during lifetime of the project. \n[OUTCOME 3, 5, 7] \nNOTE 10: This relates to all engineering, management and supporting \nprocesses. \nMAN.3.BP9: Ensure consistency. Ensure that estimates, skills, activities, \nschedules, plans, interfaces, and commitments for the project are \nconsistent across affected parties. [OUTCOME 3, 4, 5, 7] \nMAN.3.BP10: Review and report progress of the project.  Regularly \nreview and r eport the status of the project  and the fulfillment of activities \nagainst estimated effort and duration to all affected parties. Prevent \nrecurrence of problems identified. [OUTCOME 6, 7] \nNOTE 11: Project reviews may be executed at regular intervals by the \nmanagement. At the end of a project, a project review contributes to identifying  \ne.g. best practices and lessons learned.  \nOutput work \nproducts \n08-12 Project plan   → [OUTCOME 1, 3, 4, 5] \n13-04 Communication record → [OUTCOME 4, 6] \n13-16 Change request  → [OUTCOME 7] \n13-19 Review record   → [OUTCOME 2, 7] \n14-02 Corrective action register → [OUTCOME 7] \n14-06 Schedule   → [OUTCOME 3, 5] \n14-09 Work breakdown structure → [OUTCOME 3, 4, 5] \n14-50 Stakeholder groups list → [OUTCOME 4] \n15-06 Project status report  → [OUTCOME 4, 6] \n\n  \n \n \n \n© VDA Quality Management Center 71\n\n### MAN.5 Risk Management\n\nProcess ID MAN.5 \nProcess name Risk Management \nProcess purpose The purpose of the Risk Management Process is to identify, analyze, treat \nand monitor the risks continuously. \nProcess \noutcomes \nAs a result of successful implementation of this process:  \n1) the scope of the risk management to be performed is determined; \n2) appropriate risk management strategies are defined and \nimplemented; \n3) risks are identified as they develop during the conduct of the project; \n4) risks are analyzed and the priority in which to apply resources to \ntreatment of these risks is determined; \n5) risk measures are defined, applied, and assessed to determine \nchanges in the status of risk and the progress of the treatment \nactivities; and \n6) appropriate treatment is taken to correct or avoid the impact of risk \nbased on its priority, probability, and consequence or other defined \nrisk threshold. \nBase practices MAN.5.BP1: Establish risk management scope. Determine the scope of \nrisk management to be performed for the project, in accordance with \norganizational risk management policies. [OUTCOME 1] \nNOTE 1: Risks may include technical, economic and timing risks. \nMAN.5.BP2: Define risk management strateg ies. Define appropriate \nstrategies to identify risks, mitigate risks and set acceptability levels for \neach risk or set of risks, both at the project and organizational level. \n[OUTCOME 2] \nMAN.5.BP3: Identify risks. Identify risks to the project both initially within \nthe project strategy and as they develop during the conduct of the project, \ncontinuously looking for risk factors at any occurrence of technical or \nmanagerial decisions. [OUTCOME 2, 3] \nNOTE 2: Examples of risk areas that are typically analyzed for potential risk \nreasons or risks factors include: cost, schedule, effort, resource, and technical. \nNOTE 3: Examples of risk factors may include: unsolved and solved trade-offs, \ndecisions of not implementing a project feature, design changes, lack of \nexpected resources. \nMAN.5.BP4: Analyze risks.  Analyze risks to determine the priority in \nwhich to apply resources to mitigate these risks. [OUTCOME 4] \nNOTE 4: Risks are normally analyzed to determine their probability, \nconsequence and severity. \nNOTE 5: Different techniques may be used to analyze a system in order to \nunderstand if risks exist, for example, functional analysis, simulation, FMEA, \nFTA etc. \nMAN.5.BP5: Define risk treatment actions. For each risk (or set of risks) \ndefine, perform and track the  selected actions to keep/reduce the risks to \nacceptable level. [OUTCOME 5, 6] \n\n  \n \n \n \n© VDA Quality Management Center 72 \n \n \nMAN5.BP6: Monitor risks. For each risk (or set of risks) define measures \n(e.g. metrics) to determine changes in the status of a risk and to evaluate \nthe progress of the of mitig ation activities. Apply and assess these risk \nmeasures. [OUTCOME 5, 6] \nNOTE 6: Major risks may need to be communicated to and monitored by \nhigher levels of management. \nMAN.5.BP7: Take corrective action.  When expected progress in risk \nmitigation is not achi eved, take appropriate corrective action to reduce or \navoid the impact of risk. [OUTCOME 6] \nNOTE 7: Corrective actions may involve developing and implementing new \nmitigation strategies or adjusting the existing strategies. \nOutput work \nproducts \n07-07 Risk measure   → [OUTCOME 5] \n08-14 Recovery plan   → [OUTCOME 4, 6] \n08-19 Risk management plan → [OUTCOME ALL] \n08-20 Risk mitigation plan  → [OUTCOME 3, 4, 5, 6] \n13-20 Risk action request  → [OUTCOME 1, 2, 6] \n14-02 Corrective action register → [OUTCOME 6] \n14-08 Tracking system  → [OUTCOME 5, 6] \n15-08 Risk analysis report   → [OUTCOME 4] \n15-09 Risk status report  → [OUTCOME 4, 5]\n\n### MAN.6 Measurement\n\nProcess ID MAN.6 \nProcess name Measurement \nProcess purpose The purpose of the Measurement Process is to collect and analyze data \nrelating to the products developed and processes implemented within the \norganization and its projects, to support effective management of the \nprocesses and to objectively demonstrate the quality of the products. \nProcess \noutcomes \nAs a result of successful implementation of this process:  \n1) organizational commitment is established and sustained to implement \nthe measurement process; \n2) the measurement information needs of organizational and \nmanagement processes are identified; \n3) an appropriate set of measures, driven by the information needs are \nidentified and/or developed; \n4) measurement activities are identified and performed; \n5) the required data is collected, stored, analyzed, and the results \ninterpreted; \n6) information products are used to support decisions and provide an \nobjective basis for communication; and  \n7) the measurement process and measures are evaluated and \ncommunicated to the process owner. \n\n  \n \n \n \n© VDA Quality Management Center 73 \n \n \nBase practices MAN.6.BP1: Establish organizational commitment for measurement. \nA commitment of management and staff to measurement is established \nand communicated to the organizational unit. [OUTCOME 1] \nMAN.6.BP2: Develop measurement strategy.  Define an appropriate \nmeasurement strategy to identify, perform  and evaluate measurement \nactivities and results, based on organizational and project needs.  \n[OUTCOME 1] \nMAN.6.BP3: Identify measurement information needs.  Identify the \nmeasurement information needs of organizational and management \nprocesses. [OUTCOME 2] \nMAN.6.BP4: Specify measures. Identify and develop an appropriate set \nof measures based on measurement information needs. [OUTCOME 3] \nMAN.6.BP5: Perform measurement activities.  Identify and perform \nmeasurements activities. [OUTCOME 4] \nMAN.6.BP6: Retrieve measurement data. Collect and store data of both \nbase and derived measures, including any context information necessary \nto verify, understand, or evaluate the data. [OUTCOME 5] \nMAN.6.BP7: Analyze measures.  Analyze and interpret measurement \ndata and develop information products. [OUTCOME 5] \nMAN.6.BP8: Use measurement information for decision-making. Make \naccurate and current measurement information accessible for any \ndecision-making processes for which it is relevant. [OUTCOME 6] \nMAN.6.BP9: Communicate measure s. Disseminate measurement \ninformation to all affected parties who will be using them and collect \nfeedback to evaluate appropriateness for intended use. [OUTCOME 5, 6] \nMAN.6.BP10: Evaluate information products and measurement \nactivities. Evaluate informati on products and measurement activities \nagainst the identified information needs and measurement strategy. \nIdentify potential improvements. [OUTCOME 7] \nNOTE 1: Information products are produced as a result analysis of data in \norder to summarize and communicate information. \nMAN.6.BP11: Communicate potential improvements. Communicate to \nthe affected people the identified potential improvements concerning the \nprocesses they are involved in. [OUTCOME 7] \nOutput work \nproducts \n02-01 Commitment/agreement  → [OUTCOME 1] \n03-03 Benchmarking data   → [OUTCOME 5] \n03-04 Customer satisfaction data  → [OUTCOME 5] \n03-06 Process performance data  → [OUTCOME 6] \n07-01 Customer satisfaction survey  → [OUTCOME 3, 7] \n07-02 Field measure    → [OUTCOME 3, 7] \n07-03 Personnel performance measure → [OUTCOME 3, 4, 7] \n07-04 Process measure   → [OUTCOME 3, 4, 7] \n07-05 Project measure   → [OUTCOME 3, 4, 7] \n07-06 Quality measure   → [OUTCOME 3, 4, 7] \n\n  \n \n \n \n© VDA Quality Management Center 74 \n \n \n07-07 Risk measure    → [OUTCOME 3, 4, 7] \n07-08 Service level measure   → [OUTCOME 3, 4, 7] \n15-01 Analysis report    → [OUTCOME 2, 5] \n15-05 Evaluation report   → [OUTCOME 5, 7] \n15-18 Process performance report  → [OUTCOME 5, 7]\n\n## Process improvement process group (PIM)\n\n\n\n### PIM.3 Process Improvement\n\nProcess ID PIM.3 \nProcess name Process Improvement \nProcess purpose The purpose of the Process Improvement Process is to continually \nimprove the organization’s effectiveness and efficiency through the \nprocesses used and aligned with the business need. \nProcess \noutcomes \nAs a result of successful implementation of this process:  \n1) commitment is established to provide resources to sustain \nimprovement actions; \n2) issues arising from the organization's internal/external environment \nare identified as improvement opportunities and justified as reasons \nfor change; \n3) analysis of the current status of the existing process is performed, \nfocusing on those processes from which improvement stimuli arise; \n4) improvement goals are identified and prioritized, and consequent \nchanges to the process are defined, planned and implemented; \n5) the effects of process implementation are monitored, measured and \nconfirmed against the defined improvement goals; \n6) knowledge gained from the improvement is communicated within the \norganization; and \n7) the improvements made are evaluated and consideration given for \nusing the solution elsewhere within the organization.  \nBase practices PIM.3.BP1: Establish commitment.  Commitment is established to \nsupport the process group, to provide resources and further enablers \n(trainings, methods, infrastructure, etc.) to sustain improvement actions. \n[OUTCOME 1] \nNOTE 1: The process improvement process is a generic process, which can \nbe used at all levels (e.g. organizational level, process level, project level, etc.) \nand which can be used to improve all other processes.  \nNOTE 2: Commitment at all levels of management may support process \nimprovement. Personal goals may be set for the relevant managers to enforce \nmanagement commitment. \nPIM.3.BP2: Identify issues.  Processes and int erfaces are continuously \nanalyzed to identify issues arising from the organization’s internal/external \nenvironment as improvement opportunities, and with justified reasons for \nchange. This includes issues and improvement suggestions addressed by \nthe customer. [OUTCOME 2, 3] \n\n  \n \n \n \n© VDA Quality Management Center 75 \n \n \nNOTE 3: Continuous analysis may include problem report trend analysis (see \nSUP.9), analysis from Quality Assurance and Verification results and records \n(see SUP.1 – SUP.2), validation results and records, and product quality \nmeasures like ppm and recalls. \nNOTE 4: Information sources providing input for change may include: process \nassessment results, audits, customer's satisfaction reports, organizational \neffectiveness/efficiency, cost of quality. \nPIM.3.BP3: Establish process improvement g oals. Analysis of the \ncurrent status of the existing process is performed, focusing on those \nprocesses from which improvement stimuli arise, resulting in improvement \nobjectives for the processes being established. [OUTCOME 3] \nNOTE 5: The current status of processes may be determined by process \nassessment. \nPIM.3.BP4: Prioritize improvements.  The improvement objectives and \nimprovement activities are prioritized. [OUTCOME 4] \nPIM.3.BP5: Plan process changes. Consequent changes to the process \nare defined and planned. [OUTCOME 4] \nNOTE 6: Process changes may only be possible if the complete supply chain \nimproves (all relevant parties). \nNOTE 7: Traditionally process changes are mostly applied to new projects. \nWithin the automotive industry, changes could be implemented per project \nphase (e.g. product sample phases A, B, C), yielding a higher improvement \nrate. Also, the principle of low hanging fruit (that is implementing easy \nimprovements first) may be considered when planning process changes.  \nNOTE 8: Improvements may be planned in continuous incremental small \nsteps. Also, improvements are usually piloted before roll out at the \norganization. \nPIM.3.BP6: Implement process changes.  The improvements to the \nprocesses are implemented. Process documentation is updated and \npeople are trained. [OUTCOME 4] \nNOTE 9: This practice includes defining the processes and making sure these \nprocesses are applied. Process application can be supported by establishing \npolicies, adequate process infrastructure (tools, templates, example artifacts, \netc.), process training, process coaching and tailoring processes to local \nneeds. \nPIM.3.BP7: Confirm process improvement.  The effects of process \nimplementation are monitored, measured and confirmed against the \ndefined improvement goals. [OUTCOME 5] \nNOTE 10: Examples of measures may be metrics for goal achievement, \nprocess definition and process adherence. \nPIM.3.BP8: Communicate results of improvement.  Knowledge gained \nfrom the improvements and progress of the improvement implementation \nis communicated outside of the improvement project across relevant parts \nof the organization and to the customer (as appropriate). [OUTCOME 6] \nPIM.3.BP9: Evaluate the results of the improvement project. Evaluate \nthe results of the improvement project to check whether the solution was \nsuccessful and can be used elsewhere in the organization. [OUTCOME 7] \n\n  \n \n \n \n© VDA Quality Management Center 76 \n \n \nOutput work \nproducts \n02-01 Commitment/agreement → [OUTCOME 1] \n05-00 Goals    → [OUTCOME 4] \n06-04 Training material   → [OUTCOME 4, 6] \n07-04 Process measure   → [OUTCOME 6] \n08-00 Plan    → [OUTCOME 2, 4, 7] \n08-29 Improvement plan  → [OUTCOME 4] \n10-00 Process description  → [OUTCOME 4] \n13-04 Communication record → [OUTCOME 6] \n13-16 Change request  → [OUTCOME 2] \n15-05 Evaluation report   → [OUTCOME 2, 3, 4, 5, 7] \n15-13 Assessment/audit report → [OUTCOME 3, 5] \n15-16 Improvement opportunity → [OUTCOME 2, 3, 4, 7] \n16-06 Process repository  → [OUTCOME 4]\n\n## Reuse process group (REU)\n\n\n\n### REU.2 Reuse Program Management\n\nProcess ID REU.2 \nProcess name Reuse Program Management \nProcess purpose The purpose of the Reuse Program Management Process is to plan, \nestablish, manage, control, and monitor an organization’s reuse program \nand to systematically exploit reuse opportunities. \nProcess \noutcomes \nAs a result of successful implementation of this process:  \n1) the reuse strategy, including its purpose, scope, goals and objectives, \nis defined; \n2) each domain is assessed to determine its reuse potential; \n3) the domains in which to investigate reuse opportunities, or in which it \nis intended to practice reuse, are identified; \n4) the organization's systematic reuse capability is assessed; \n5) reuse proposals are evaluated to ensure the reuse product is suitable \nfor the proposed application; \n6) reuse is implemented according to the reuse strategy; \n7) feedback, communication, and notification mechanisms are \nestablished, that operate between affected parties; and \n8) the reuse program is monitored and evaluated. \nBase practices REU.2.BP1: Define organizational reuse strategy.  Define the reuse \nprogram and necessary supporting infrastructure for the organization. \n[Outcome 1] \nREU.2.BP2: Identify domains for potential reuse.  Identify set(s) of \nsystems and their components in terms of common properties that can be \norganized into a collection of reusable assets that may be used to construct \nsystems in the domain. [OUTCOME 2] \n\n  \n \n \n \n© VDA Quality Management Center 77 \n \n \nREU.2.BP3: Assess domains for potential reuse. Assess each domain \nto identify potential use and applications of reusable components and \nproducts. [OUTCOME 3] \nREU.2.BP4: Assess reuse maturity. Gain an understanding of the reuse \nreadiness and maturity of the organization, to provide a baseline and \nsuccess criteria for reuse program management. [OUTCOME 4] \nREU.2.BP5: Evaluate reuse proposals.  Evaluate suitability of the \nprovided reusable components and product(s) to proposed use. \n[OUTCOME 5] \nREU.2.BP6: Implement the reuse program. Perform the defined activities \nidentified in the reuse program. [OUTCOME 6] \nREU.2.BP7: Get feedback from reuse. Establish feedback, assessment, \ncommunication and notification mechanism that operate between affected \nparties to control the progress of reuse program. [OUTCOME 7, 8] \nNOTE 1: Affected parties may include reuse program administrators, asset \nmanagers, domain engineers, developers, operators, and maintenance \ngroups. \nREU.2.BP8: Monitor reuse.  Monitor the implementation of the reuse \nprogram periodically and evaluate its suitability to actual needs. \n[OUTCOME 6, 8] \nNOTE 2: The quality requirements for re-use work products should be defined. \nOutput work \nproducts \n04-02 Domain architecture  → [OUTCOME 2] \n04-03 Domain model   → [OUTCOME 2] \n08-17 Reuse plan   → [OUTCOME 5, 6] \n09-03 Reuse policy   → [OUTCOME 1] \n12-03 Reuse proposal  → [OUTCOME 4] \n13-04 Communication record → [OUTCOME 7] \n15-07 Reuse evaluation report → [OUTCOME 5, 6, 8] \n15-13 Assessment/audit report → [OUTCOME 3, 4] \n19-05 Reuse strategy   → [OUTCOME 1] \n  \n\n  \n \n \n \n© VDA Quality Management Center 78\n\n# Process capability levels and process attributes\n\nProcess capability indicators are the means of achieving the capabilities addressed by the \nconsidered process attributes. Evidence of process capability indicators supports the judgment of \nthe degree of achievement of the process attribute. \nThe capability dimension of the process assessment model consists of six capability levels matching \nthe capability levels defined in ISO/IEC 33020. The process capability indicators for the 9 process \nattributes included in the capability dimension for process capability level 1 to 5 are described.  \nEach of the process attributes in this process assessment model is identical to the process attribute \ndefined in the process measurement framework. The generic practices address the chara cteristics \nfrom each process attribute. The generic resources relate to the process attribute as a whole. \nProcess capability level 0 does not include any type of indicators, as it reflects a non -implemented \nprocess or a process which fails to partially achieve any of its outcomes. \nNOTE: ISO/IEC 33020 process attribute definitions and attribute outcomes are duplicated from ISO/IEC \n33020 in italic font and marked with a left side bar.\n\n## Process capability Level 0: Incomplete process\n\nThe process is not implemented, or fails to achieve its process purpose. At this level there is little \nor no evidence of any systematic achievement of the process purpose.\n\n## Process capability Level 1: Performed process\n\nThe implemented process achieves its process purpose. The follo wing process attribute \ndemonstrates the achievement of this level\n\n### PA 1.1 Process performance process attribute\n\nThe process performance process attribute is a measure of the extent to which the process purpose \nis achieved. As a result of full achievement of this attribute: \na) the process achieves its defined outcomes \n \nGeneric practices GP 1.1.1 Achieve the process outcomes [ACHIEVEMENT a] \nAchieve the intent of the base practices. \nProduce work products that evidence the process outcomes. \nGeneric \nresources \nResources are used to achieve the intent of process specific \nbase practices [ACHIEVEMENT a]\n\n## Process capability Level 2: Managed process\n\nThe previously described Performed process is now implemented in a managed fashion (planned, \nmonitored and adjusted) and its work products are appropriately established, controlled and \nmaintained. \nThe following process attributes, together with the previously defined process attribute, \ndemonstrate the achievement of this level: \n\n  \n \n \n \n© VDA Quality Management Center 79\n\n### PA 2.1 Performance management process attribute\n\nThe performance management process attribute is a measure of the extent to which the \nperformance of the process is managed. As a result of full achievement of this process attribute: \na) Objectives for the performance of the process are identified; \nb) Performance of the process is planned; \nc) Performance of the process is monitored; \nd) Performance of the process is adjusted to meet plans; \ne) Responsibilities and authorities for performing the process are defined, assigned and \ncommunicated; \nf) Personnel performing the process are prepared for executing their responsibilities; \ng) Resources and information necessary for performing the process are identified, made available, \nallocated and used; \nh) Interfaces between the involved parties are managed to ensure both effective communic ation \nand clear assignment of responsibility. \n \nGeneric practices GP 2.1.1 Identify the objectives for the performance of the process. \n[ACHIEVEMENT a]  \nPerformance objectives are identified based on process requirements.  \nThe scope of the process performance is defined. \nAssumptions and constraints are considered when identifying the \nperformance objectives. \nNOTE 1: Performance objectives may include  \n(1) timely production of artifacts meeting the defined quality criteria, \n(2) process cycle time or frequency \n(3) resource usage; and \n(4) boundaries of the process. \nNOTE 2: At minimum, process performance objectives for resources, effort \nand schedule should be stated.  \nGP 2.1.2 Plan the performance of the process to fulfill the identified \nobjectives. [ACHIEVEMENT b] \nPlan(s) for the performance of the process are developed.  \nThe process performance cycle is defined. \nKey milestones for the performance of the process are established. \nEstimates for process performance attributes are determined and \nmaintained. \nProcess activities and tasks are defined. \nSchedule is defined and aligned with the approach to performing the \nprocess. \nProcess work product reviews are planned. \nGP 2.1.3 Monitor the performance of the process against the plans. \n[ACHIEVEMENT c]  \n\n  \n \n \n \n© VDA Quality Management Center 80 \n \n \nThe process is performed according to the plan(s). \nProcess performance is monitored to ensure planned results are \nachieved and to identify possible deviations \nGP 2.1.4 Adjust the performance of the process. [ACHIEVEMENT d] \nProcess performance issues are identified. \nAppropriate actions are taken when planned results and objectives are \nnot achieved. \nThe plan(s) are adjusted, as necessary.  \nRescheduling is performed as necessary. \nGP 2.1.5 Define responsibilities and authorities for performing the \nprocess. [ACHIEVEMENT e] \nResponsibilities, commitments and authorities to perform the process \nare defined, assigned and communicated. \nResponsibilities and authorities to verify process work products are \ndefined and assigned. \nThe needs for process performance experience, knowledge and skills \nare defined. \nGP 2.1.6 Identify, prepare, and make available resources to \nperform the process according to plan. [ACHIEVEMENT f, g] \nThe human and infrastructure resources, necessary for performing the \nprocess are identified made available, allocated and used. \nThe individuals performing and managing the process are prepared by \ntraining, mentoring, or coaching to execute their responsibilities. \nThe information necessary to perform the process is identified and \nmade available. \nGP 2.1. 7 Manage the interfaces between involved parties. \n[ACHIEVEMENT h] \nThe individuals and groups involved in the process performance are \ndetermined. \nResponsibilities of the involved parties are assigned. \nInterfaces between the involved parties are managed. \nCommunication is assured between the involved parties. \nCommunication between the involved parties is effective. \nGeneric resources Human resources with identified objectives, responsibilities and \nauthorities [ACHIEVEMENT e, f, h] \nFacilities and infrastructure resources [ACHIEVEMENT g, h] \nProject planning, management and control tools, including time and \ncost reporting [ACHIEVEMENT a, b, c, d] \nWorkflow management system [ACHIEVEMENT d, f, g, h] \n\n  \n \n \n \n© VDA Quality Management Center 81 \n \n \nEmail and/or other communication mechanisms \n[ACHIEVEMENT b, c, d, f, g, h] \nInformation and/or experience repository [ACHIEVEMENT b, d, e] \nProblem and issues management mechanisms [ACHIEVEMENT c]\n\n### PA 2.2 Work product management process attribute\n\nThe work product management process attribute is a measure of the extent to which the work \nproducts produced by the process are appropriately managed. As a result of full achievement of \nthis process attribute: \na) Requirements for the work products of the process are defined; \nb) Requirements for documentation and control of the work products are defined; \nc) Work products are appropriately identified, documented, and controlled; \nd) Work products are reviewed in accordance with planned arrangements and adjusted as \nnecessary to meet requirements. \nNOTE 1: Requirements for documentation and control of work products may include requirements for the \nidentification of changes and revision status, approval and re-approval of work products, distribution \nof work products, and for making relevant versions of applicable work produc ts available at points \nof use. \nNOTE 2: The work products referred to in this clause are those that result from the achievement of the \nprocess purpose through the process outcomes. \n \nGeneric practices GP 2.2.1 Define the requirements for the work products. \n[ACHIEVEMENT a] \nThe requirements for the work products to be produced are defined. \nRequirements may include defining contents and structure. \nQuality criteria of the work products are identified. \nAppropriate review and approval criteria for the work products are \ndefined. \nGP 2.2.2 Define the requirements for documentation and control of \nthe work products. [ACHIEVEMENT b] \nRequirements for the documentation and control of the work products \nare defined. Such requirements may include requirements for \n(1) distribution, \n(2) identification of work products and their components and  \n(3) traceability. \nDependencies between work products are identified and understood. \nRequirements for the approval of work products to be controlled are \ndefined. \nGP 2.2.3 Identify, document and control the work products. \n[ACHIEVEMENT c] \nThe work products to be controlled are identified. \n\n  \n \n \n \n© VDA Quality Management Center 82 \n \n \nChange control is established for work products.  \nThe work products are documented and controlled in accordance with \nrequirements. \nVersions of work products are assigned to product configurations as \napplicable. \nThe work products are made available through appropriate access \nmechanisms. \nThe revision status of the work products may readily be ascertained. \nGP 2.2.4 Review and adjust work products to meet the defin ed \nrequirements. [ACHIEVEMENT d] \nWork products are reviewed against the defined requirements in \naccordance with planned arrangements. \nIssues arising from work product reviews are resolved. \nGeneric resources Requirement management method/toolset [ACHIEVEMENT a, b, c] \nConfiguration management system [ACHIEVEMENT b, c] \nDocumentation elaboration and support tool [ACHIEVEMENT b, c] \nDocument identification and control procedure [ACHIEVEMENT b, c] \nWork product review methods and experiences [ACHIEVEMENT d] \nReview management method/toolset [ACHIEVEMENT d] \nIntranets, extranets and/or other communication mechanisms \n[ACHIEVEMENT b, c] \nProblem and issue management mechanisms [ACHIEVEMENT d]\n\n## Process capability Level 3: Established process\n\nThe previously described Managed process is now implemented using a defined process that is \ncapable of achieving its process outcomes. \nThe following process attributes, together with the previously defined process attributes, \ndemonstrate the achievement of this level:\n\n### PA 3.1 Process definition process attribute\n\nThe process definition process attribute is a measure of the extent to which a standard process is \nmaintained to support the deployment of the defined process. As a result of full achievement of this \nprocess attribute: \na) A standard process, including appropriate tailoring guidelines, is defined and maintained that \ndescribes the fundamental elements that must be incorporated into a defined process; \nb) The sequence and interaction of the standard process with other processes is determined. \nc) Required competencies and roles for performing the process are identified as part of the \nstandard process; \nd) Required infrastructure and work environment for performing the process are identified as part \nof the standard process; \n\n  \n \n \n \n© VDA Quality Management Center 83 \n \n \ne) Suitable methods and measures for monitoring the effectiveness and suitability of the process \nare determined. \n \nGeneric practices GP 3.1.1 Define and maintain the standard process that will support \nthe deployment of the defined process. [ACHIEVEMENT a] \nA standard process is developed and maintained that includes the \nfundamental process elements. \nThe standard process identifies the deployment needs and deployment \ncontext. \nGuidance and/or procedures are provided to support implementation of \nthe process as needed. \nAppropriate tailoring guideline(s) are available as needed. \nGP 3.1.2 Determine the sequence and interaction between \nprocesses so that they work as an integrated system of processes. \n[ACHIEVEMENT b] \nThe standard process’s sequence and interaction with other processes \nare determined. \nDeployment of the standard process as a defined process maintains \nintegrity of processes. \nGP 3.1.3 Identify the roles and competencies, responsibilities , and \nauthorities for performing the standard process. [ACHIEVEMENT c] \nProcess performance roles are identified \nCompetencies for performing the process are identified. \nAuthorities necessary for executing responsibilities are identified. \nGP 3.1.4 Identify the required infrastructure and work environment \nfor performing the standard process. [ACHIEVEMENT d] \nProcess infrastructure components are identified (facilities, tools, \nnetworks, methods, etc.). \nWork environment requirements are identified. \nGP 3.1.5 Determine suitable methods and measures to monitor the \neffectiveness and suitability of the standard process. [ACHIEVEMENT \ne] \nMethods and measures for monitoring the effectiveness and suitability \nof the process are determined. \nAppropriate criteria and data needed to monitor the effectiveness and \nsuitability of the process are defined. \nThe need to conduct internal audit and management review is \nestablished. \nProcess changes are implemented to maintain the standard process. \nGeneric resources Process modeling methods/tools [ACHIEVEMENT a, b, c, d] \nTraining material and courses [ACHIEVEMENT a, b, c, d] \n\n  \n \n \n \n© VDA Quality Management Center 84 \n \n \nResource management system [ACHIEVEMENT d] \nProcess infrastructure [ACHIEVEMENT a, b, d] \nAudit and trend analysis tools [ACHIEVEMENT e] \nProcess monitoring method [ACHIEVEMENT e]\n\n### PA 3.2 Process deployment process attribute\n\nThe process deployment process attribute is a measure of the extent to which the standard process \nis deployed as a defined process to achieve its process outcomes. As a result of full achievement \nof this process attribute: \na) A defined process is deployed based upon an a ppropriately selected and/or tailored standard \nprocess; \nb) Required roles, responsibilities and authorities for performing the defined process are assigned \nand communicated; \nc) Personnel performing the defined process are competent on the basis of appropriate education, \ntraining, and experience; \nd) Required resources and information necessary for performing the defined process are made \navailable, allocated and used; \ne) Required infrastructure and work environment for performing the defined process are made \navailable, managed and maintained; \nf) Appropriate data are collected and analysed as a basis for understanding the behaviour of the \nprocess, to demonstrate the suitability and effectiveness of the process, and to evaluate where \ncontinual improvement of the process can be made. \n \nGeneric practices GP 3.2.1 Deploy a defined process that satisfies the context specific \nrequirements of the use of the standard process. [ACHIEVEMENT a] \nThe defined process is appropriately selected and/or tailored from the \nstandard process. \nConformance of defined process with standard process requirements is \nverified. \nGP 3.2.2 Assign and communicate roles, responsibilities and \nauthorities for performing the defined process. [ACHIEVEMENT b] \nThe roles for performing the defined process are assigned and \ncommunicated. \nThe responsibilities and authorities for performing the defined process \nare assigned and communicated. \nGP 3.2.3 Ensure necessary competencies for performing the \ndefined process. [ACHIEVEMENT c] \nAppropriate competencies for assigned personnel are identified. \nSuitable training is available for those deploying the defined process. \nGP 3.2.4 Provide resources and information to support the \nperformance of the defined process. [ACHIEVEMENT d] \n\n  \n \n \n \n© VDA Quality Management Center 85 \n \n \nRequired human resources are made available, allocated and used. \nRequired information to perform the process is made available, \nallocated and used. \nGP 3.2.5 Provide adequate process infrastructure to support the \nperformance of the defined process. [ACHIEVEMENT e] \nRequired infrastructure and work environment is available. \nOrganizational support to effectively manage and maintain the \ninfrastructure and work environment is available. \nInfrastructure and work environment is used and maintained. \nGP 3.2.6 Collect and analyze data about performance of the process \nto demonstrate its suitability and effectiveness. [ACHIEVEMENT f] \nData required to understand the behavior, suitability and effectiveness \nof the defined process are identified. \nData is collected and analyzed to understand the behavior, suitability \nand effectiveness of the defined process. \nResults of the analysis are used to identify where continual \nimprovement of the standard and/or defined process can be made. \nNOTE 1: Data about process performance may be qualitative or \nquantitative. \nGeneric resources Feedback mechanisms (customer, staff, other stakeholders) \n[ACHIEVEMENT f] \nProcess repository [ACHIEVEMENT a] \nResource management system [ACHIEVEMENT b, c, d] \nKnowledge management system [ACHIEVEMENT a, b, d, f] \nProblem and change management system [ACHIEVEMENT f] \nWorking environment and infrastructure [ACHIEVEMENT d, e] \nData collection analysis system [ACHIEVEMENT f] \nProcess assessment framework [ACHIEVEMENT f] \nAudit/review system [ACHIEVEMENT f]\n\n## Process capability Level 4: Predictable process\n\nThe previously described Established process now operates predictively within defined limits to \nachieve its process outcomes. Quantitative management needs are identified, measurement data \nare collected and analysed to identify assignable causes of variation. Corrective action is taken to \naddress assignable causes of variation. \nThe following process attributes, together with the previously defined process attributes, \ndemonstrate the achievement of this level: \n\n  \n \n \n \n© VDA Quality Management Center 86\n\n### PA 4.1 Quantitative analysis process attribute\n\nThe quantitative analysis process attribute is a measure of the extent to which information needs \nare defined, relationships between process elements are identified and data are collected. As a \nresult of full achievement of this process attribute: \na) The process is aligned with quantitative business goals; \nb) Process information needs in support of relevant defined quantitative business goals are \nestablished; \nc) Process measurement objectives are derived from process information needs; \nd) Measurable relationships between pr ocess elements that contribute to the process \nperformance are identified; \ne) Quantitative objectives for process performance in support of relevant business goals are \nestablished; \nf) Appropriate measures and frequency of measurement are identified and defined in  line with \nprocess measurement objectives and quantitative objectives for process performance; \ng) Results of measurement are collected, validated and reported in order to monitor the extent to \nwhich the quantitative objectives for process performance are met. \nNOTE 1: Information needs typically reflect management, technical, project, process or product needs.  \n \nGeneric practices GP 4.1.1 Identify business goals. [ACHIEVEMENT a] \nBusiness goals are identified that are supported by the quantitatively \nmeasured process. \nGP 4.1.2 Establish process information needs. [ACHIEVEMENT a, b]  \nStakeholders of the identified business goals and the quantitatively \nmeasured process, and their information needs are identified, defined \nand agreed. \nGP 4.1.3 Derive process measurement objectives from process \ninformation needs. [ACHIEVEMENT a, c] \nThe process measurement objectives to satisfy the established process \ninformation needs are derived. \nGP 4.1.4 Identify measurable relationships between process \nelements. [ACHIEVEMENT a, d] \nIdentify the relationships between process elements, which contribute \nto the derived measurement objectives. \nGP 4.1.5 Establish quantitative objectives. [ACHIEVEMENT a, e]  \nEstablish quantitative objectives for the identified measurable process \nelements and their relationships. Agreement with process stakeholders \nis established. \nGP 4.1.6 Identify process measures that support the achievement \nof the quantitative objectives. [ACHIEVEMENT a, f] \nDetailed measures are defined to support monitoring, analysis an d \nverification needs of the quantitative objectives. \nFrequency of data collection is defined. \n\n  \n \n \n \n© VDA Quality Management Center 87 \n \n \nAlgorithms and methods to create derived measurement results from \nbase measures are defined, as appropriate. \nVerification mechanism for base and derived measures is defined. \nNOTE 1: Typically, the standard process definition is extended to include the \ncollection of data for process measurement. \nGP 4.1.7 Collect product and process measurement results through \nperforming the defined process. [ACHIEVEMENT a, g] \nData collection mechanism is created for all identified measures. \nRequired data is collected within the defined frequency, and recorded. \nMeasurement results are analyzed, and reported to the identified \nstakeholders. \nNOTE 2: A product measure can contribute to a process measure, e.g. the \nproductivity of testing characterized by the number of defects found in a \ngiven timeframe in relation to the product defect rate in the field. \nGeneric resources Management information (cost, time, reliability, profitability, customer \nbenefits, risks etc.) [ACHIEVEMENT a, b, c, d, e, f] \nApplicable measurement techniques [ACHIEVEMENT a, d] \nProduct and Process measurement tools and results databases \n[ACHIEVEMENT a, d, e, f, g] \nProcess measurement framework  [ACHIEVEMENT a, d, e, f, g] \nTools for data analysis and measurement [ACHIEVEMENT a, b, c, d, e, f]\n\n### PA 4.2 Quantitative control process attribute\n\nThe quantitative control process attribute is a measure of the extent to which objectiv e data are \nused to manage process performance that is predictable. As a result of full achievement of this \nprocess attribute: \na) Techniques for analyzing the collected data are selected; \nb) Assignable causes of process variation are determined through analysis of the collected data; \nc) Distributions that characterize the performance of the process are established; \nd) Corrective actions are taken to address assignable causes of variation; \ne) Separate distributions are established (as necessary) for analyzing the process under the \ninfluence of assignable causes of variation. \n \nGeneric practices GP 4.2.1 Select analysis techniques. [ACHIEVEMENT a] \nAnalysis methods and techniques for control of the process \nmeasurements are defined. \nGP 4.2.2 Establish distributions that charac terize the process \nperformance. [ACHIEVEMENT c] \nExpected distributions and corresponding control limits for measurement \nresults are defined. \n\n  \n \n \n \n© VDA Quality Management Center 88 \n \n \nGP 4.2.3 Determine assignable causes of process variation. \n[ACHIEVEMENT b] \nEach deviation from the defined control limits is identified and recorded. \nDetermine assignable causes of these deviations by analyzing collected \ndata using the defined analysis techniques. \nAll deviations and assigned causes are recorded. \nGP 4.2.4 Identify and implement corrective actions to add ress \nassignable causes. [ACHIEVEMENT d] \nCorrective actions are determined, recorded, and implemented to \naddress assignable causes of variation. \nCorrective action results are monitored and evaluated to determine their \neffectiveness. \nGP 4.2.5 Establish separate distributions for analyzing the process \n[ACHIEVEMENT e] \nSeparate distributions are used to quantitatively understand the variation \nof process performance under the influence of assignable causes. \nGeneric resources Process control and analysis techniques [ACHIEVEMENT a, c] \nStatistical analysis tools/applications [ACHIEVEMENT a, b, c, e] \nProcess control tools/applications [ACHIEVEMENT d, e]\n\n## Process capability Level 5: Innovating process\n\nThe previously described Predictable process is now continually improved to respond to change \naligned with organizational goals. \nThe following process attributes, together with the previously defined process attributes, \ndemonstrate the achievement of this level:\n\n### PA 5.1 Process innovation process attribute\n\nThe process innovation process attribute is a measure of the extent to which changes to the \nprocess are identified from investigations of innovative approaches to the definition and deployment \nof the process. As a result of full achievement of this process attribute: \na) Process innovation objectives are defined that support the relevant business goals; \nb) Appropriate data are analysed to identify opportunities for innovation; \nc) Innovation opportunities derived from new technologies and process concepts are identified; \nd) An implementation strategy is established to achieve the process innovation objectives. \n \nGeneric practices GP 5.1.1 Define the process innovation objectives for the process \nthat support the relevant business goals. [ACHIEVEMENT a] \nNew business visions and goals are analyzed to give guidance for new \nprocess objectives and potential areas of process innovation. \nGP 5.1.2 Analyze data of the process to identify opportunities for \ninnovation. [ACHIEVEMENT b] \n\n  \n \n \n \n© VDA Quality Management Center 89 \n \n \nCommon causes of variation in process performance are identified and \nanalyzed to get a quantitative understanding of their impact. \nIdentify opportunities for innovation based on the quantitative \nunderstanding of the analyzed data. \nGP 5.1.3 Analyze new technologies and process concepts  to \nidentify opportunities for innovation. [ACHIEVEMENT c] \nIndustry best practices, new technologies and process concepts are \nidentified and evaluated.  \nFeedback on opportunities for innovation is actively sought. \nEmergent risks are considered in evaluating improvement opportunities. \nGP 5.1.4 Define and maintain an implementation strategy based on \ninnovation vision and objectives. [ACHIEVEMENT d] \nCommitment to innovation is demonstrated by organizational \nmanagement including the process owner(s) and other relevant \nstakeholders. \nDefine and maintain an implementation strategy to achieve identified \nopportunities for innovation and objectives. \nBased on implementation strategy process changes are planned, \nprioritized based on their impact on defined innovations. \nMeasures that validate the results of process changes are defined to \ndetermine the expected effectiveness of the process changes and the \nexpected impact on defined business objectives.           \nGeneric resources Process improvement framework [ACHIEVEMENT a, c, d] \nProcess feedback and analysis system  (measurement data, causal \nanalysis results etc.) [ACHIEVEMENT b, c] \nPiloting and trialing mechanism [ACHIEVEMENT c, d]\n\n### PA 5.2 Process innovation implementation process attribute\n\nThe process innovation process implementation attribute is a measure of the extent to which \nchanges to the definition, management and performance of the process achieves the relevant \nprocess innovation objectives. As a result of full achievement of this process attribute: \na) Impact of all proposed changes is assessed against the objectives of the defined process and \nstandard process; \nb) Implementation of all agreed changes is managed to ensure that any disruption to the process \nperformance is understood and acted upon; \nc) Effectiveness of process change on the basis of actual performance is evaluated against the \ndefined product requirements and process objectives. \n \nGeneric practices GP 5.2.1 Assess the impact of each proposed change against the \nobjectives of the defined and standard process. [ACHIEVEMENT a] \nObjective priorities for process innovation are established. \n\n  \n \n \n \n© VDA Quality Management Center 90 \n \n \nSpecified changes are assessed against product quality and process \nperformance requirements and goals. \nImpact of changes to other defined and standard processes is \nconsidered. \nGP 5.2.2. Manage the implementation of agreed  changes. \n[ACHIEVEMENT b] \nA mechanism is established for incorporating accepted changes into \nthe defined and standard process(es) effectively and completely. \nThe factors that impact the effectiveness and full deployment of the \nprocess change are identified and managed, such as: \n• Economic factors (productivity, profit, growth, efficiency, quality, \ncompetition, resources, and capacity ); \n• Human factors (job satisfaction, motivation, morale, \nconflict/cohesion, goal consensus, participation, training, span of \ncontrol); \n• Management factors (skills, commitment, leadership, knowledge, \nability, organizational culture and risks); \n• Technology factors (sophistication of system, technical expertise, \ndevelopment methodology, need of new technologies). \nTraining is provided to users of the process. \nProcess changes are effectively communicated to all affected parties. \nRecords of the change implementation are maintained. \nGP 5.2.3 Evaluate the effectiveness of process  change. \n[ACHIEVEMENT c] \nPerformance and capability of the changed process are measured and \nevaluated against process objectives and historical data. \nA mechanism is available for documenting and reporting analysis \nresults to management and owners of standard and defined process. \nMeasures are analyzed to determine whether the process performance \nhas improved with respect to common causes of variations. \nOther feedback is recorded, such as opportunities for further innovation \nof the predictable process. \nGeneric resources Change management system [ACHIEVEMENT a, b, c] \nProcess evaluation system (impact analysis, etc.) [ACHIEVEMENT a, c] \n\n  \n \n \n \n© VDA Quality Management Center 91 \n \n \nAnnex A Conformity of the process assessment and reference model  \nA.1 Introduction \nThe Automotive SPICE process assessment and process reference model are meeting the \nrequirements for conformance defined in ISO/IEC  33004. The process assessment model can be \nused in the performance of assessments that meet the requirements of ISO/IEC 33002. \nThis clause serves as the statement of conformance of the process ass essment and process \nreference models to the requirements defined in ISO/IEC 33004. \n[ISO/IEC 33004, 5.5 and 6.4] \nDue to copyright reasons each  requirement is only referred by its number . T he full text  of the \nrequirements can be drawn from ISO/IEC 33004. \nA.2 Conformance to the requirements for process reference models \nClause 5.3, \"Requirements for process reference models\" \nThe following information is provided in chapter 1 and 3 of this document: \n• the declaration of the domain of this process reference model; \n• the description  of the relationship between this  process reference model and its intended \ncontext of use; and \n• the description of the relationship between the processes defined within this process reference \nmodel. \nThe descriptions of the processes within the  scope of th is process reference model  meeting the \nrequirements of ISO/IEC 33004 clause 5.4 is provided in chapter 4 of this document. \n [ISO/IEC 33004, 5.3.1] \nThe relevant communities of  interest and their mode of use  and the consensus achieved for this  \nprocess reference model is documented in the copyright notice and the scope of this document.  \n[ISO/IEC 33004, 5.3.2] \nThe process descriptions are unique. The identification is provided by unique names and by the \nidentifier of each process of this document. \n[ISO/IEC 33004, 5.3.3] \nClause 5.4, \"Process descriptions\" \nThese requirements are met by the process descriptions in chapter 4 of this document. \n[ISO/IEC 33004, 5.4] \n  \n\n  \n \n \n \n© VDA Quality Management Center 92 \n \n \nA.3 Conformance to the requirements for process assessment models \nClause 6.1, \"Introduction\" \nThe purpose of this process assessment model is to support assessment of process capability within \nthe automotive domain using the process measurement framework defined in ISO/IEC 33020.  \n[ISO/IEC 33004, 6.1] \nClause 6.2, \"Process assessment model scope\" \nThe process scope of this process assessment model is defined in the process reference model  \nincluded in chapter 3.1 of this document. The Automotive SPICE process reference m odel is \nsatisfying the requirements of ISO/IEC 33004, clause 5 as described in Annex A.2. \nThe process capability scope of this process assessment model is defined in the process \nmeasurement framework  specified in ISO/IEC 33020 , which defines a process measurement \nframework for process capability satisfying the requirements of ISO/IEC 33003. \n[ISO/IEC 33004, 6.2] \nClause 6.3, \"Requirements for process assessment models\" \nThe Automotive SPICE process assessment model is related to process capability. \n[ISO/IEC 33004, 6.3.1] \n \nThis process assessment model incorporates the process measurement framework specified in \nISO/IEC 33020, which satisfies the requirements of ISO/IEC 33003. \n[ISO/IEC 33004, 6.3.2] \n \nThis process assessment model is based on the Automotive SPICE Reference Model included in \nthis document. \nThis process assessment model is base d on the Measurement Framework defined in ISO/IEC \n33020. \n[ISO/IEC 33004, 6.3.3] \n \nThe processes included in this  process assessment model are identical to those specified  in the \nProcess Reference Model \n[ISO/IEC 33004, 6.3.4] \n \nFor all processes in this process assessment model all levels defined in the process measurement \nframework from ISO/IEC 33020 are addressed. \n[ISO/IEC 33004, 6.3.5] \n \nThis process assessment model defines  \n• the selected process quality characteristic; \n• the selected process measurement framework; \n• the selected process reference model(s); \n• the selected processes from the process reference model(s) \n\n  \n \n \n \n© VDA Quality Management Center 93 \n \n \nin chapter 3 of this document. \n[ISO/IEC 33004, 6.3.5 a-d] \n \nIn the capability dimension, this process assessment model addresses all of the process attributes \nand capability levels defined in the process measurement framework in ISO/IEC 33020. \n[ISO/IEC 33004, 6.3.5 e] \nClause 6.3.1, \"Assessment indicators\" \nNOTE: Due to an error in numbering in the published version of ISO/IEC 33004 the following  reference \nnumbers are redundant to those stated above. To refer to the correct clauses from ISO/IEC 33004, the \ntext of clause heading is additionally specified for the following three requirements.  \nThe Automotive SPICE process assessment model provides a two-dimensional view of process \ncapability for the processes in the process reference model, through the inclusion of assessment \nindicators as defined in chapter 3.3. The assessment indicators used are: \n• Base practices and output work products \n [ISO/IEC 33004, 6.3.1 a, \"Assessment indicators\"] \n \n• Generic practices and Generic resources \n[ISO/IEC 33004, 6.3.1 b, \"Assessment indicators\"] \nClause 6.3.2, \"Mapping process assessment models to process reference models\" \nThe mapping of the assessment indicators to the purpose and process outcomes of the processes \nin the process reference model is included in each description of the base practices in chapter 4.  \nThe mapping of the assessment indicators to the process attributes in the process measurement \nframework including all of the process attribute achievements is included in each description of the \ngeneric practices in chapter 5. \nEach mapping is indicated by a reference in square brackets. \n[ISO/IEC 33004, 6.3.2, \"Mapping process assessment models\"] \nClause 6.3.3, \"Expression of assessment results\" \nThe process attributes and the process attribute ratings in this process assessment model are \nidentical to those defined in the measurement framework. As a consequence, results of assessments \nbased upon this process ass essment model are expressed directly as a set of process attribute \nratings for each process within the scope of the assessment. No form of translation or conversion is \nrequired. \n[ISO/IEC 33004, 6.3.3, \"Expression of assessment results\"] \n\n  \n \n \n \n© VDA Quality Management Center 94 \n \n \nAnnex B Work product characteristics \nWork product characteristics listed in this Annex can be used when reviewing potential outputs of \nprocess implementation. The characteristics are provided as guidance for the attributes to look for, \nin a particular sample work product, to provide objective evidence supporting the assessment of a \nparticular process. \nA documented process and assessor judgment is needed to ensure that the process context \n(application domain, business purpose, development methodology, size of the organization, etc.) is \nconsidered when using this information. \nWork products are defined using the schema in  table B.1. Work products and their characteristics \nshould be considered as a starting point for considering whether, given the  context, they are \ncontributing to the intended purpose of the process, not as a check -list of what every organization \nmust have.  \nTable B.1 — Structure of WPC tables \nWork product \nidentifier \nAn identifier number for the work product which is used to reference the work product. \nWork product \nname \nProvides an example of a typical name associated with the work product \ncharacteristics. This name is provided as an identifier of the type of work product the \npractice or process might produce. Organizations may call these work products by \ndifferent names. The name of the work product in the organization is not significant. \nSimilarly, organizations may have several equivalent work products which contain the \ncharacteristics defined in one work product type. The formats for the work products can \nvary. It is up to the assessor and the organizational unit coordinator to map the actual \nwork products produced in their organization to the examples given here.  \nWork product \ncharacteristics \nProvides examples of the potential characteristics associated with the work product \ntypes. The assessor may look for these in the samples provided by the organizational \nunit.  \nWork products (with the ID NN-00) are sets of characteristics that would be expected to be evident \nin work products  of generic types as a result of achievement of an attribute. The generic work \nproducts form the basis for the classification of specific work products defined as process \nperformance indicators.  \nSpecific work product types are typically created by process  owners and applied by process \ndeployers in order to satisfy an outcome of a particular process purpose. \nNOTE: The generic work products denoted with * are not used in the Automotive SPICE process \nassessment model but are included for completeness. \nTable B.2 — Work product characteristics \nWP ID WP Name WP Characteristics \n01-00 Configuration item \n \n• Item which is maintained under configuration control: \n- may include components, subsystems, libraries, test cases, \ncompilers, data, documentation, physical media, and external \ninterfaces \n• Version identification is maintained \n• Description of the item is available including the: \n- type of item \n- associated configuration management library, file, system \n- responsible owner \n- date when placed under configuration control \n- status information (i.e., development, baselined, released) \n- relationship to lower level configured items \n\n  \n \n \n \n© VDA Quality Management Center 95 \n \n \nWP ID WP Name WP Characteristics \n- identification of the change control records \n- identification of change history \n01-03 Software item • Integrated software consisting of: \n- source code \n- software elements \n- executable code \n- configuration files \n• Documentation, which: \n- describes and identifies source code \n- describes and identifies software elements \n- describes and identifies configuration files \n- describes and identifies executable code \n- describes software life-cycle status \n- describes archive and release criteria \n- describes compilation of software units \n- describes building of software item \n01-50 Integrated software • An aggregate of software items \n• A set of executables for a specific ECU configuration and possibly \nassociated documentation and data \n01-51 Application \nparameter \n \n• Name \n• Description \n• Value domain, threshold values, characteristic curves \n• Owner \n• Means of data application (e.g. flashing interfaces) \n• If necessary a grouping/a categorization: \n- name of the category/group/file name \n- description \n• Actual value or characteristic curve applied  \n02-00 Contract • Defines what is to be purchased or delivered \n• Identifies time frame for delivery or contracted service dates \n• Identifies any statutory requirements \n• Identifies monetary considerations \n• Identifies any warranty information \n• Identifies any copyright and licensing information \n• Identifies any customer service requirements \n• Identifies service level requirements \n• References to any performance and quality \nexpectations/constraints/monitoring  \n• Standards and procedures to be used \n• Evidence of review and approval \n• As appropriate to the contract the following are considered: \n- references to any acceptance criteria \n- references to any special customer needs (i.e., confidentiality \nrequirements, security, hardware, etc.) \n- references to any change management and problem resolution \nprocedures \n- identification of any interfaces to independent agents and \nsubcontractors \n- identification of customer's role in the development and \nmaintenance process \n- identification of resources to be provided by the customer \n02-01 Commitment / \nagreement \n• Signed off by all parties involved in the commitment/agreement \n\n  \n \n \n \n© VDA Quality Management Center 96 \n \n \nWP ID WP Name WP Characteristics \n• Establishes what the commitment is for \n• Establishes the resources required to fulfill the commitment, such as: \n- time \n- people \n- budget \n- equipment \n- facilities \n03-00 * Data • Result of applying a measure \n03-03 Benchmarking data • Results of measurement of current performance that allow comparison \nagainst historical or target values \n• Relates to key goals/process/product/market need criteria and \ninformation to be benchmarked \n03-04 Customer \nsatisfaction data \n• Determines levels of customer satisfaction with products and services \n• Mechanism to collect data on customer satisfaction: \n- results of field performance data \n- results of customer satisfaction survey \n- interview notes \n- meeting minutes from customer meetings \n03-06 Process \nperformance data \n• Data comparing process performance against expected levels \n• Defined input and output work products available \n• Meeting minutes \n• Change records \n• Task completion criteria met \n• Quality criteria met \n• Resource allocation and tracking \n04-00 * Design • Describes the overall product/system structure \n• Identifies the required product/system elements \n• Identifies the relationship between the elements \n• Consideration is given to: \n- any required performance characteristics \n- any required interfaces \n- any required security characteristics \n04-02 Domain architecture • Identified domain model(s) tailored from \n• Identified asset specifications \n• Definition of boundaries and relationships with other domains (Domain \nInterface Specification) \n• Identification of domain vocabulary \n• Identification of the domain representation standard \n• Provides an overview of the functions, features capabilities and \nconcepts in the domains \n04-03 Domain model • Must provide a clear explanation and description, on usage and \nproperties, for reuse purposes \n• Identification of the management and structures used in the model \n04-04 \n \nSoftware \narchitectural design  \n \n• Describes the overall software structure \n• Describes the operative system including task structure \n• Identifies inter-task/inter-process communication \n• Identifies the required software elements \n• Identifies own developed and supplied code \n• Identifies the relationship and dependency between software elements \n• Identifies where the data (such as application parameters or variables) \nare stored and which measures (e.g. checksums, redundancy) are \ntaken to prevent data corruption  \n\n  \n \n \n \n© VDA Quality Management Center 97 \n \n \nWP ID WP Name WP Characteristics \n• Describes how variants for different model series or configurations are \nderived \n• Describes the dynamic behavior of the software (Start-up, shutdown, \nsoftware update, error handling and recovery, etc.) \n• Describes which data is persistent and under which conditions  \n• Consideration is given to: \n- any required software performance characteristics \n- any required software interfaces \n- any required security characteristics required \n- any database design requirements \n04-05 \n \nSoftware detailed \ndesign \n• Provides detailed design (could be represented as a prototype, flow \nchart, entity relationship diagram, pseudo code, etc.) \n• Provides format of input/output data \n• Provides specification of CPU, ROM, RAM, EEPROM and Flash needs \n• Describes the interrupts with their priorities \n• Describes the tasks with cycle time and priority \n• Establishes required data naming conventions \n• Defines the format of required data structures \n• Defines the data fields and purpose of each required data element \n• Provides the specifications of the program structure \n04-06 System architectural \ndesign \n• Provides an overview of all system design \n• Describes the interrelationship between system elements \n• Describes the relationship between the system elements and the \nsoftware \n• Specifies the design for each required system element, consideration is \ngiven to aspects such as: \n- memory/capacity requirements \n- hardware interface requirements \n- user interface requirements \n- external system interface requirements \n- performance requirements \n- command structures \n- security/data protection characteristics \n- settings for system parameters (such as application parameters or \nglobal variables) \n- manual operations \n- reusable components \n• Mapping of requirements to system elements \n• Description of the operation modes of the system components (startup, \nshutdown, sleep mode, diagnosis mode, etc.) \n• Description of the dependencies among the system components \nregarding the operation modes \n• Description of the dynamic behavior of the system and the system \ncomponents \n05-00 Goals • Identifies the objective to be achieved \n• Identifies who is expected to achieve the goal \n• Identifies any incremental supporting goals \n• Identifies any conditions/constraints \n• Identifies the timeframe for achievement \n• Are reasonable and achievable within the resources allocated \n• Are current, established for current project, organization \n• Are optimized to support known performance criteria and plans \n06-00 User documentation • Identifies: \n\n  \n \n \n \n© VDA Quality Management Center 98 \n \n \nWP ID WP Name WP Characteristics \n- external documents \n- internal documents \n- current site distribution and maintenance list maintained \n• Documentation kept synchronized with latest product release \n• Addresses technical issues \n06-01 Customer manual • Takes account of: \n- audience and task profiles \n- the environment in which the information will be used \n- convenience to users \n- the range of technical facilities, including resources and the \nproduct, available for developing and delivering on-screen \ndocumentation \n- information characteristics \n- cost of delivery and maintainability \n• Includes information needed for operation of the system, including but \nnot limited to: \n- product and version information \n- instructions for handling the system \n- initial familiarization information \n- long examples \n- structured reference material, particularly for advanced features of \nthe software \n- checklists \n- guides to use input devices \n06-02 Handling and \nstorage guide \n• Defines the tasks to perform in handling and storing products including:  \n- providing for master copies of code and documentation \n- disaster recovery \n- addressing appropriate critical safety and security issues \n• Provides a description of how to store the product including: \n- storage environment required \n- the protection media to use \n- packing materials required \n- what items need to be stored \n- assessments to be done on stored product \n• Provides retrieval instructions \n06-04 Training material • Updated and available for new releases \n• Coverage of system, application, operations, maintenance as \nappropriate to the application \n• Course listings and availability \n07-00 Measure • Available to those with a need to know \n• Understood by those expected to use them \n• Provides value to the organization/project \n• Non-disruptive to the work flow \n• Appropriate to the process, life cycle model, organization: \n- is accurate \n- source data is validated \n- results are validated to ensure accuracy \n• Has appropriate analysis and commentary to allow meaningful \ninterpretation by users \n07-01 Customer \nsatisfaction survey \n• Identification of customer and customer information \n• Date requested \n• Target date for responses \n• Identification of associated hardware/software/product configuration \n\n  \n \n \n \n© VDA Quality Management Center 99 \n \n \nWP ID WP Name WP Characteristics \n• Ability to record feedback \n07-02 Field measure • Measures attributes of the performance of system's operation at field \nlocations, such as: \n- field defects \n- performance against defined service level measures \n- system ability to meet defined customer requirements \n- support time required \n- user complaints (may be third party users) \n- customer requests for help \n- performance trends \n- problem reports \n- enhancements requested \n07-03 Personnel \nperformance \nmeasure \n• Real time measures of personnel performance or expected service \nlevel \n• Identifies aspects such as: \n- capacity \n- throughput \n- operational performance \n- operational service \n- availability \n07-04 Process measure • Measures about the process' performance: \n- ability to produce sufficient work products \n- adherence to the process \n- time it takes to perform process \n- defects related to the process \n• Measures the impact of process change \n• Measures the efficiency of the process \n07-05 Project measure • Monitors key processes and critical tasks, provides status information \nto the project on: \n- project performance against established plan \n- resource utilization against established plan \n- time schedule against established plan \n- process quality against quality expectations and/or criteria \n- product quality against quality expectations and/or criteria \n- highlight product performance problems, trends \n• Measures the results of project activities: \n- tasks are performed on schedule \n- product's development is within the resource commitments \nallocated \n• References any goals established \n07-06 Quality measure • Measures quality attributes of the work products defined: \n- functionality \n- reliability \n- usability \n- efficiency \n- maintainability \n- portability \n• Measures quality attributes of the \"end customer\" product quality and \nreliability \nNOTE: Refer ISO/IEC 25010 for detailed information on measurement of \nproduct quality. \n07-07 Risk measure • Identifies the probability of risk occurring \n\n  \n \n \n \n© VDA Quality Management Center 100 \n \n \nWP ID WP Name WP Characteristics \n• Identifies the impact of risk occurring  \n• Establishes measures for each risk defined \n• Measures the change in the risk state \n07-08 Service level \nmeasure \n• Real time measures taking while a system is operational, it measures \nthe system's performance or expected service level \n• Identifies aspects such as: \n- capacity \n- throughput \n- operational performance \n- operational service \n- service outage time \n- up time \n- job run time \n08-00 Plan As appropriate to the application and purpose: \n• Identifies what objectives or goals there are to be satisfied \n• Establishes the options and approach for satisfying the objectives, or \ngoals \n• Identification of the plan owner \n• Includes: \n- the objective and scope of what is to be accomplished \n- assumptions made \n- constraints \n- risks \n- tasks to be accomplished \n- schedules, milestones and target dates \n- critical dependencies \n- maintenance disposition for the plan \n• Method/approach to accomplish plan \n• Identifies: \n- task ownership, including tasks performed by other parties (e.g. \nsupplier, customer) \n- quality criteria \n- required work products \n• Includes resources to accomplish plan objectives: \n- time \n- staff (key roles and authorities e.g. sponsor) \n- materials/equipment \n- budget \n• Includes contingency plan for non-completed tasks \n• Plan is approved \n08-04 Configuration \nmanagement plan \n• Defines or references the procedures to control changes to \nconfiguration items \n• Defines measurements used to determine the status of the \nconfiguration management activities \n• Defines configuration management audit criteria \n• Approved by the configuration management function \n• Identifies configuration library tools or mechanism \n• Includes management records and status reports that show the status \nand history of controlled items \n• Specifies the location and access mechanisms for the configuration \nmanagement library \n• Storage, handling and delivery (including archival and retrieval) \nmechanisms specified \n\n  \n \n \n \n© VDA Quality Management Center 101 \n \n \nWP ID WP Name WP Characteristics \n08-12 Project plan • Defines: \n- work products to be developed \n- life cycle model and methodology to be used \n- customer requirements related to project management \n- project resources \n• Milestones and target dates: \n- estimates \n- quality criteria \n- processes and methods to employ \n- contingency actions  \n08-13 Quality plan • Objectives/goal for quality \n• Defines the activities tasks required to ensure quality \n• References related work products \n• References any regulatory requirements, standards, customer \nrequirements \n• Identifies the expected quality criteria \n• Specifies the monitoring and quality checkpoints for the defined life \ncycle and associated activities planned \n• Defines the methods of assuring quality \n• Identifies the quality criteria for work products and process tasks \n• Specifies the threshold/tolerance level allowed prior to requiring \ncorrective actions \n• Defines quality measurements and timing of the collection \n• Specifies mechanism to feed collected quality record back into process \nimpacted by poor quality \n• Defines the approach to guaranteeing objectivity \n• Approved by the quality responsible organization/function: \n- identifies escalations opportunities and channels  \n- defines the cooperation with customer and supplier QA \n08-14 Recovery plan • Identifies what is to be recovered: \n- procedures/methods to perform the recovery \n- schedule for recovery \n- time required for the recovery \n- critical dependencies \n- resources required for the recovery \n- list of backups maintained \n- staff responsible for recovery and roles assigned \n- special materials required \n- required work products \n- required equipment \n- required documentation \n- locations and storage of backups \n- contact information on who to notify about the recovery \n- verification procedures \n- cost estimation for recovery \n08-16 Release plan • Identifies the functionality to be included in each release \n• Identifies the associated elements required (i.e., hardware, software, \ndocumentation etc.) \n• Mapping of the customer requests, requirements satisfied to particular \nreleases of the product \n08-17 Reuse plan • Defines the policy about what items to be reused \n• Defines standards for construction of reusable objects: \n- defines the attributes of reusable components \n\n  \n \n \n \n© VDA Quality Management Center 102 \n \n \nWP ID WP Name WP Characteristics \n- quality/reliability expectations \n- standard naming conventions \n• Defines the reuse repository (library, CASE tool, file, data base, etc.) \n• Identifies reusable components: \n- directory of component \n- description of components \n- applicability of their use \n- method to retrieve and use them \n- restrictions for modifications and usage \n• Method for using reusable components \n• Establishes goal for reusable components \n08-18 Review plan • Defines: \n- what to be reviewed \n- roles and responsibilities of reviewers \n- criteria for review (check-lists, requirements, standards) \n- expected preparation time \n- schedule for reviews \n• Identification of: \n- procedures for conducting review \n- review inputs and outputs \n- expertise expected at each review \n- review records to keep \n- review measurements to keep \n- resources, tools allocated to the review \n08-19 Risk management \nplan \n• Project risks identified and prioritized \n• Mechanism to track the risk \n• Threshold criteria to identify when corrective action required \n• Proposed ways to mitigate risks: \n- risk mitigator \n- work around \n- corrective actions activities/tasks \n- monitoring criteria \n- mechanisms to measure risk \n08-20 Risk mitigation plan • Planned risk treatment activities and tasks: \n- describes the specifics of the risk treatment selected for a risk or \ncombination of risks found to be unacceptable \n- describes any difficulties that may be found in implementing the \ntreatment \n• Treatment schedule \n• Treatment resources and their allocation \n• Responsibilities and authority: \n- describes who is responsible for ensuring that the treatment is \nbeing implemented and their authority \n• Treatment control measures: \n- defines the measures that will be used to evaluate the \neffectiveness of the risk treatment \n• Treatment cost \n• Interfaces among parties involved: \n- describes any coordination among stakeholders or with the \nproject’s master plan that must occur for the treatment to be \nproperly implemented \n• Environment/infrastructure: \n\n  \n \n \n \n© VDA Quality Management Center 103 \n \n \nWP ID WP Name WP Characteristics \n- describes any environmental or infrastructure requirements or \nimpacts (e.g., safety or security impacts that the treatment may \nhave) \n• Risk treatment plan change procedures and history \n08-26 Documentation plan \n \n• Identifies documents to be produced \n• Defines the documentation activities during the life cycle of the \nsoftware product or service \n• Identifies any applicable standards and templates \n• Defines requirements for documents \n• Review and authorization practices \n• Distribution of the documents  \n• Maintenance and disposal of the documents \n08-27 Problem \nmanagement plan \n• Defines problem resolution activities including identification, recording, \ndescription and classification \n• Problem resolution approach: evaluation and correction of the problem  \n• Defines problem tracking \n• Mechanism to collect and distribute problem resolutions \n08-28 Change \nmanagement plan \n• Defines change management activities including identification, \nrecording, description, analysis and implementation \n• Defines approach to track status of change requests \n• Defines verification and validation activities \n• Change approval and implication review \n08-29 Improvement plan • Improvement objectives derived from organizational business goals \n• Organizational scope \n• Process scope, the processes to be improved \n• Key roles and responsibilities \n• Appropriate milestones, review points and reporting mechanisms \n• Activities to be performed to keep all those affected by the \nimprovement program informed of progress \n08-50 Test specification • Test Design Specification  \n• Test Case Specification \n• Test Procedure Specification \n• Identification of test cases for regression testing \n• Additionally, for system integration: \n- identification of required system elements (hardware elements, \nwiring elements, settings for parameters (such as application \nparameters or global variables) , data bases, etc.) \n- necessary sequence or ordering identified for integrating the \nsystem elements \n08-51 Technology \nmonitoring plan \nNo requirements additional to Plan (Generic) \n08-52 Test plan • Test Plan according to ISO29119-3 \n• Context: \n- project/Test sub-process \n- test item(s) \n- test scope \n- assumptions and constraints \n- stakeholder \n- testing communication \n• Test strategy \n- identifies what needs there are to be satisfied \n\n  \n \n \n \n© VDA Quality Management Center 104 \n \n \nWP ID WP Name WP Characteristics \n- establishes the options and approach for satisfying the needs \n(black-box and/or white-box-testing, boundary class test \ndetermination, regression testing strategy, etc.)  \n- establishes the evaluation criteria against which the strategic \noptions are evaluated \n- identifies any constraints/risks and how these will be addressed \n- test design techniques \n- test completion criteria \n- test ending criteria \n- test start, abort and re-start criteria \n- metrics to be collected \n- test data requirements \n- retesting and regression testing \n- suspension and resumption criteria \n- deviations from the Organizational Test Strategy \n• Test data requirements \n• Test environment requirements \n• Test sub-processes \n• Test deliverables \n• Testing activities and estimates \n09-00 Policy • Authorized \n• Available to all personnel impacted by the policy \n• Establishes practices/rules to be adhered to \n09-03 Reuse policy • Identification of reuse requirements \n• Establishes the rules of reuse \n• Documents the reuse adoption strategy including goals and objectives  \n• Identification of the reuse program \n• Identification of the name of the reuse sponsor \n• Identification of the reuse program participants \n• Identification of the reuse steering function \n• Identification of reuse program support functions \n10-00 Process description • A detailed description of the process/procedure which includes: \n- tailoring of the standard process (if applicable) \n- purpose of the process \n- outcomes of the process \n- task and activities to be performed and ordering of tasks \n- critical dependencies between task activities \n- expected time required to execute task \n- input/output work products \n- links between input and outputs work products \n• Identifies process entry and exit criteria \n• Identifies internal and external interfaces to the process \n• Identifies process measures \n• Identifies quality expectations \n• Identifies functional roles and responsibilities \n• Approved by authorized personnel \n11-00 Product • Is a result/deliverable of the execution of a process, includes services, \nsystems (software and hardware) and processed materials \n• Has elements that satisfy one or more aspects of a process purpose \n• May be represented on various media (tangible and intangible) \n11-03 Product release \ninformation \n• Coverage for key elements (as appropriate to the application): \n• Description of what is new or changed (including features removed) \n• System information and requirements \n\n  \n \n \n \n© VDA Quality Management Center 105 \n \n \nWP ID WP Name WP Characteristics \n• Identification of conversion programs and instructions \n• Release numbering implementation may include: \n- the major release number \n- the feature release number \n- the defect repair number \n- the alpha or beta release; and the iteration within the alpha or beta \nrelease \n• Identification of the component list (version identification included): \n- hardware / software / product elements, libraries, etc. \n- associated documentation list \n• New/changed parameter information (e.g. for application parameters or \nglobal variables) and/or commands \n• Backup and recovery information \n• List of open known problems, faults, warning information, etc. \n• Identification of verification and diagnostic procedures \n• Technical support information \n• Copyright and license information \n• The release note may include an introduction, the environmental \nrequirements, installation procedures, product invocation, new feature \nidentification and a list of defect resolutions, known defects and \nworkarounds \n11-04 Product release \npackage \n• Includes the hardware/software/product \n• Includes and associated release elements such as: \n- system hardware/software/product elements \n- associated customer documentation \n- application parameter definitions defined \n- command language defined \n- installation instructions \n- release letter \n11-05 Software unit • Follows established coding standards (as appropriate to the language \nand application): \n- commented \n- structured or optimized \n- meaningful naming conventions \n- parameter information identified \n- error codes defined \n- error messages descriptive and meaningful \n- formatting - indented, levels \n• Follows data definition standards (as appropriate to the language and \napplication): \n- variables defined \n- data types defined \n- classes and inheritance structures defined \n- objects defined \n• Entity relationships defined \n• Database layouts are defined \n• File structures and blocking are defined \n• Data structures are defined \n• Algorithms are defined  \n• Functional interfaces defined \n11-06 System • All elements of the product release are included \n• Any required hardware \n• Integrated product \n\n  \n \n \n \n© VDA Quality Management Center 106 \n \n \nWP ID WP Name WP Characteristics \n• Customer documentation \n• Fully configured set of the system elements: \n- application parameters defined \n- commands defined \n- data loaded or converted \n11-07 Temporary solution • Problem identification \n• Release and system information \n• Temporary solution, target date for actual fix identified \n• Description of the solution: \n- limitations, restriction on usage \n- additional operational requirements \n- special procedures \n- applicable releases \n• Backup/recovery information \n• Verification procedures \n• Temporary installation instructions \n12-00 Proposal • Defines the proposed solution \n• Defines the proposed schedule \n• Identifies the coverage identification of initial proposal: \n- identifies the requirements that would be satisfied \n- identifies the requirements that could not be satisfied, and provides \na justification of variants \n• Defines the estimated price of proposed development, product, or \nservice \n12-01 Request for proposal • Reference to the requirements specifications \n• Identifies supplier selection criteria \n• Identifies desired characteristics, such as: \n- system architecture, configuration requirements or the \nrequirements for service (consultants, maintenance, etc.) \n- quality criteria or requirements \n- project schedule requirements \n- expected delivery/service dates \n- cost/price expectations \n- regulatory standards/requirements \n• Identifies submission constraints: \n- date for resubmission of the response \n- requirements with regard to the format of response \n12-03 Reuse proposal • Identifies the project name \n• Identifies the project contact \n• Identifies the reuse goals and objectives \n• Identifies the list of reuse assets \n• Identifies the issues/risks of reusing the component including specific \nrequirements (hardware, software, resource and other reuse \ncomponents) \n• Identifies the person who will be approving the reuse proposal \n12-04 Supplier proposal \nresponse \n• Defines the suppliers proposed solution \n• Defines the suppliers proposed delivery schedule \n• Identifies the coverage identification of initial proposal: \n- identifies the requirements that would be satisfied \n- identifies the requirements that could not be satisfied, and provides \na justification of variants \n\n  \n \n \n \n© VDA Quality Management Center 107 \n \n \nWP ID WP Name WP Characteristics \n• Defines the estimated price of proposed development, product, or \nservice \n13-00 Record • Work product stating results achieved or provides evidence of activities \nperformed in a process \n• An item that is part of a set of identifiable and retrievable data \n13-01 Acceptance record • Record of the receipt of the delivery \n• Identification of the date received \n• Identification of the delivered components \n• Records the verification of any customer acceptance criteria defined \n• Signed by receiving customer \n13-04 Communication \nrecord \n• All forms of interpersonal communication including: \n- letters \n- faxes \n- e-mails \n- voice recordings \n- podcast \n- blog \n- videos \n- forum \n- live chat \n- wikis \n- photo protocol \n- meeting support record \n13-05 Contract review \nrecord \n• Scope of contract and requirements \n• Possible contingencies or risks \n• Alignment of the contract with the strategic business plan of the \norganization \n• Protection of proprietary information \n• Requirements which differ from those in the original documentation \n• Capability to meet contractual requirements \n• Responsibility for subcontracted work \n• Terminology \n• Customer ability to meet contractual obligations. \n13-06 Delivery record • Record of items shipped/delivered electronically to customer \n• Identification of: \n- who it was sent to \n- address where delivered \n- the date delivered \n• Record receipt of delivered product \n13-07 Problem record • Identifies the name of submitted and associated contact details \n• Identifies the group/person(s) responsible for providing a fix \n• Includes a description of the problem \n• Identifies classification of the problem (criticality, urgency, relevance \netc.) \n• Identifies the status of the reported problem \n• Identifies the target release(s) in which the problem will be fixed  \n• Identifies the expected closure date \n• Identifies any closure criteria \n• Identifies re-review actions \n13-08 Baseline • Identifies a state of one or a set of work products and artifacts which \nare consistent and complete  \n• Basis for next process steps \n\n  \n \n \n \n© VDA Quality Management Center 108 \n \n \nWP ID WP Name WP Characteristics \n• Is unique and may not be changed \nNOTE: This should be established before a release to identify consistent \nand complete delivery \n13-09 Meeting support \nrecord \n• Agenda and minutes that are records that define: \n- purpose of meeting \n- attendees \n- date, place held \n- reference to previous minutes \n- what was accomplished \n- identifies issues raised \n- any open issues \n- next meeting, if any \n13-10 Configuration \nmanagement record \n• Status of the work products/items and modifications \n• Identifies items under configuration control  \n• Identifies activities performed e.g. backup, storage, archiving, handling \nand delivery of configured items \n• Supports consistency of the product \n13-13 Product release \napproval record \n• Content information of what is to be shipped or delivered \n• Identification of: \n- for whom it is intended \n- the address where to deliver \n- the date released \n• Record of supplier approval \n13-14 Progress status \nrecord \n• Record of the status of a plan(s) (actual against planned) such as: \n- status of actual tasks against planned tasks \n- status of actual results against established objectives/goals \n- status of actual resources allocation against planned resources \n- status of actual cost against budget estimates \n- status of actual time against planned schedule \n- status of actual quality against planned quality \n• Record of any deviations from planned activities and reason why \n13-15 Proposal review \nrecord \n• Scope of proposal and requirements \n• Possible contingencies or risks \n• Alignment of the proposal with the strategic business plan of the \norganization \n• Protection of proprietary information \n• Requirements which differ from those in the original documentation \n• Capability to meet contractual requirements \n• Responsibility for subcontracted work \n• Terminology \n• Supplier ability to meet obligations \n• Approved \n13-16 Change request • Identifies purpose of change \n• Identifies request status (e.g., open, allocated, implemented, closed)  \n• Identifies requester contact information \n• Impacted system(s) \n• Impact to operations of existing system(s) defined \n• Impact to associated documentation defined \n• Criticality of the request, due date \n13-17 Customer request • Identifies request purpose, such as: \n- new development \n\n  \n \n \n \n© VDA Quality Management Center 109 \n \n \nWP ID WP Name WP Characteristics \n- enhancement \n- internal customer \n- operations \n- documentation \n- informational \n• Identifies request status information, such as: \n- date opened \n- current status \n- date assigned and responsible owner \n- date verified \n- date closed \n• Identifies priority/severity of the request \n• Identifies customer information, such as: \n- company/person initiating the request \n- contact information and details \n- system site configuration information \n- impacted system(s) \n- impact to operations of existing systems \n- criticality of the request \n- expected customer response/closure requirements \n• Identifies needed requirements/standards \n• Identifies information sent with request (i.e., RFPs, dumps, etc.) \n13-18 Quality record • Identifies what information to keep \n• Identifies what tasks/activities/process produce the information \n• Identifies when the data was collected \n• Identifies source of any associated data \n• Identifies the associated quality criteria \n• Identifies any associated measurements using the information \n• Identifies any requirements to be adhered to create the record, or \nsatisfied by the record \n13-19 Review record • Provides the context information about the review: \n- what was reviewed \n- lists reviewers who attended \n- status of the review \n• Provides information about the coverage of the review: \n- check-lists \n- review criteria \n- requirements \n- compliance to standards \n• Records information about: \n- the readiness for the review \n- preparation time spent for the review \n- time spent in the review \n- reviewers, roles and expertise \n• Review findings: \n- non-conformances  \n- improvement suggestions \n• Identifies the required corrective actions: \n- risk identification \n- prioritized list of deviations and problems discovered \n- the actions, tasks to be performed to fix the problem \n- ownership for corrective action \n- status and target closure dates for identified problems \n\n  \n \n \n \n© VDA Quality Management Center 110 \n \n \nWP ID WP Name WP Characteristics \n13-20 Risk action request • Date of initiation \n• Scope \n• Subject \n• Request originator \n• Risk management process context:  \n- this section may be provided once, and then referenced in \nsubsequent action requests if no changes have occurred \n- process scope \n- stakeholder perspective \n- risk categories \n- risk thresholds \n- project objectives \n- project assumptions \n- project constraints \n• Risks:  \n- this section may cover one risk or many, as the user chooses \n- where all the information above applies to the whole set of risks, \none action request may suffice \n- where the information varies, each request may cover the risk or \nrisks that share common information \n- risk description(s) \n- risk probability \n- risk consequences \n- expected timing of risk \n• Risk treatment alternatives: \n- alternative descriptions \n- recommended alternative(s) \n- justifications \n• Risk action request disposition: \n- each request should be annotated as to whether it is accepted, \nrejected, or modified, and the rationale provided for whichever \ndecision is taken \n13-21 Change control \nrecord \n• Used as a mechanism to control change to baselined \nproducts/products in official project release libraries \n• Record of the change requested and made to a baselined product \n(work products, software, customer documentation, etc.): \n- identification of system, documents impacted with change \n- identification of change requester \n- identification of party responsible for the change \n- identification of status of the change \n• Linkage to associated customer requests, internal change requests, \netc. \n• Appropriate approvals \n• Duplicate requests are identified and grouped \n13-22 \n \nTraceability record • All requirements (customer and internal) are to be traced \n• Identifies a mapping of requirement to life cycle work products \n• Provides the linkage of requirements to work product decomposition \n(i.e., requirement  design  code  test  deliverables, etc.) \n• Provides forward and backwards mapping of requirements to \nassociated work products throughout all phases of the life cycle \nNOTE: this may be included as a function of another defined work \nproduct (example: A CASE tool for design decomposition may have a \nmapping ability as part of its features) \n\n  \n \n \n \n© VDA Quality Management Center 111 \n \n \nWP ID WP Name WP Characteristics \n13-24 Validation results • Validation check-list \n• Passed items of validation \n• Failed items of validation \n• Pending items of validation  \n• Problems identified during validation \n• Risk analysis \n• Recommendation of actions \n• Conclusions of validation \n• Signature of validation \n13-25 Verification results • Verification check-list \n• Passed items of verification \n• Failed items of verification \n• Pending items of verification  \n• Problems identified during verification \n• Risk analysis \n• Recommendation of actions \n• Conclusions of verification \n• Signature of verification  \n13-50 Test result • Level Test Log  \n• Anomaly Report  \n• Level Test Report (Summary) \n- test cases not passed \n- test cases not executed \n- information about the test execution (date, tester name etc.) \nAdditionally where necessary: \n• Level Interim Test Status Report  \n• Master Test Report (Summary) \n14-00 * Register • A register is a compilation of data or information captured in a defined \nsequence to enable:  \n- an overall view of evidence of activities that have taken place \n- monitoring and analyzes \n- provides evidence of performance of a process over time \n14-01 Change history • Historical records of all changes made to an object (document, file, \nsoftware component, etc.): \n- description of change \n- version information about changed object \n- date of change \n- change requester information \n- change control record information \n14-02 Corrective action \nregister \n• Identifies the initial problem \n• Identifies the ownership for completion of defined action \n• Defines a solution (series of actions to fix problem) \n• Identifies the open date and target closure date \n• Contains a status indicator \n• Indicates follow up audit actions \n14-05 Preferred suppliers \nregister \n• Subcontractor or supplier history \n• List of potential subcontractor/suppliers \n• Qualification information \n• Identification of their qualifications \n• Past history information when it exists \n14-06 Schedule • Identifies the tasks to be performed \n\n  \n \n \n \n© VDA Quality Management Center 112 \n \n \nWP ID WP Name WP Characteristics \n• Identifies the expected and actual start and completion date for \nrequired tasks against progress/completion of tasks \n• Allows for the identification of critical tasks and task dependencies \n• Identifies task completion status, vs. planned date \n• Has a mapping to scheduled resource data \nNOTE: A schedule is consistent with the work breakdown structure, see \n14-09 \n14-08 Tracking system • Ability to record customer and process owner information \n• Ability to record related system configuration information \n• Ability to record information about problem or action needed: \n- date opened and target closure date \n- severity/criticality of item \n- status of any problem or actions needed \n- information about the problem or action owner \n- priority of problem resolution \n• Ability to record proposed resolution or action plan \n• Ability to provide management status information \n• Information is available to all with a need to know \n• Integrated change control system(s)/records \n14-09 Work breakdown \nstructure \n• Defines tasks to be performed, and their amendments \n• Documents ownership for tasks \n• Documents critical dependencies between tasks \n• Documents inputs and output work products \n• Documents the critical dependencies between defined work products \nNOTE: A work breakdown structure may be integrated into/part of the \nschedule, see 14-06 \n14-11 Work product list • Identifies:  \n- name of work product \n- work product reference ID \n- work product revision \n- when updated \n- work product status \n- when approved \n- reference to approval source \n- file reference \n14-50 Stakeholder groups \nlist \n• Identifies:  \n- relevant stakeholder groups \n- weight/importance of each stakeholder group \n- representative(s) for each stakeholder group \n- information needs of each stakeholder group \n15-00 * Report • A work product describing a situation that: \n- includes results and status \n- identifies applicable/associated information \n- identifies considerations/constraints \n- provides evidence/verification \n15-01 Analysis report • What was analyzed? \n• Who did the analysis? \n• The analysis criteria used: \n- selection criteria or prioritization scheme used \n- decision criteria \n- quality criteria \n\n  \n \n \n \n© VDA Quality Management Center 113 \n \n \nWP ID WP Name WP Characteristics \n• Records the results: \n- what was decided/selected \n- reason for the selection \n- assumptions made \n- potential risks \n• Aspects of correctness to analyze include: \n- completeness \n- understandability \n- testability \n- verifiability \n- feasibility \n- validity \n- consistency \n- adequacy of content \n15-03 Configuration status \nreport \n• Identification of the number of items under configuration management \n• Identification of risks associated to configuration management \n• Identification of the number of configuration management items lost \nand reason for their loss \n• Identification of problem and issues related to configuration \nmanagement \n• Identification of receiving parties \n• Identification of baselines made \n15-05 Evaluation report • States the purpose of evaluation \n• Method used for evaluation \n• Requirements used for the evaluation \n• Assumptions and limitations \n• Identifies the context and scope information required: \n- date of evaluation \n- parties involved \n- context details \n- evaluation instrument (check-list, tool) used \n• Records the result: \n- data \n- identifies the required corrective and preventive actions \n- improvement opportunities, as appropriate \n15-06 Project status report • A report of the current status of the project \n• Schedule: \n- planned progress (established objectives/goals) or completion \n(dates/deadlines) of tasks against \n- actual progress of tasks \n- reasons for variance from planned progress  \n- threats to continued progress  \n- contingency plans to maintain progress \n• Resources (human resources, infrastructure, hardware/materials, \nbudget): \n- planned expenditure against actual expenditure  \n- reasons for variance between planned and actual expenditure  \n- expected future expenditure  \n- contingency plans to achieve budget goals \n• Quality goals: \n- actual quality measures  \n- reasons for variance from planned quality measures  \n- contingency plans to achieve quality goals \n\n  \n \n \n \n© VDA Quality Management Center 114 \n \n \nWP ID WP Name WP Characteristics \n• Project issues: \n- issues which may affect the ability of the project to achieve its \ngoals.  \n- contingency plans to overcome threats to project goals \n15-07 Reuse evaluation \nreport \n• Identification of reuse opportunities \n• Identification of investment in Reuse \n• Identification of current skills and experience \n• Identification of reuse infrastructure \n• The evaluation report must represent current status in implementation \nof the reuse program \n15-08 Risk analysis report • Identifies the risks analyzed \n• Records the results of the analysis: \n- potential ways to mitigate the risk \n- assumptions made \n- constraints \n15-09 Risk status report • Identifies the status of an identified risk: \n- related project or activity \n- risk statement \n- condition \n- consequence \n- changes in priority \n- duration of mitigation, when started \n- risk mitigation activities in progress \n- responsibility \n- constraints \n15-12 Problem status \nreport \n• Presents a summary of problem records: \n- by problem categories/classification \n• Status of problem solving: \n- development of solved vs. open problems \n15-13 Assessment/audit \nreport \n• States the purpose of assessment \n• Method used for assessment \n• Requirements used for the assessment \n• Assumptions and limitations \n• Identifies the context and scope information required: \n- date of assessment \n- organizational unit assessed \n- sponsor information \n- assessment team \n- attendees \n- scope/coverage \n- assessees’ information \n- assessment Instrument (check-list, tool) used \n• Records the result: \n- data  \n- identifies the required corrective actions \n- improvement opportunities \n15-16 Improvement \nopportunity \n• Identifies what the problem is \n• Identifies what the cause of a problem is \n• Suggest what could be done to fix the problem \n• Identifies the value (expected benefit) in performing the improvement \n• Identifies the penalty for not making the improvement \n\n  \n \n \n \n© VDA Quality Management Center 115 \n \n \nWP ID WP Name WP Characteristics \n15-18 Process \nperformance report \nNo requirements additional to Evaluation report (Generic) \n15-21 Supplier evaluation \nreport \nNo requirements additional to Evaluation report (Generic) \n16-00 * Repository • Repository for components  \n• Storage and retrieval capabilities \n• Ability to browse content \n• Listing of contents with description of attributes \n• Sharing and transfer of components between affected groups \n• Effective controls over access \n• Maintain component descriptions \n• Recovery of archive versions of components \n• Ability to report component status \n• Changes to components are tracked to change/user requests \n16-03 Configuration \nmanagement system \n• Supports the configuration management strategy \n• Correct configuration of products \n• Can recreate any release or test configuration \n• Ability to report configuration status \n• Has to cover all relevant tools \n16-06 Process repository • Contains process descriptions \n• Supports multiple presentations of process assets \n17-00 Requirement \nspecification \n• Each requirement is identified \n• Each requirement is unique \n• Each requirement is verifiable or can be assessed (see 17-50) \n• Includes statutory and regulatory requirements \n• Includes issues/requirements from (contract) reviews \n17-02 Build list • Identification of aggregates of the software application system \n• Identification of required system elements (application parameter \nsettings, macro libraries, data bases, job control languages, etc.) \n• Necessary sequence ordering identified for compiling the software \nrelease \n• Input and output source libraries identified \n17-03 Stakeholder \nrequirements \n• Purpose/objectives defined \n• Includes issues/requirements from (contract) reviews \n• Identifies any:  \n- time schedule/constraints  \n- required feature and functional characteristics \n- necessary performance considerations/constraints \n- necessary internal/external interface considerations/constraints \n- required system characteristics/constraints \n- human engineering considerations/constraints \n- security considerations/constraints \n- environmental considerations/constraints \n- operational considerations/constraints \n- maintenance considerations/constraints \n- installation considerations/constraints \n- support considerations/constraints \n- design constraints \n- safety/reliability considerations/constraints \n- quality requirements/expectations \n17-05 * Documentation \nrequirements \n• Purpose/objectives defined \n• Proposed contents (coverage) defined \n\n  \n \n \n \n© VDA Quality Management Center 116 \n \n \nWP ID WP Name WP Characteristics \n• Intended audience defined \n• Identification of supported hardware/software/product release, system \ninformation \n• Identification of associated hardware/software/product requirements \nand designs satisfied by document \n• Identification of style, format, media standards expected definition of \nthe intended distribution requirement \n• Includes storage requirements \n17-08 Interface \nrequirements \nspecification \n• Defines relationships between two products, process or process tasks \n• Defines criteria and format for what is common to both \n• Defines critical timing dependencies or sequence ordering \n• Description of the physical interfaces of each system component like: \n- bus interfaces (CAN, MOST, LIN, Flexray etc.) \n- transceiver (type, manufacturer, etc.) \n- analogue interfaces \n- digital interfaces (PWM, I/O) \n- additional interfaces (IEEE, ISO, Bluetooth, USB, etc.) \n• Identification of the software interfaces of software components and \nother software item in terms of: \n- inter-process communication mechanisms \n- bus communication mechanisms \n17-11 Software \nrequirements \nspecification \n• Identifies standards to be used \n• Identifies any software structure considerations/constraints  \n• Identifies the required software elements \n• Identifies the relationship between software elements \n• Consideration is given to: \n- any required software performance characteristics \n- any required software interfaces \n- any required security characteristics required \n- any database design requirements \n- any required error handling and recovery attributes \n- any required resource consumption characteristics \n17-12 System \nrequirements \nspecification \n• System requirements include: functions and capabilities of the system; \nbusiness, organizational and user requirements; safety, security, \nhuman-factors engineering (ergonomics), interface, operations, and \nmaintenance requirements; design constraints and qualification \nrequirements. \n• Identifies the required system overview  \n• Identifies any interrelationship considerations/constraints between \nsystem elements \n• Identifies any relationship considerations/constraints between the \nsystem elements and the software \n• Identifies any design considerations/constraints for each required \nsystem element, including: \n- memory/capacity requirements \n- hardware interface requirements \n- user interface requirements \n- external system interface requirements \n- performance requirements \n- command structures \n- security/data protection characteristics \n- application parameter settings \n- manual operations \n- reusable components \n\n  \n \n \n \n© VDA Quality Management Center 117 \n \n \nWP ID WP Name WP Characteristics \n• Describes the operation capabilities  \n• Describes environmental capabilities \n• Documentation requirements \n• Reliability requirements \n• Logistical Requirements \n• Describes security requirements \n• Diagnosis requirements \n17-50 Verification criteria • Each requirement is verifiable or can be assessed \n• Verification criteria define the qualitative and quantitative criteria for \nverification of a requirement. \n• Verification criteria demonstrate that a requirement can be verified \nwithin agreed constraints. (Additional Requirement to 17-00 \nRequirements specification) \n18-00 * Standard • Identification of to whom/what they apply \n• Expectations for conformance are identified \n• Conformance to requirements can be demonstrated \n• Provisions for tailoring or exception to the requirements are included  \n18-01 Acceptance criteria • Defines expectations for acceptance like e.g.: \n- interfaces \n- schedules \n- messages \n- documents \n- meetings \n- joint reviews \n18-06 Product release \ncriteria \n• Defines expectations for product release: \n- release type and status \n- required elements of the release \n- product completeness including documentation \n- adequacy and coverage of testing \n- limit for open defects \n- change control status \n18-07 Quality criteria • Defines expectations for quality: \n- establishes what is an adequate work product (required elements, \ncompleteness expected, accuracy, etc.) \n- identifies what constitutes the completeness of the defined tasks \n- establishes life cycle transition criteria and the entry and exit \nrequirements for each process and/or activity defined \n- establishes expected performance attributes \n- establishes product reliability attributes \n18-50 Supplier qualification \ncriteria \n• Expectations for conformance, to be fulfilled by competent suppliers, \nare identified \n• Links from the expectations to national/international/domains-specific \nstandards/laws/regulations are described \n• Conformance to requirements can be demonstrated by the potential \nsuppliers or assessed by the acquiring organization \n• Provisions for tailoring or exception to the requirements are included  \n19-00 Strategy • Identifies what needs and objectives or goals there are to be satisfied \n• Establishes the options and approach for satisfying the needs, \nobjectives, or goals \n• Establishes the evaluation criteria against which the strategic options \nare evaluated \n• Identifies any constraints/risks and how these will be addressed \n\n  \n \n \n \n© VDA Quality Management Center 118 \n \n \nWP ID WP Name WP Characteristics \n19-05 Reuse strategy • Identify the goals for reuse \n• Identify the commitment for creating reusable components \n• Determine which product lines and type of artifacts should be \nsupported with reuse \n• Identify system and hardware/software/product elements which can be \nreused within the organization \n• Identify the reuse repository and tools \n19-10 Verification strategy • Verification methods, techniques, and tools \n• Work product or processes under verification \n• Degrees of independence for verification \n• Schedule for performing the above activities \n• Identifies what needs there are to be satisfied \n• Establishes the options and approach for satisfying the need \n• Establishes the evaluation criteria against which the strategic options \nare evaluated \n• Identifies any constraints/risks and how these will be addressed \n• Verification ending criteria \n• Verification start, abort and re-start criteria \n19-11 Validation strategy • Validation methods, techniques, and tools \n• Work products under validation \n• Degrees of independence for validation \n• Schedule for performing the above activities \n• Identifies what needs there are to be satisfied \n• Establishes the options and approach for satisfying the need \n• Establishes the evaluation criteria against which the strategic options \nare evaluated \n• Identifies any constraints/risks and how these will be addressed \n20-00 Template • Defines the attributes associated with a work product to be created as \na consequence of a process execution \n• Identifies technical elements typically associated with this product type \n• Defines expected form and style \n21-00 Work product • Defines the attributes associated with an artifact from a process \nexecution: \n- key elements to be represented in the work product \n\n  \n \n \n \n© VDA Quality Management Center 119 \n \n \nAnnex C Terminology \nAnnex C lists the applicable terminology references from ISO/IEC/IEEE 24765 and ISO/IEC/IEEE\n\n# It also provides terms which are specifically defined within Automotive SPICE. Some of these\n\ndefinitions are based on ISO/IEC/IEEE 24765.  \nTable C.1 — Terminology \nTerm Origin Description \nAcceptance testing ISO/IEC/IEEE\n\n# Formal testing conducted to enable a user, customer, or authorized\n\nentity to determine whether to accept a system or component. \nApplication \nparameter \nAutomotive \nSPICE V3.1 \nAn application parameter is a parameter containing data applied to \nthe system or software functions, behavior or properties. T he notion \nof application parameter is expressed in two ways: firstly, the logical \nspecification (including name, description, unit, value domain or \nthreshold values or characteristic curves, respectively), and, \nsecondly, the actual quantitative data value it receives by means of \ndata application. \nArchitecture \nelement \nAutomotive \nSPICE V3.1 \nResult of the decomposition of the architecture on system and \nsoftware level: \n• The system is decomposed into elements of the system \narchitecture across appropriate hierarchical levels. \n• The software is decomposed into elements of the software \narchitecture across appropriate hierarchical levels down to \nthe software components (the lowest level elements of the \nsoftware architecture). \nBaseline ISO/IEC/IEEE\n\n# A speci fication or product that has been formally reviewed and\n\nagreed upon, that thereafter serves as the basis for further \ndevelopment, and can be changed only through formal change \ncontrol procedures. \nBlack-box testing Automotive \nSPICE V3.1 \nMethod of requireme nt testing where tests are developed without \nknowledge of the internal structure and mechanisms of the tested \nitem. \nCode review Automotive \nSPICE V3.1 \nA check of the code by one or more qualified persons to determine \nits suitability for its intended use an d identify discrepancies from \nspecifications and standards.  \nCoding ISO/IEC/IEEE\n\n# The transforming of logic and data from design specifications (design\n\ndescriptions) into programming language. \nConsistency Automotive \nSPICE V3.1 \nConsistency addresses content and semantics and ensures that work \nproducts are not in contradiction to each other. Consistency is \nsupported by bidirectional traceability. \nDefect  → [FAULT] \nDynamic analysis ISO/IEC/IEEE\n\n# A process of evaluating a system or component based on its behavior\n\nduring execution. \nElement Automotive \nSPICE V3.1 \nElements are all structural objects on architectural and design level \non the left side of the \"V\". Such elements can be further decomposed \ninto more fine -grained sub -elements of the architecture or design \nacross appropriate hierarchical levels. \nError ISO/IEC/IEEE\n\n# The difference between a computed, observed, or measured value or\n\ncondition and the true, specified, or theoretically correct value or \ncondition. \n\n  \n \n \n \n© VDA Quality Management Center 120 \n \n \nFault ISO/IEC/IEEE\n\n# A manifestation of an error in software.\n\nFunctional \nrequirement \nISO/IEC/IEEE\n\n# A statement that identifies what a product or process must\n\naccomplish to produce required behavior and/or results. \nFunctional \nspecification \nISO/IEC/IEEE\n\n# A document that specifies the functions that a system or component\n\nmust perform. Often part of a requirements specification. \nFunctional testing ISO/IEC/IEEE\n\n# Testing conducted to evaluate the compliance of a system or\n\ncomponent with specified functional requirements. \nHardware ISO/IEC/IEEE\n\n# Physical equipment used to process, store, or transmit computer\n\nprograms or data. \nHardware item Automotive \nSPICE V3.1 \nA physical representation of a hardware element. \nIntegration Automotive \nSPICE V3.1 \nA process of combining items to larger items up to an overall system. \nIntegrated software \nitem \nAutomotive \nSPICE V3.1 \nA set of software units or items that are integrated into a larger \nassembly for the purpose of integration testing. \nIntegration testing Automotive \nSPICE V3.1 \nTesting in which items (software items, hardware items, or system \nitems) are combined and tested to evaluate the interaction among \nthem. \nIntegrated system \nitem \nAutomotive \nSPICE V3.1 \nA set of items that are integrated into a larger assembly for the \npurpose of integration testing. \nQuality assurance ISO/IEC/IEEE\n\n# A planned and systematic pattern of all actions necessary to provide\n\nadequate confidence that an item or product conforms to established \ntechnical requirements. \nRegression testing Automotive \nSPICE V3.1 \nSelective retesting of a system or item to verify that modifications \nhave not caused unintended effects and that the system or item still \ncomplies with its specified requirements. \nRequirement Automotive \nSPICE V3.1 \nA prope rty or capability that must be achieved or possessed by a \nsystem, system item, product, or service to satisfy a contract, \nstandard, specification or other formally imposed documents. \nRequirements \nspecification \nAutomotive \nSPICE V3.1 \nA document that specifies the requirements for a system or item. \nTypically included are functional requirements, performance \nrequirements, interface requirements, design requirements, and \ndevelopment standards. \nSoftware ISO/IEC/IEEE\n\n# Computer programs, procedures, and possibly associated\n\ndocumentation and data pertaining to the operation of a computer \nsystem. \nSoftware \ncomponent \nAutomotive \nSPICE V3.1 \nIn Automotive SPICE V3.1 the term \"software component\" is used for \nthe lowest level elements of the software architecture for which finally \nthe detailed design is defined. A software \"component\" consists of \none or more software \"units\". \n→ [ARCHITECTURE ELEMENT], [UNIT] \nSoftware element  → [ARCHITECTURE ELEMENT] \nSoftware item ISO/IEC/IEEE\n\n# Identifiable part of a software product.\n\nSoftware unit  → [UNIT] \n\n  \n \n \n \n© VDA Quality Management Center 121 \n \n \nStatic analysis Automotive \nSPICE V3.1 \nA process of evaluating an item based on its form, structure, content, \nor documentation. \nSystem Automotive \nSPICE V3.1 \nA collection of interacting items organized to accompli sh a specific \nfunction or set of functions within a specific environment. \nSystem item Automotive \nSPICE V3.1 \nIdentifiable part of the system. \nSystem test ISO/IEC/IEEE\n\n# Testing conducted on a complete, integrated system to evaluate the\n\nsystem's compliance with its specified requirements. \nTesting Automotive \nSPICE V3.1 \nActivity in which an item (system, hardware, or software) is executed \nunder specific conditions; and  the results are recorded , summarized \nand communicated. \nTraceability ISO/IEC/IEEE\n\n# The degree to which a relationship can be established between two\n\nor more products of the development process, especially products \nhaving a predecessor -successor or master -subordinate relationship \nto one another. \nUnit Automotive \nSPICE V3.1 \nPart of a software component which is not further subdivided.  \n→ [SOFTWARE COMPONENT] \nUnit test Automotive \nSPICE V3.1 \nThe testing of individual software units or a set of combined software \nunits. \nValidation ISO/IEC/IEEE\n\n# Validation demonstrates that the work item can be used by the users\n\nfor their specific tasks. \nVerification ISO/IEC/IEEE\n\n# Verification is confirmation, through the provision of objective\n\nevidence, that specified requirements have been fulfilled in a given \nwork item. \nWhite-box testing Automotive \nSPICE V3.1 \nMethod of testing where tests are developed based on the knowledge \nof the internal structure and mechanisms of the tested item. \n\n  \n \n \n \n© VDA Quality Management Center 122 \n \n \nAnnex D Key concepts \nThe following sections describe the key concepts that have been introduced in the Au tomotive \nSPICE PRM resp. PAM 3.1. They relate to the terminology described in Annex C Terminology. \nD.1 The \"Plug-in\" concept \nThe following figure shows the basic principle of the \"plug-in\" concept. The top-level comprises all \nsystem engineering processes organized in a system \"V\". Depending on the product to be developed \nthe corresponding engineering disciplines with their domain -specific processes (e.g. hardware \nengineering HWE, mechanical engineeri ng MEE, or software engineering SWE) can be added to \nthe assessment scope. All other processes such as management processes and supporting \nprocesses are domain-independent and are therefore designed in a way that they can be applied to \nboth the system level and the domain levels."
  },
  "64": {
    "id": "6ab20e4f-9e62-4ebb-b187-928e08ea9ab9",
    "title": "SYS.2",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "65": {
    "id": "14a272fa-5c6f-4c0e-8736-e7ef1f4f0810",
    "title": "SYS.5",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "66": {
    "id": "c2bdeab4-19d7-483e-b122-1684dcfff231",
    "title": "System level",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "67": {
    "id": "0bebeea3-911b-495c-b8f7-af6ae8fcc16b",
    "title": "HWE.2",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "68": {
    "id": "63890606-2482-4673-b6f8-684cd73964f0",
    "title": "HWE.5",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "69": {
    "id": "1fe9dc55-d7db-4565-b5e4-7be134ff5d5e",
    "title": "HWE.6 MEE.1",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "70": {
    "id": "df2cd1d3-f450-4a2f-b609-7ef411c7f32b",
    "title": "MEE.3",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "71": {
    "id": "79d08a64-95c5-429b-ae3c-9c81f13f5c8b",
    "title": "MEE.4",
    "level": 2,
    "parent_id": null,
    "content": "MEE.6SWE.1"
  },
  "72": {
    "id": "b4c5f8de-5af0-4968-98f5-c063cef2c515",
    "title": "SWE.3",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "73": {
    "id": "e94a829b-a51b-4506-bb07-2e4470c9137a",
    "title": "SWE.4",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "74": {
    "id": "ccc8c35e-fa4f-47bb-8959-b74e39d060e9",
    "title": "Domain level",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "75": {
    "id": "d732c6b8-d4e9-4322-b65f-7cb1212668b9",
    "title": "MAN.3",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "76": {
    "id": "f3ec7d23-f33d-475d-8493-c41d285d186c",
    "title": "SUP.1",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "77": {
    "id": "95962856-ff69-4722-ac05-ce3159786175",
    "title": "SUP.10",
    "level": 2,
    "parent_id": null,
    "content": "SYS : System engineering\n SWE : Software engineering \n HWE : Hardware engineering\n MEE : Mechanical engineering\nVDA scope as part of Automotive SPICE 3.1 PRM/PAM\nExamplary processes for other domains, neither developed \nby VDA nor part of Automotive SPICE PRM/PAM\n \nFigure D.1 — The \"Plug-in\" concept \nAll processes printed in bold are part of the Automotive SPICE 3. 1 PRM/PAM whereas the other \nprocesses (mechanical engineering and hardware engineering) are not developed under VDA QMC \nmandate. \n  \n\n  \n \n \n \n© VDA Quality Management Center 123 \n \n \nD.2 The Tip of the \"V\" \nAll engineering processes (i.e. system engineering and software engineering) have been organized \naccording to the \"V model\" principle in such a way that each process on the left side is corresponding \nto exactly one process on the right side. Therefor e, the process SWE.3 \"Software Detailed Design \nand Unit Construction\" is separated from the process SWE.4 \"Software Unit Verification\"."
  },
  "78": {
    "id": "44289413-cfab-4201-b44b-7cae79fa77e5",
    "title": "System Requirements",
    "level": 2,
    "parent_id": null,
    "content": "Analysis"
  },
  "79": {
    "id": "d0248e1c-2649-4502-bbe1-05fe04faeb0e",
    "title": "System Architectural",
    "level": 2,
    "parent_id": null,
    "content": "Design"
  },
  "80": {
    "id": "f2056591-9160-4cc8-a5bb-c7056c5b5236",
    "title": "System Integration and",
    "level": 2,
    "parent_id": null,
    "content": "Integration Test"
  },
  "81": {
    "id": "898ccc40-6929-421e-bdb0-8bf6c145e932",
    "title": "System Qualification Test",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "82": {
    "id": "a3846210-22dc-4b8f-bdbe-2cd1b462cf59",
    "title": "Software Requirements",
    "level": 2,
    "parent_id": null,
    "content": "Analysis"
  },
  "83": {
    "id": "af3ea7e2-5ed0-4049-a877-07cb8a29f2b0",
    "title": "Software Architectural",
    "level": 2,
    "parent_id": null,
    "content": "Design"
  },
  "84": {
    "id": "db2d5e70-1121-4c3a-9578-0799904c7aa6",
    "title": "Software Detailed Design",
    "level": 2,
    "parent_id": null,
    "content": "and Unit Construction"
  },
  "85": {
    "id": "983bf435-2a3a-4272-9237-e31b81903ddb",
    "title": "Software Unit Verification",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "86": {
    "id": "8eefba96-2973-498a-a9df-877bd3558732",
    "title": "Software Integration and",
    "level": 2,
    "parent_id": null,
    "content": "Integration Test"
  },
  "87": {
    "id": "3f9a713e-1fdb-4556-a3f6-11d0b1386216",
    "title": "Software Qualification Test",
    "level": 2,
    "parent_id": null,
    "content": "Figure D.2 — The tip of the \"V\" \n \nD.3 Terms \"Element\", \"Component\", \"Unit\", and \"Item\" \nThe following figure depicts the relationships between element, component, software unit, and item, \nwhich are used consistently in the engineering processes. \nUnit\nComponent"
  },
  "88": {
    "id": "49582863-f4ba-48f3-82da-0c3fe6ceb485",
    "title": "System Requirements",
    "level": 2,
    "parent_id": null,
    "content": "Analysis"
  },
  "89": {
    "id": "ee8f2ece-35e5-4921-87b7-a3a9c97b7956",
    "title": "System Architectural",
    "level": 2,
    "parent_id": null,
    "content": "Design"
  },
  "90": {
    "id": "af670d03-d73f-497a-8190-28803990be0a",
    "title": "System Integration and",
    "level": 2,
    "parent_id": null,
    "content": "Integration Test"
  },
  "91": {
    "id": "ab605b25-7d3d-49d7-a8b3-0688a01477fd",
    "title": "System Qualification Test",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "92": {
    "id": "5dcf1081-44c1-4ae7-8ca9-1762fa90fe2b",
    "title": "Software Requirements",
    "level": 2,
    "parent_id": null,
    "content": "Analysis"
  },
  "93": {
    "id": "4fe89dc9-9dcf-444b-8de5-5aaf0b447b32",
    "title": "Software Architectural",
    "level": 2,
    "parent_id": null,
    "content": "Design"
  },
  "94": {
    "id": "55fba59d-fb0e-4c36-8435-f7744bad8d4e",
    "title": "Software Detailed Design",
    "level": 2,
    "parent_id": null,
    "content": "and Unit Construction"
  },
  "95": {
    "id": "42d1a408-85b4-4e8c-aa3e-e61f7b229cc5",
    "title": "Software Unit Verification",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "96": {
    "id": "28a41f48-a00a-412b-95ca-c437fad0e82a",
    "title": "Software Integration and",
    "level": 2,
    "parent_id": null,
    "content": "Integration Test"
  },
  "97": {
    "id": "34dbacbf-a8c9-4eaa-bf59-8c3888b2f0e4",
    "title": "Software Qualification Test",
    "level": 2,
    "parent_id": null,
    "content": "Element Item\n \nFigure D.3 — Element, component, unit, and item \n\n  \n \n \n \n© VDA Quality Management Center 124 \n \n \nAn architecture consists of architectural \"elements\" that can be further decomposed into more fine- \ngrained architectural sub -\"elements\" across appropriate hierarchical levels. The software \n\"components\" are the lowest -level \"elements\" of the software architecture for which finally  the \ndetailed design is defined. A software \"component\" consists of one or more software \"units\". \n\"Items\" on the right side of the V -model correspond to \"elements\" on the left side (e.g. a software \n\"item\" can be an object file, a library or an executable). This can be a 1:1 or m:n relationship, e.g. an \n\"item\" may represent more than one architectural \"element\".  \n \nD.4 Traceability and consistency \nTraceability and consistency are addressed by two separate base practices in the Automotive SPICE\n\n## PAM. Traceability refers to the existence of references or links between work products thereby\n\nfurther supporting coverage, impact analysis, requirements implementation status tracking etc. In \ncontrast, consistency addresses content and semantics. \nFurthermore, bidirectional traceability has been explicitly defined between \n• test cases and test results, and \n• change requests and work products affected by these change requests. \nAn overview of bidirectional traceability and consistency is depicted in the following figure."
  },
  "98": {
    "id": "c2aedeeb-510e-4575-bda6-0b60e338ca6b",
    "title": "BP6",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "99": {
    "id": "6be0473d-4159-48ac-a6b1-5de8403d3d0d",
    "title": "BP7",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "100": {
    "id": "daebf8dd-6782-4e11-9122-62c89fd149d3",
    "title": "BP6",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "101": {
    "id": "2fed140a-30d9-4e24-80ea-5354944722ab",
    "title": "BP7",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "102": {
    "id": "edc5e36e-d2d1-4ce4-be07-65913ca09522",
    "title": "BP5",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "103": {
    "id": "279e593b-0ee8-4dc9-90bd-910bde95b923",
    "title": "BP6",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "104": {
    "id": "d8e4b2cd-6327-4ed3-b7c7-76b13260b0d5",
    "title": "BP6",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "105": {
    "id": "c264263e-bee0-4699-8ca9-bc3b55423923",
    "title": "BP7",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "106": {
    "id": "7c08e343-2de8-43a5-b1ec-a6de9c043234",
    "title": "BP6",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "107": {
    "id": "908e3102-5669-4694-aaa3-a1322ace9dcf",
    "title": "BP7",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "108": {
    "id": "1ae069f8-d3e9-4c78-a718-73686ae7a37c",
    "title": "BP7",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "109": {
    "id": "ca9532b8-1b7f-44b9-a016-91eb03441a4d",
    "title": "BP8",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "110": {
    "id": "d3828447-3289-410c-be90-1c27774f01ed",
    "title": "BP5",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "111": {
    "id": "f5a5ad85-857a-483a-896c-defafa647424",
    "title": "BP6",
    "level": 2,
    "parent_id": null,
    "content": "SWE.5.BP7"
  },
  "112": {
    "id": "15ef46ef-8ecf-4ecf-afc4-a0158bd043fe",
    "title": "BP8",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "113": {
    "id": "c3f8d3a8-7191-4c89-88bb-86cd99db40a5",
    "title": "BP5",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "114": {
    "id": "006c372d-5f41-480a-9da1-68e75a9ba68a",
    "title": "BP6",
    "level": 2,
    "parent_id": null,
    "content": "SWE.3.BP5"
  },
  "115": {
    "id": "b4d3eb84-178e-48db-8288-5072cd3dc163",
    "title": "BP6",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "116": {
    "id": "17e97f01-7b50-4566-8ce1-a6926deccf4d",
    "title": "BP5",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "117": {
    "id": "28b58d3d-d969-4fdd-8461-3c2e841de7ea",
    "title": "BP6",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "118": {
    "id": "edb8abc8-6ce6-4b1d-99ad-5c6931560a60",
    "title": "BP5",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "119": {
    "id": "4253148b-59f8-4e13-84ae-fa3580d1649f",
    "title": "BP6",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "120": {
    "id": "6c065021-d2ef-4d22-8a8c-ed4d873c32ab",
    "title": "BP5",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "121": {
    "id": "b9858207-b69b-497d-b535-cb35f137d87c",
    "title": "BP7",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "122": {
    "id": "3d5da8b7-2d44-4c66-a723-f6c89a9a59a0",
    "title": "BP7",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "123": {
    "id": "51e240f8-2346-42d6-8db0-d63a102d0ac2",
    "title": "BP5",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "124": {
    "id": "80b8b4a4-d1a6-4cae-91e5-2cfa9b392b20",
    "title": "BP8",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "125": {
    "id": "7944a1fb-e6d5-46b4-93d8-c8ec222f21ae",
    "title": "BP7",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "126": {
    "id": "8821472e-baa1-4bd7-9322-5934aa7b92c5",
    "title": "BP8",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "127": {
    "id": "ef21c046-b018-4ac0-a681-a93ee690e8f4",
    "title": "BP5",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "128": {
    "id": "1c397ab0-c676-40e4-b611-5789cb7d959b",
    "title": "BP5",
    "level": 2,
    "parent_id": null,
    "content": "consistency\nbidirectional traceabilityStakeholder\nrequirements\nSystem requirements\nSystem architecture\nSoftware requirements\nSoftware architecture\nSoftware detailed\ndesign\nSoftware units\nUnit test specification Unit test results\nStatic verification\nresults\nChange requests\nTo affected work products\nSoftware integration\ntest results\nSoftware qualification\ntest results\nSoftware qualification\ntest specification\nTest cases\nSoftware integration\ntest specification\nTest cases\nSystem integration\ntest results\nSystem qualification\ntest results\nSystem qualification\ntest specification\nTest cases\nSystem integration\ntest specification\nTest cases\n \nFigure D.4 — Bidirectional traceability and consistency \n  \n\n  \n \n \n \n© VDA Quality Management Center 125 \n \n \nD.5 \"Agree\" and \"Summarize and Communicate\" \nThe information flow on the left side o f the \"V\" is ensured through a b ase practice \"Communicate \nagreed ‘work product x’\". The term \"agreed\" here means that there is a joint understanding between \naffected parties of what is meant by the content of the work product. \nThe information flow on the right side of the \"V\" is ensured through a base practice \"Summarize and \ncommunicate results\". The term \"Summarize\" refers to abstracted information resulting from test \nexecutions made available to all relevant parties. \nNote that these communication-oriented base practices do not necessarily require a formal approval, \nconfirmation, or release as rather targeted at by GP 2.1.7 on capability level 2. At capability level 1 \nthe communication-oriented base practices mean that the work products (or their content) are to be \ndisseminated to relevant parties.  \nAn overview of these aspects is shown in the following figure:"
  },
  "129": {
    "id": "48669bf9-911f-4ad8-b5ab-fadd5d461c4a",
    "title": "System Requirements",
    "level": 2,
    "parent_id": null,
    "content": "Analysis"
  },
  "130": {
    "id": "514edd6b-a141-474c-8bde-7172c85d10cc",
    "title": "System Architectural",
    "level": 2,
    "parent_id": null,
    "content": "Design"
  },
  "131": {
    "id": "617fc370-0f66-4a98-8b18-b3132ab3f559",
    "title": "System Integration and",
    "level": 2,
    "parent_id": null,
    "content": "Integration Test"
  },
  "132": {
    "id": "9a6100a8-5745-4115-ae67-979311089c98",
    "title": "System Qualification Test",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "133": {
    "id": "941edc0a-fd77-45b6-921b-8e2767f40b02",
    "title": "Software Requirements",
    "level": 2,
    "parent_id": null,
    "content": "Analysis"
  },
  "134": {
    "id": "aee624af-b31c-42e6-afd1-bc059f306d5b",
    "title": "Software Architectural",
    "level": 2,
    "parent_id": null,
    "content": "Design"
  },
  "135": {
    "id": "c760d4c8-43c6-499d-a8bb-04a475eb62b4",
    "title": "Software Detailed Design",
    "level": 2,
    "parent_id": null,
    "content": "and Unit Construction"
  },
  "136": {
    "id": "adc0bd7a-46be-4843-a87f-e33922a5cb91",
    "title": "Software Unit Verification",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "137": {
    "id": "28b5dd67-46af-4d05-883c-27e01032005c",
    "title": "Software Integration and",
    "level": 2,
    "parent_id": null,
    "content": "Integration Test"
  },
  "138": {
    "id": "987d33f9-5db6-4208-9149-1611b8be4ef8",
    "title": "Software Qualification Test",
    "level": 2,
    "parent_id": null,
    "content": "BP: „communicate agreed…“ BP: „summarize and communicate…“\n \nFigure D.5 — Agree, summarize and communicate \nD.6 \"Evaluate\", \"Verification Criteria\" and \"Ensuring compliance\" \nThis section describes relations, differences, and commonalities between verification, testing, \nevaluation, and compliance. The following Figure D.6 provides an overview. \nVerification criteria are used as input for the development of the test cases or other verification \nmeasures that ensures compliance with the requirements. Verification criteria are only used in the \ncontext of System Requirements Analysis (SYS.2) and Software Requirements Analysis (SWE.1) \nprocesses. Verification aspects which cannot be covered by testing are cov ered by the verification \nprocess (SUP.2). \nCriteria for unit verification ensure compliance of the source code with the software detailed design \nand the non-functional requirements. Possible criteria for unit verification include unit test cases, unit \ntest data, coverage goals and coding standards and coding guidelines, e.g. MISRA. For unit testing, \nsuch criteria shall be defined in a unit test specification. This unit test specific ation may be \nimplemented e.g. as a script in an automated test bench.  \n\n  \n \n \n \n© VDA Quality Management Center 126 \n \n \nSWE.3.BP4: Evaluate\nSYS.3.BP5: Evaluate\nSWE.2.BP6: Evaluate"
  },
  "139": {
    "id": "1371b960-8d1a-4767-830f-cd0ead17388e",
    "title": "System Requirements",
    "level": 2,
    "parent_id": null,
    "content": "Analysis"
  },
  "140": {
    "id": "7a427341-5be8-4362-88dc-3730021cb074",
    "title": "System Architectural",
    "level": 2,
    "parent_id": null,
    "content": "Design"
  },
  "141": {
    "id": "3a46b5d0-a9c5-47aa-b5dc-b1676bb6112d",
    "title": "System Integration and",
    "level": 2,
    "parent_id": null,
    "content": "Integration Test"
  },
  "142": {
    "id": "762200a3-6751-41f9-83b9-f7b6b3ff0b97",
    "title": "System Qualification Test",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "143": {
    "id": "b944a7c0-07a3-496d-ac48-a7235529866d",
    "title": "Software Requirements",
    "level": 2,
    "parent_id": null,
    "content": "Analysis"
  },
  "144": {
    "id": "6d3f54ed-79d7-4365-8b09-5b13e96a2303",
    "title": "Software Architectural",
    "level": 2,
    "parent_id": null,
    "content": "Design"
  },
  "145": {
    "id": "38d49a1a-e26d-46be-8333-f31a6238aebe",
    "title": "Software Detailed Design",
    "level": 2,
    "parent_id": null,
    "content": "and Unit Construction"
  },
  "146": {
    "id": "b54d48b2-764c-4439-ab65-4d74fc09fa1e",
    "title": "Software Unit Verification",
    "level": 2,
    "parent_id": null,
    "content": ""
  },
  "147": {
    "id": "6df70c36-81ac-4db1-b604-7acba1e2ce13",
    "title": "Software Integration and",
    "level": 2,
    "parent_id": null,
    "content": "Integration Test"
  },
  "148": {
    "id": "2cb1fae8-fa05-4a24-ae43-3feeec2c41f9",
    "title": "Software Qualification Test",
    "level": 2,
    "parent_id": null,
    "content": "SYS.2.BP5: Verification criteria\nSWE.1.BP5: Verification criteria\nSWE.5.BP3: Compliance\nSYS.4.BP3: Compliance\nSWE.4.BP2: Criteria for unit verification"
  },
  "149": {
    "id": "27e242bf-467b-4b54-8047-1ac6ec62a906",
    "title": "Verification",
    "level": 2,
    "parent_id": null,
    "content": "SYS.5.BP2: Compliance\nSWE.6.BP2: Compliance\nSWE.4.BP2: Compliance\n \nFigure D.6 — Evaluation, verification criteria and compliance \nEvaluation of alternative solutions is required for system and software architectures as well as for \nsoftware detailed designs. The evaluation has to be done according to defined criteria. Such \nevaluation criteria may include quality characteristics like modularity, reliability, security, and \nusability, or results of make-or-buy or reuse analysis. The evaluation result including a rationale for \nthe architecture/design selection has to be recorded. \nCompliance with an architectural design means that the specified integration tests are capable of \nproving that interfaces and relevant interactions between \n• the software units, \n• the software items and \n• the system items \nfulfill the specification given by the architectural design. \nD.7 The relation between \"Strategy\" and \"Plan\" \nBoth terms \"Strategy\" and \"Plan\" are commonly used across following processes of the Automotive \nSPICE 3.1 PAM: \n• SYS.4   System Integration and Integration Test \n• SYS.5   System Qualification Test \n• SWE.4  Software Unit Verification \n• SWE.5  Software Integration and Integration Test \n• SWE.6  Software Qualification Test \n• SUP.1  Quality Assurance \n• SUP.8  Configuration Management \n• SUP.9  Problem Resolution Management \n• SUP.10  Change Request Management \nThe following figure shows the general relationship between strategy and plan in any of these \nprocesses. \n\n  \n \n \n \n© VDA Quality Management Center 127 \n \n \nPlan\nCL >= 2\nCL = 1\nProcess specific plan\nWP 08-nn\nGeneric plan\nWP 08-00\nBP1:\nDevelop Strategy\n \nFigure D.7— Strategy and plan \nCapability Level 1: \nEach of these processes requires the development of a process -specific strategy. The strategy \nalways corresponds to a process-specific \"Plan\". For each process-specific \"Plan\" there are process-\nspecific work product characteristics defined (e.g. \"08-52 Tes t Plan \", \"08-04 Configuration \nManagement Plan\").  \nCapability Level 2 or higher: \nEach process-specific \"Plan\" (WP 08-nn) inherits the work product characteristics represented by \nthe Generic Plan (WP 08-00). This means that for a process-specific \"Plan\" both the process-specific \ncharacteristics (WP 08-nn) and the generic characteristics (WP 08-00) apply. \n\n  \n \n \n \n© VDA Quality Management Center 128 \n \n \nAnnex E Reference standards \nAnnex E provides a list of reference standards and guidelines that support implementation of the \nAutomotive SPICE PAM / PRM. \nTable E.1 — Reference standards \nISO/IEC 33001:2015 Information technology -- Process assessment – \nConcepts and terminology \nISO/IEC 33002:2015 Information technology -- Process assessment – \nRequirements for performing process assessment \nISO/IEC 33003:2015 Information technology -- Process assessment – \nRequirements for process measurement frameworks  \nISO/IEC 33004:2015 Information technology -- Process assessment – \nRequirements for process reference, process assessment and \nmaturity models  \nISO/IEC 33020:2015 Information technology -- Process assessment – \nProcess measurement framework for assessment of process \ncapability \nISO/IEC 15504-5:2006 Information Technology – Process assessment – Part 5: An \nexemplar Process Assessment Model \nISO/IEC 12207:2008 Systems and software engineering -- Software life cycle processes \nISO/IEC/IEEE 29119-1:2013 Software and systems engineering -- Software testing -- Part 1: \nConcepts and definitions \nISO/IEC/IEEE 29119-3:2013 Software and systems engineering -- Software testing -- Part 3: \nTest documentation \nISO/IEC/IEEE 24765:2010 Systems and software engineering -- Vocabulary \nISO/IEC 25010:2011 Systems and software engineering -- Systems and software \nQuality Requirements and Evaluation (SQuaRE) -- System and \nsoftware quality models"
  }
}