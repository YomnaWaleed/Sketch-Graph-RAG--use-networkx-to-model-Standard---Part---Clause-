{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3679c7f3",
   "metadata": {},
   "source": [
    "### 1) extract text from PDFs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e58a189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    pages = []\n",
    "\n",
    "    for p in reader.pages:\n",
    "        text = p.extract_text() or \"\"\n",
    "        pages.append(text)\n",
    "    return \"\\n\\n\".join(pages)\n",
    "\n",
    "\n",
    "def save_text(pdf_path: str, out_txt_path: str):\n",
    "    text = extract_text_from_pdf(pdf_path=pdf_path)\n",
    "    Path(out_txt_path).write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# PDF paths\n",
    "pdf1 = \"../data/raw/Automotive_SPICE_PAM_31_EN.pdf\"\n",
    "out1 = \"../data/txt/automotivespice.txt\"\n",
    "\n",
    "pdf2 = \"../data/raw/AUTOSAR_SWS_ECUStateManager.pdf\"\n",
    "out2 = \"../data/txt/autosar_ecum.txt\"\n",
    "\n",
    "# Run extraction\n",
    "save_text(pdf1, out1)\n",
    "save_text(pdf2, out2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940fc9b7",
   "metadata": {},
   "source": [
    "### 2) Parse handling convert text to Markdown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c6f570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Markdown: ..\\data\\md\\automotivespice.md\n",
      "Saved Markdown: ..\\data\\md\\autosar_ecum.md\n"
     ]
    }
   ],
   "source": [
    "import re, uuid\n",
    "from typing import List, Dict\n",
    "\n",
    "numbered_header_re = re.compile(\n",
    "    r\"^\\s*(\\d+(?:\\.\\d+){0,3})\\s*\\.?\\s+(?P<title>[A-Z][^\\n]{3,200})$\", re.M\n",
    ")\n",
    "\n",
    "code_header_re = re.compile(r\"^(?P<code>[A-Z]{2,5}\\.\\d+)\\s+(?P<title>.+)$\", re.M)\n",
    "\n",
    "def parse_numbered_headings(text: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse text into hierarchical chunks with full content under each header.\n",
    "    \"\"\"\n",
    "    matches = list(numbered_header_re.finditer(text))\n",
    "    if not matches:\n",
    "        matches = list(code_header_re.finditer(text))\n",
    "    \n",
    "    if not matches:\n",
    "        return [{\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"level\": 1,\n",
    "            \"title\": \"Document\",\n",
    "            \"parent_id\": None,\n",
    "            \"content\": text.strip(),\n",
    "        }]\n",
    "\n",
    "    chunks = []\n",
    "    boundaries = [(m.start(), m.end(), m.group(0), m.groupdict().get(\"title\") or m.group(0)) for m in matches]\n",
    "    boundaries.append((len(text), len(text), \"\", \"\"))  \n",
    "\n",
    "    for i in range(len(boundaries)-1):\n",
    "        header_line = boundaries[i][2]\n",
    "        title = boundaries[i][3].strip()\n",
    "        content = text[boundaries[i][1]:boundaries[i+1][0]].strip()\n",
    "        num_match = re.match(r\"^\\s*(\\d+(?:\\.\\d+)*)\", header_line)\n",
    "        level = num_match.group(1).count(\".\")+1 if num_match else 2\n",
    "        chunks.append({\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"level\": level,\n",
    "            \"title\": title,\n",
    "            \"parent_num\": num_match.group(1) if num_match else None,\n",
    "            \"content\": content,\n",
    "        })\n",
    "\n",
    "    for c in chunks:\n",
    "        if c.get(\"parent_num\"):\n",
    "            parts = c[\"parent_num\"].split(\".\")\n",
    "            found_parent = None\n",
    "            for candidate in reversed(chunks):\n",
    "                pnum = candidate.get(\"parent_num\")\n",
    "                if pnum and pnum == \".\".join(parts[:-1]):\n",
    "                    found_parent = candidate[\"id\"]\n",
    "                    break\n",
    "            c[\"parent_id\"] = found_parent\n",
    "        else:\n",
    "            c[\"parent_id\"] = None\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_markdown(chunks: List[Dict], out_md_path: Path):\n",
    "    out_md_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    lines = []\n",
    "    for c in chunks:\n",
    "        header_prefix = \"#\" * c[\"level\"]\n",
    "        lines.append(f\"{header_prefix} {c['title']}\\n\")\n",
    "        lines.append(c[\"content\"] + \"\\n\")\n",
    "    out_md_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\"Saved Markdown: {out_md_path}\")\n",
    "\n",
    "txt_files = Path(\"../data/txt\").glob(\"*.txt\")\n",
    "for txt_file in txt_files:\n",
    "    text = txt_file.read_text(encoding=\"utf-8\")\n",
    "    chunks = parse_numbered_headings(text)\n",
    "    md_path = Path(\"../data/md\") / txt_file.with_suffix(\".md\").name\n",
    "    save_markdown(chunks, md_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb8fac5",
   "metadata": {},
   "source": [
    "### 3) build index \n",
    "####  convert each chunk to embedding and build FAISS index one for each file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b905e64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAPSHOP\\anaconda3\\envs\\slrag\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LAPSHOP\\anaconda3\\envs\\slrag\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879bf04e99d54679816042446f63954a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved index + meta for automotivespice_clauses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAPSHOP\\anaconda3\\envs\\slrag\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc37126596547b4be06b331342d5b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved index + meta for autosar_ecum_clauses\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss, json\n",
    "from pathlib import Path\n",
    "\n",
    "EMB_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "def build_embeddings(chunks, model_name=EMB_MODEL):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    texts = [c[\"content\"] or c[\"title\"] for c in chunks]\n",
    "    embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "    return embeddings\n",
    "\n",
    "def build_faiss_index(chunks, embeddings, out_dir: str, index_name: str):\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    d = embeddings.shape[1]\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, f\"{out_dir}/{index_name}.index\")\n",
    "\n",
    "    # save metadata\n",
    "    meta = {\n",
    "        i: {\n",
    "            \"id\": chunks[i][\"id\"],\n",
    "            \"title\": chunks[i].get(\"title\"),\n",
    "            \"level\": chunks[i][\"level\"],\n",
    "            \"parent_id\": chunks[i].get(\"parent_id\"),\n",
    "            \"content\": chunks[i].get(\"content\")  # مهم جدًا\n",
    "        }\n",
    "        for i in range(len(chunks))\n",
    "    }\n",
    "    Path(f\"{out_dir}/{index_name}_meta.json\").write_text(\n",
    "        json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "    )\n",
    "    print(f\"Saved index + meta for {index_name}\")\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    md_dir = Path(\"../data/md\")\n",
    "    out_dir = Path(\"../outputs/indices\")\n",
    "\n",
    "    # process all md files\n",
    "    for md_file in md_dir.glob(\"*.md\"):\n",
    "        text = md_file.read_text(encoding=\"utf-8\")\n",
    "        chunks = parse_numbered_headings(text)\n",
    "        embeddings = build_embeddings(chunks)\n",
    "        index_name = md_file.stem + \"_clauses\"\n",
    "        build_faiss_index(chunks, embeddings, out_dir=out_dir, index_name=index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe76f2",
   "metadata": {},
   "source": [
    "### 4) hierarchical Retrieval\n",
    "#### headers -> sections -> clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6774aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYS.2\n",
      "\n",
      "\n",
      "SYS.5\n",
      "\n",
      "\n",
      "Reuse Program\n",
      "Management\n",
      "System Engineering Process Group (SYS)\n"
     ]
    }
   ],
   "source": [
    "# hierarchical Retrieval\n",
    "## headers -> sections -> clauses\n",
    "import faiss, json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "\n",
    "EMB_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "class HierarchicalRetriever:\n",
    "    def __init__(self, indices_meta, emb_model=EMB_MODEL):\n",
    "        self.indices = []\n",
    "        self.metas = []\n",
    "        for idx_path, meta_path in indices_meta:\n",
    "            self.indices.append(faiss.read_index(idx_path))\n",
    "            self.metas.append(json.loads(Path(meta_path).read_text(encoding=\"utf-8\")))\n",
    "        self.model = SentenceTransformer(emb_model)\n",
    "\n",
    "    def _embed(self, query):\n",
    "        v = self.model.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(v)\n",
    "        return v\n",
    "\n",
    "    def search_topk(self, query, k=5):\n",
    "        v = self._embed(query)\n",
    "        results = []\n",
    "        for index, meta in zip(self.indices, self.metas):\n",
    "            D, I = index.search(v, k)\n",
    "            for idx, score in zip(I[0], D[0]):\n",
    "                if idx == -1:\n",
    "                    continue\n",
    "                info = meta[str(idx)] if str(idx) in meta else meta[idx]\n",
    "                results.append({\"idx\": idx, \"score\": float(score), **info})\n",
    "        results = sorted(results, key=lambda x: -x[\"score\"])\n",
    "        return results[:k]\n",
    "\n",
    "    def hierarchical_retrieve(self, query, k_header=3, k_section=3, k_clause=3):\n",
    "        top = self.search_topk(query, k=max(k_header,k_section,k_clause))\n",
    "        headers = [r for r in top if r[\"level\"]==1][:k_header]\n",
    "        if not headers:\n",
    "            headers = top[:k_header]\n",
    "\n",
    "        final_clauses = []\n",
    "        for h in headers:\n",
    "            q_section = f\"{query} context: {h.get('title')}\"\n",
    "            sections = self.search_topk(q_section, k=k_section)\n",
    "            if sections:\n",
    "                top_section = sections[0]\n",
    "                q_clause = f\"{query} context: {h.get('title')} > {top_section.get('title')}\"\n",
    "                clauses = self.search_topk(q_clause, k=k_clause)\n",
    "                final_clauses.extend(clauses)\n",
    "\n",
    "        # deduplicate & sort\n",
    "        seen = set()\n",
    "        deduped = []\n",
    "        for c in sorted(final_clauses, key=lambda x: -x[\"score\"]):\n",
    "            if c['idx'] in seen:\n",
    "                continue\n",
    "            seen.add(c['idx'])\n",
    "            deduped.append(c)\n",
    "        return {\"headers\": headers, \"clauses\": deduped[:k_clause]}\n",
    "\n",
    "# usage\n",
    "if __name__ == \"__main__\":\n",
    "    indices_meta = [\n",
    "        (\n",
    "            \"../outputs/indices/automotivespice_clauses.index\",\n",
    "            \"../outputs/indices/automotivespice_clauses_meta.json\",\n",
    "        ),\n",
    "        (\n",
    "            \"../outputs/indices/autosar_ecum_clauses.index\",\n",
    "            \"../outputs/indices/autosar_ecum_clauses_meta.json\",\n",
    "        ),\n",
    "    ]\n",
    "    hr = HierarchicalRetriever([\n",
    "    (\"../outputs/indices/automotivespice_clauses.index\",\n",
    "     \"../outputs/indices/automotivespice_clauses_meta.json\"),\n",
    "    (\"../outputs/indices/autosar_ecum_clauses.index\",\n",
    "     \"../outputs/indices/autosar_ecum_clauses_meta.json\"),\n",
    "])\n",
    "\n",
    "res = hr.hierarchical_retrieve(\"What does SYS.2 require?\", k_header=2, k_section=2, k_clause=3)\n",
    "\n",
    "retrieved_texts = []\n",
    "for clause in res['clauses']:\n",
    "    retrieved_texts.append(f\"{clause['title']}\\n{clause['content']}\")  \n",
    "\n",
    "context = \"\\n\\n\".join(retrieved_texts)\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ccd6e1",
   "metadata": {},
   "source": [
    "LLM Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11488583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "  api_key = GEMINI_API_KEY,\n",
    "  model = \"gemini-2.5-pro\", \n",
    "  temperature=0.2,\n",
    "  max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "834aae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPSHOP\\AppData\\Local\\Temp\\ipykernel_28568\\2235667284.py:6: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm([system_prompt, human_prompt])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, it is impossible to determine what SYS.2 requires. The text only lists \"SYS.2\" as an item under the \"System Engineering Process Group (SYS)\" without any further description.\n",
      "\n",
      "---\n",
      "\n",
      "However, as an expert in automotive standards, I can tell you that **SYS.2** is a standard process area identifier from **Automotive SPICE (ASPICE)**.\n",
      "\n",
      "In ASPICE, **SYS.2** is titled **\"System Requirements Analysis\"**.\n",
      "\n",
      "The purpose of this process is to transform the stakeholder requirements into a complete, consistent, and technically correct set of system requirements.\n",
      "\n",
      "**SYS.2 requires an organization to:**\n",
      "\n",
      "1.  **Specify system requirements:** Elicit and document the functional and non-functional requirements for the system.\n",
      "2.  **Structure system requirements:** Organize and categorize the requirements logically.\n",
      "3.  **Analyze system requirements:** Ensure the requirements are correct, technically feasible, verifiable, and consistent with each other.\n",
      "4.  **Analyze the impact on the operating environment:** Understand how the system will interact with and affect its environment (e.g., other vehicle systems, the driver, physical surroundings).\n",
      "5.  **Develop verification criteria:** Define how each system requirement will be tested and proven to be met.\n",
      "6.  **Establish bidirectional traceability:** Link system requirements back to the stakeholder requirements (e.g., customer needs) and forward to the system architectural design.\n",
      "7.  **Ensure consistency:** Verify that the system requirements are aligned with the stakeholder requirements.\n",
      "8.  **Communicate agreed system requirements:** Ensure all relevant parties have access to and understand the approved set of requirements.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "system_prompt = SystemMessage(content=\"You are an expert in automotive standards and AUTOSAR.\")\n",
    "human_prompt = HumanMessage(content=f\"Answer the question based on this context:\\n{context}\\nQuestion: What does SYS.2 require?\")\n",
    "\n",
    "response = llm([system_prompt, human_prompt])\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedcee91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
